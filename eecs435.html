<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2016-02-01 Mon 15:20 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title>EECS 435 Notes</title>
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Stephen Brennan" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://orgmode.org/mathjax/MathJax.js"></script>
</head>
<body>
<div id="content">
<h1 class="title">EECS 435 Notes</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline1">1. 2016-01-26 Tuesday</a></li>
<li><a href="#orgheadline2">2. 2016-01-21 Thursday</a></li>
<li><a href="#orgheadline7">3. 2016-01-19 Tuesday</a>
<ul>
<li><a href="#orgheadline6">3.1. Preprocessing</a>
<ul>
<li><a href="#orgheadline3">3.1.1. Missing Data</a></li>
<li><a href="#orgheadline4">3.1.2. Data Integration</a></li>
<li><a href="#orgheadline5">3.1.3. Data Transformation</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline8">4. 2016-01-14 Thursday</a></li>
<li><a href="#orgheadline11">5. 2016-01-12 Tuesday</a>
<ul>
<li><a href="#orgheadline9">5.1. Introductory Material</a></li>
<li><a href="#orgheadline10">5.2. Chapter 5</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1"><span class="section-number-2">1</span> 2016-01-26 Tuesday</h2>
<div class="outline-text-2" id="text-1">
<p>
Clustering still.
</p>

<p>
I got here while discussing PAM.  It is apparently n<sup>2</sup> per iteration, so there
is CLARA, which is a sampling-based technique.  And therefore more scalable
than PAM.  But since I don't know what that was, it's not too helpful.  But,
that concludes the discussion of partitioning-based algorithms.
</p>

<p>
Now, Hierarchical Clustering.  Two ways:
</p>
<ul class="org-ul">
<li>Agglomerative (AGNES = Agglomerative Nesting): start with individuals and
merge things into clusters.
<ul class="org-ul">
<li>At each iteration, merge the two "nodes" (clusters, which could be
individuals) that have the least dissimilarity.</li>
<li>Uses the Single-Link method (i.e. the shortest distance between
individuals in the clusters).</li>
<li>This method gives you a dendrogram, which shows how the individuals were
merged over time.</li>
</ul></li>
<li>DIANA (= Divisive Analysis):
<ul class="org-ul">
<li>Initially, there is one large cluster consisting of all n objects.</li>
<li>Find the object which has the highest average dissimilarity to all other
objects.  This object initiates a new cluster, S.</li>
<li>For each object i outside the S, compute D<sub>i</sub> = (average distance to
elements not in S) - (average distance to elements in S).</li>
<li>If D<sub>i</sub> &gt; 0, put this element into S.</li>
</ul></li>
</ul>

<p>
Neither of these methods scale well.  So, more recent algorithms aim to do
better:
</p>
<ul class="org-ul">
<li>BIRCH: Balanced Iterative Reducing and Clustering Using Hierarchies
<ul class="org-ul">
<li>Idea: creates a "clustering feature" (CF) tree incrementally.</li>
<li>Phase 1: Scan DB to build initial tree</li>
<li>Phase 2: use an arbitrary clustering algorithm to cluster leaf nodes</li>
<li>This scales well.</li>
<li>Similarity metrics used:
<ul class="org-ul">
<li>Centroid: take all feature vectors, find the average.</li>
<li>Radius: average distance from member points to centroid</li>
<li>Diameter: average pairwise distance between members</li>
</ul></li>
<li>Clustering Feature: (N, LS, SS) = (number of features, linear sum, square
sum)
<ul class="org-ul">
<li>Has enough information to compute the above distance metrics.</li>
<li>Can be merged simply by vector addition.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline2" class="outline-2">
<h2 id="orgheadline2"><span class="section-number-2">2</span> 2016-01-21 Thursday</h2>
<div class="outline-text-2" id="text-2">
<p>
Clustering
</p>
<ul class="org-ul">
<li>Group things together into groups based on "similarity".</li>
<li>No class labels, so this is unsupervised learning (kind of&#x2026;)</li>
<li>Can be used as a stand-alone tool to gain insight on data.</li>
<li>Can be used as a preprocessing step for other algorithms.</li>
</ul>

<p>
What is a good clustering algorithm?
</p>
<ul class="org-ul">
<li>gives you high intra-cluster similarity, low extra-cluster similarity.</li>
<li>Need a similarity metric (some sort of function) to determine this</li>
</ul>

<p>
Ideal properties of a clustering algorithm:
</p>
<ul class="org-ul">
<li>Scalable</li>
<li>Able to deal with different types of attributes</li>
<li>Able to handle "dynamic" (changing) data</li>
<li>Can discover clusters with arbitrary shapes</li>
<li>Requires minimal domain knowledge to determine input parameters</li>
<li>Able to deal with noise and outliers</li>
<li>Insensitive to order of input records</li>
<li>High dimensionality</li>
<li>Incorporation of user specified constraints</li>
<li>Interpretability and usability</li>
</ul>

<p>
Data structures for clustering:
</p>
<ul class="org-ul">
<li>data matrix, n by p, contains each record</li>
<li>dissimilarity matrix, n by n, triangular, uses the similarity/dissimilarity
metric on all pairs of records</li>
</ul>

<p>
Types of clustering:
</p>
<ul class="org-ul">
<li>partitioning</li>
<li>hierarchical</li>
<li>density-based</li>
<li>grid-based</li>
<li>model-based</li>
<li>frequent pattern-based</li>
<li>user-guided or constraint-based</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline7" class="outline-2">
<h2 id="orgheadline7"><span class="section-number-2">3</span> 2016-01-19 Tuesday</h2>
<div class="outline-text-2" id="text-3">
<p>
<b>Constraints + Apriori Algorithm</b>
</p>

<ul class="org-ul">
<li>For anti-monotone constraints, you can prevent candidates from being
generated when you know they and their successors will not satisfy the
constraint.</li>
</ul>
</div>

<div id="outline-container-orgheadline6" class="outline-3">
<h3 id="orgheadline6"><span class="section-number-3">3.1</span> Preprocessing</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>Data mining is based on existing databases</li>
<li>In the real world, data is dirty:
<ul class="org-ul">
<li>Incomplete (lacking attribute values)</li>
<li>Noisy (containing errors or outliers)</li>
<li>Inconsistent (may have contradictions or discrepancies)</li>
</ul></li>
<li>Quality of data mining relies on quality of data.
<ul class="org-ul">
<li>Garbage in, garbage out!</li>
</ul></li>
</ul>

<p>
<b>Types of Preprocessing</b>
</p>
<ul class="org-ul">
<li>Cleaning:
<ul class="org-ul">
<li>Fill in missing values, smooth noisy data, identify or remove outliers,
resolve inconsistencies.</li>
</ul></li>
<li>Integration
<ul class="org-ul">
<li>Integrating multiple databases, data cubes, or files</li>
</ul></li>
<li>Transformation
<ul class="org-ul">
<li>Normalization, aggregation</li>
</ul></li>
<li>Reduction
<ul class="org-ul">
<li>Reduce number of records, attributes, attribute values</li>
<li>(maybe use bagging/boosting, feature selection, dimensionality reduction?
&#x2013; Dr. Soumya Ray, EECS 440, last semester!)</li>
</ul></li>
</ul>
</div>

<div id="outline-container-orgheadline3" class="outline-4">
<h4 id="orgheadline3"><span class="section-number-4">3.1.1</span> Missing Data</h4>
<div class="outline-text-4" id="text-3-1-1">
<ul class="org-ul">
<li>Missing data may be due to (not sure why this matters for us):
<ul class="org-ul">
<li>Equipment malfunction</li>
<li>Inconsistent, and therefore deleted</li>
<li>Not entered.</li>
<li>Data taken at different times.</li>
<li>Etc.</li>
</ul></li>
<li>Strategies:
<ul class="org-ul">
<li>Delete entries with missing data</li>
<li>Fill in missing data.  How?
<ul class="org-ul">
<li>Manually?  Could be tedious.</li>
<li>Default value (e.g. unknown)</li>
<li>Average data (mean imputation, that is)</li>
<li>Most probable value (i.e. model-based imputation, perhaps using a
Bayesian/other probabilistic model)</li>
</ul></li>
</ul></li>
</ul>

<p>
<b>Noisy Data</b>
</p>
<ul class="org-ul">
<li>Binning:
<ul class="org-ul">
<li>partition continuous data into discrete "bins"</li>
<li>more details in next section</li>
</ul></li>
<li>Regression:
<ul class="org-ul">
<li>Smooth by fitting a regression function, then use that.</li>
</ul></li>
<li>Clustering
<ul class="org-ul">
<li>Could be used to bin things.</li>
<li>Could also be used to detect outliers.</li>
</ul></li>
<li>Human + Computer
<ul class="org-ul">
<li>Examine records manually?</li>
</ul></li>
</ul>

<p>
<b>Binning</b>
</p>
<ul class="org-ul">
<li>Equal width:
<ul class="org-ul">
<li>divide range into N intervals of equal size.</li>
<li>simple, but outliers may dominate your result.</li>
</ul></li>
<li>Equal depth:
<ul class="org-ul">
<li>sort data, divide into N groups with approximately the same number of
records</li>
<li>handles skewed data better</li>
</ul></li>
<li>After partitioning, you can smooth the bins by giving each member the mean
value of the bin, or you can use the boundaries of the bin (whichever the
record is closest to)</li>
</ul>

<p>
<b>Regression</b>
</p>
<ul class="org-ul">
<li>Replace noisy or missing values by predicted values.</li>
<li>Of course, this requires some model of the relationship between
attributes.  And you might be wrong.
<ul class="org-ul">
<li>Especially since if you have a solid model of the relationships between
these attributes, you may be less likely to do data mining!</li>
</ul></li>
<li>Useful both for smoothing and for handling missing data.</li>
</ul>

<p>
Looks like we're skipping over clustering et al. and moving on to the next
form of preprocessing:
</p>
</div>
</div>

<div id="outline-container-orgheadline4" class="outline-4">
<h4 id="orgheadline4"><span class="section-number-4">3.1.2</span> Data Integration</h4>
<div class="outline-text-4" id="text-3-1-2">
<p>
Want to identify duplicates, can use automated methods for comparing text,
numbers, etc, for approximate equality.
</p>

<p>
Could use information theoretic approaches to determine how much information
each record adds to the database.
</p>
</div>
</div>

<div id="outline-container-orgheadline5" class="outline-4">
<h4 id="orgheadline5"><span class="section-number-4">3.1.3</span> Data Transformation</h4>
<div class="outline-text-4" id="text-3-1-3">
<ul class="org-ul">
<li>Normalization: makes different records comparable.</li>
<li>Discretization: make continuous attributes discrete so we can apply
methods that require discrete input</li>
<li>Attribute/feature construction: maybe new attributes created from the
given ones are more useful for our purposes than ones given.</li>
<li>Map into vector space (awwww yeah)</li>
</ul>

<p>
<b>Normalization</b>
</p>
<ul class="org-ul">
<li><p>
min-max normalization
</p>
\begin{equation}
  v' = \frac{v - min}{max - min}
\end{equation}</li>
<li><p>
z-score normalization
</p>
\begin{equation}
  v' = \frac{v - \mu}{\sigma}
\end{equation}</li>
<li>another one I missed</li>
</ul>

<p>
<b>Discretization</b>
</p>
<ul class="org-ul">
<li>Three types of attributes:
<ul class="org-ul">
<li>Nominal (categorical) - values from an unordered set</li>
<li>Ordinal - values from an ordered set (presumably either finite or
countably infinite, but not certain)</li>
<li>Continuous - values chosen from a continuous range (presumably
uncountably infinite set).</li>
</ul></li>
<li>Methods:
<ul class="org-ul">
<li>Binning</li>
<li>Clustering</li>
<li>Entropy-based approaches</li>
</ul></li>
</ul>

<p>
<b>Entropy-Based Discretization</b>
</p>
<ul class="org-ul">
<li>Binary discretization:
<ul class="org-ul">
<li>determine a boundary to partition the data into two intervals</li>
<li>pick the boundary that minimizes the entropy</li>
<li>this doesn't make sense to me, wouldn't this just partition into a set
containing one, and a set containing the rest (this minimizes entropy)
<ul class="org-ul">
<li>google this later</li>
</ul></li>
</ul></li>
</ul>

<p>
<b>Mapping Into Vector Space</b>
</p>
<ul class="org-ul">
<li>take "objects" or whatever our data is</li>
<li>determine attributes (e.g. counts of particular words, etc)</li>
<li>put these attributes as a vector</li>
</ul>

<p>
<b>Data Reduction: Feature Selection</b>
</p>
<ul class="org-ul">
<li>sometimes additional information is hurting your performance</li>
<li>maybe cleaner data would help!</li>
<li>you want to select the attributes that give you the important information,
get rid of the ones that are duplicate</li>
<li>could do greedy top-down or bottom up approaches that add/remove the next
"best" feature
<ul class="org-ul">
<li>requires some measure of feature quality, and typically a cutoff</li>
<li>measure is typically information theory based: conditional entropy</li>
<li>heuristic: may not return the optimal feature set according to your
criteria.</li>
</ul></li>
<li>"branch and bound"
<ul class="org-ul">
<li>optimal, but takes longer</li>
<li>requires "monotone" shape of feature space (??)</li>
</ul></li>
</ul>

<p>
<b>Principle Component Analysis</b>
</p>
<ul class="org-ul">
<li>Projects vectors from d-dimensional space into a smaller (c &lt; d) space.</li>
<li>Chooses first the dimension that minimizes "reproduction error".</li>
<li>Then the dimension that minimizes the error from the residuals, etc.</li>
<li>drawbacks:
<ul class="org-ul">
<li>difficult to interpret</li>
<li>requires numeric data</li>
</ul></li>
</ul>

<p>
<b>Sampling</b>
</p>
<ul class="org-ul">
<li>Random sampling can be used to reduce data (I guess).</li>
<li>You can choose with or without replacement</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline8" class="outline-2">
<h2 id="orgheadline8"><span class="section-number-2">4</span> 2016-01-14 Thursday</h2>
<div class="outline-text-2" id="text-4">
<p>
<b>Illustration of Closed Patterns, Max-Patterns</b>
</p>

<ul class="org-ul">
<li>If your DB is {&lt;a<sub>1</sub>, &#x2026;, a<sub>100</sub>&gt;, &lt;a<sub>1</sub>, &#x2026;, a<sub>50</sub>&gt;}, and your minimum (absolute)
support is 1:</li>
<li>Closed itemsets:
<ul class="org-ul">
<li>&lt;a<sub>1</sub>, &#x2026;, a<sub>100</sub>&gt;</li>
<li>&lt;a<sub>1</sub>, &#x2026;, a<sub>50</sub>&gt;</li>
</ul></li>
<li>Max itemsets:
<ul class="org-ul">
<li>&lt;a<sub>1</sub>, &#x2026;, a<sub>100</sub>&gt;</li>
</ul></li>
</ul>

<p>
<b>Algorithm</b>
</p>
<ul class="org-ul">
<li>Subsets of frequent itemsets are frequent as well.</li>
<li>So itemsets which are infrequent should not have their supersets generated
or tested.</li>
<li>Method (Apriori Algorithm):
<ul class="org-ul">
<li>Initially, scan DB once to get frequent 1-itemsets</li>
<li>Generate length (k+1) candidate itemsets from length k frequent itemsets</li>
<li>Test the candidates against DB</li>
<li>Terminate when no frequent or candidate set can be generated</li>
</ul></li>
<li>Seems simple, but there are lost of detail on the algorithm.</li>
<li>How to generate candidates?  Two steps:
<ul class="org-ul">
<li>Step 1: self-joining</li>
<li>Step 2: pruning</li>
</ul></li>
<li>Example of candidate generation:
<ul class="org-ul">
<li>l<sub>3</sub> = {abc, abd, acd, ace, bcd}</li>
<li>First, compute L<sub>3</sub> * L<sub>3</sub> (cartesian product, sort of)</li>
<li>abcd from abc and abd</li>
<li>acde from acd and ace</li>
<li>However, groups like acde are removed because their superset (ade) are not
in the previous frequent itemset.</li>
</ul></li>
<li>There was some Candidate Generation pseudo-code written in a SQL-like syntax
that was rather confusing.</li>
<li>The pruning is pretty simple because you just need to check the k subsets of
size k-1 to make sure they are all frequent.</li>
<li>Counting support of candidates:
<ul class="org-ul">
<li>Candidates are stored in a hash-tree</li>
<li>A leaf node contains a list of itemsets and counts.</li>
<li>Interior node contains a hash table of something (I hope the slides go up
soon)</li>
</ul></li>
<li>The issues for frequent pattern mining are:
<ul class="org-ul">
<li>Multiple scans of transaction database</li>
<li>Huge number of candidates</li>
<li>Tedious workload of support counting for candidates.</li>
</ul></li>
<li>The improvements to the algorithm aim to address these concerns.</li>
<li>Partitioning:
<ul class="org-ul">
<li>Itemsets that are potentially frequent must be frequent in at least one of
the partitions.</li>
</ul></li>
<li>DHP: Reduce the number of candidates
<ul class="org-ul">
<li>A k-itemset whose corresponding hashing bucket count is below the
threshold cannot be frequent.</li>
</ul></li>
</ul>

<p>
Multiple-Level Association Rules
</p>
<ul class="org-ul">
<li>Items frequently form hierarchies, eg milk can be whole, skim, 2%, etc.</li>
<li>May want to have different support thresholds for lower levels of the
hierarchy.</li>
<li>Need to filter "redundant" association rules where child items are involved
but they add no more information to the parent rule.</li>
</ul>

<p>
Constraint based mining
</p>
<ul class="org-ul">
<li>Anti-monotone constraint: when an itemset S violates a constraint, so do all
supersets of S.</li>
<li>Monotone constraint: when an itemset S satisfies a constraint, so do all
supersets of S.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline11" class="outline-2">
<h2 id="orgheadline11"><span class="section-number-2">5</span> 2016-01-12 Tuesday</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-orgheadline9" class="outline-3">
<h3 id="orgheadline9"><span class="section-number-3">5.1</span> Introductory Material</h3>
<div class="outline-text-3" id="text-5-1">
<p>
I was late (and probably will be late frequently due to another class).
Hopefully I didn't miss too much.
</p>

<p>
Grading:
</p>
<ul class="org-ul">
<li>30% paper presentation</li>
<li>10% attendance, participation</li>
<li>60% project</li>
</ul>

<p>
Paper presentation:
</p>
<ul class="org-ul">
<li>Group of two</li>
<li>Present one or two papers.</li>
<li>Lead discussion.</li>
<li>May be helpful to read a few references.</li>
</ul>

<p>
Project:
</p>
<ul class="org-ul">
<li>Best to come up with your own ideas, but he does have some ideas on his
slides.</li>
<li>Project proposal (3-5 pages, due Feb 16) should include:
<ul class="org-ul">
<li>Title, idea, survey of related work, data source, key
algorithms/technology, and what you expect to submit at the end of the
semester.</li>
</ul></li>
<li>Final report (1-20 pages, due April 21) must include:
<ul class="org-ul">
<li>Comprehensive description of your project</li>
<li>Project idea, extended survey of related work, result, etc.</li>
<li>What worked, what didn't work, what surprised you, etc.</li>
</ul></li>
</ul>

<p>
Data mining conferences:
</p>
<ul class="org-ul">
<li>ACM-SIGKDD</li>
<li>IEEE-ICDM</li>
<li>SIAM-DM</li>
<li>PKDD</li>
<li>PAKDD</li>
</ul>

<p>
Journal:
</p>
<ul class="org-ul">
<li>Data Mining and Knowledge Discovery</li>
<li>KDD Explorations</li>
</ul>

<p>
Other references can be found in the conferences and journals for the fields:
</p>
<ul class="org-ul">
<li>Database systems</li>
<li>Artificial Intelligence</li>
<li>Statistics</li>
<li>Bioinformatics</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline10" class="outline-3">
<h3 id="orgheadline10"><span class="section-number-3">5.2</span> Chapter 5</h3>
<div class="outline-text-3" id="text-5-2">
<p>
<b>Mining Frequent Patterns, Association, and Correlations</b>
</p>

<ul class="org-ul">
<li>Original slides at: <a href="http://www.cs.uiuc.edu/homes/hanj/bk2/slidesindex.htm">http://www.cs.uiuc.edu/homes/hanj/bk2/slidesindex.htm</a>
     (broken link).</li>
<li>Frequent pattern analysis - finding items, subsequences, substructures,
etc, that occur frequently in a data set.
<ul class="org-ul">
<li>Related - frequent itemsets, association rule mining</li>
<li>Agrawal, Imielinski, and Swami '93 introduced these ideas</li>
</ul></li>
<li>Motivation is to find regularities, rules, etc that give you insight into
data.  For instance:
<ul class="org-ul">
<li>Beer and diapers are frequently purchased together.</li>
<li>What are subsequent purchases after a PC?</li>
</ul></li>
<li>Itemset: a set of items</li>
<li>k-itemset: an itemset containing k items</li>
<li>Call the set of all items \(I = \{I_1, \dots, I_m\}\)</li>
<li>\(T \subseteq I\) is a transaction, and has a transaction id.</li>
<li>An association rule is of the form \(A \to B\), with minimum support and
confidence, where \(A \subset I\), \(B \subset I\), \(A \cap B =
     \emptyset\).</li>
<li>Support (for \(A \to B\)) is \(Pr[A \cup B \subseteq T]\).
<ul class="org-ul">
<li>That is, the probability that a transaction contains all of A and B.</li>
<li>As a percentage, also referred to as relative support.</li>
<li>As a number, referred to as the absolute support</li>
</ul></li>
<li>Confidence (for \(A \to B\)) is \(Pr[B \subset T | A \subset T]\)
<ul class="org-ul">
<li>That is, the conditional probability that a transaction contains B, given
it contains A.</li>
<li>\(Pr[B \subset T | A \subset T] = \frac{Pr[B \cup A \subseteq T]}{Pr[A
       \subset T]} = \frac{support(A \cup B)}{support(A)}\)</li>
</ul></li>
<li>Confidence can be easily derived from the support of \(A\) and \(A \cup
     B\).  Which means you can "reduce" this problem to finding itemsets with
high confidence, and then looking for disjoint pairs with high confidence.</li>
<li>Big patterns have lots of subsets, and therefore lots of other frequent
patterns.  So, you want to find closed patterns and max-patterns.</li>
<li>Closed pattern: X is closed if X is frequent and there is no super-pattern
\(Y \supset X\) with the same support as X.</li>
<li>An itemset X is a max-pattern if X is frequent and there is no frequent
super-pattern \(Y \supset X\).
<ul class="org-ul">
<li>A "maximal" frequent super-pattern.</li>
<li>All max-patterns are closed.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Stephen Brennan</p>
<p class="date">Created: 2016-02-01 Mon 15:20</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
