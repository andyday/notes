#+TITLE: EECS 440 Notes
#+AUTHOR: Stephen Brennan
#+SETUPFILE: config.setup

* 2015-11-12 Thursday

  Examples of PAC learnable functions:
  - Boolean conjunctions over $n$ attributes.
  - $k$-DNF formulas over $n$ attributes, for fixed $k$.
  - Arbitrary Boolean functions.  (PAC learnable, but not efficiently due to the
    fact that it's doubly exponential)

  Assumptions:
  - Target concept is in the hypothesis class.
  - Hypothesis class is finite.
  - No noise
  - Consistency

  Relaxing the assumptions:
  - Consistency is tied to the hypothesis class containing the hypothesis class.
    - Instead, we should bound the difference between the output hypothesis and
      the /best/ hypothesis.
    - Let \(E_i = 1\) if output hypothesis makes a mistake on example $i$, 0
      otherwise.
    - \(E_i\) is a Bernoulli random variable with parameter \(err(h,f)\).
    - Our estimate of \(err(h,f)\) is \(err_S(h,f) = \frac{1}{m} \sum_i E_i\)
    - Chernoff Bound: if a rv can be expressed as a sum of independent indicator
      rv's, you can use this to bound the deviation of the sum from the true
      value: \(Pr(|err(h,f)-err_S(h,f)| > \epsilon) \le 2e^{-2m\epsilon^2}\)
    - When you plug a lot of things in, you get \(m \ge \frac{1}{2\epsilon^2} \log
      \left(\frac{2|H|}{\delta}\right)\), which ensures that the probability of all
      hypotheses being good is greater than $\delta$.

  ERM Learners
  - Empirical risk minimization algorithms output a hypothesis with the least
    training error.
    - \(h_{best} = \arg \min_h (err(h,f))\)
    - \(H_{ERM} = \arg \min_h (err_S(h,f))\)
  - Observe:
    \begin{align*}
      err(h_{ERM}) &\le err_S(h_{ERM}) + \epsilon \text{  by choice of }m\\
               &\le err_S(h_{best}) + \epsilon \text{  by ERM}\\
               &\le err(h_{best}) + 2\epsilon \text{  by choice of }m
    \end{align*}
  - So the error of $H_{ERM}$ is at most $2\epsilon$.
  - Importance here is that even when the target concept isn't guaranteed to be
    in the hypothesis class, we can still guarantee that we can learn something
    suitably close to best concept.

  Infinite Hypothesis Classes
  - Many hypothesis classes aren't finite.  For instance, there are infinite
    choices for a line in R^2
  - But we have least squares regression, and in general we don't expect that we
    need more than $O(n)$ examples to fit such a model with $n$ parameters.

  Let's look at an example: Learning axis parallel rectangles in R^2.  The
  algorithm is to pick the rectangle with the tightest fit to the positives.
  - This strategy will always produce a rectangle that is within the target
    rectangle.
  - To show this is pac learnable, we want to show that the "strip" of the
    target rectangle to the learned rectangle contains a probability mass of
    $\epsilon/4$.
  - We call this region of the rectangle $T$.  The only way we have too much
    strip on that edge is if none of our examples come from $T$.
    \begin{align*}
      Pr(err(R', R) > \epsilon) &= Pr(\text{mass under strips exceeds }\epsilon) \\
    &\le 4 Pr(\text{mass under 1 strip exceeds }\epsilon/4) \\
    &\le 4 Pr(\text{no point in }m\text{ samples lies in }T)\\
    &\le 4\left(1-\frac{\epsilon}{4}\right)^m
    \end{align*}
    There was more to the proof, but I couldn't write it all down.  See slides

  This worked because there was a nice geometric shape that allowed some cool
  tricks.  What about the general case of infinite hypotheses?  We can use a
  clever trick called "projecting the function onto the data."  We know that
  there are an infinite number of possible hypotheses, but we also know that on
  a finite training sample, the behaviors of the infinite set of hypotheses is
  finite.  We can define equivalence classes based on the performance on the
  training set.

  Growth Function
  - For a sample \(S=x_1, \dots, x_m\), let \(F(S) = \{[h(x_1), \dots, h(x_m)],
    \text{ for all }h\in H\}\)
  - Growth function: \(G(m) = \max_S |F(S)|\).
  - If \(G(m) = 2^m\), the classifier can memorize any examples you give it.  We
    say that it shatters the examples.  Or shatters something.  I'm not sure
    what it shatters.
  - The size of the largest set that $H$ can shatter is the Vapnik-Chervonenkis
    (VC) dimension of $H$.
  - If \(VC(H) = d\), then there is some set of size $d$ that can be shattered,
    and
  - No set of size \(d+1\) can be shattered.
  - If we can find sets of arbitrary size that can be shattered, \(VC(H) =
    \infty\).

  Examples:
  - Suppose $H$ is the class of all intervals on the real line.  What is its VC
    dimension?
    - 1? we can shatter that!
    - 2? we can shatter that too!
    - 3? nope, no shattering that (say we had +,-,+).
    - So, the VC dimension is 2
  - How about linear functions in 2D?
    - VC dimension is 3.

* 2015-11-03 Tuesday

** Learning with Prior Knowledge

   - Humans don't just learn by examples and inductive reasoning.  If we did, we
     would have a really hard time!
   - Much of how/what we learn is due to prior knowledge.
   - How to integrate that into machine learning algorithms?

*** Pros of Prior Knowledge

    - Can provide accurate inductive bias.
    - Can provide good starting points for optimization.
    - Can reduce the number of examples we need to see to find a good concept.

*** Cons of Prior Knowledge

    - Frequently prior knowledge is /approximate/.  If we provide wrong prior
      knowledge, it may be worse than no prior knowledge.

*** Inductive and Analytical Learning

    - Inductive Learning (what we've been doing so far): given training data,
      find the hypothesis from the hypothesis space that best generalizes data.
      - focuses on generalizing examples to derive a theory
    - *Analytical Learning:* Given a (logical) "domain theory", find the
      hypothesis /deduced from theory/ that best /explains/ data
      - focuses on "specializing" your domain theory to explain data

*** Prior Knowledge and ML

    - An active research area.
    - Two main directions:
      - Algorithm-specific approaches for non-probabilistic algorithms.
      - Bayesian priors for probabilistic algorithms!

*** Learning Problem Setup

    - Given:
      - A set of training examples and
      - A /domain/ theory about the learning problem.
    - Output a hypothesis that best fits both.
    - Goal: both training examples and prior knowledge are imperfect.
      - We must try to learn a concept that will do better than concepts
        produced by either of them in isolation.

*** Rule-Based Prior Knowledge

    - Often, prior knowledge we have is expressed as rules.
      - EG: "I think any furry scary thing with long teeth is a lion."
    - Can express this as a logical proposition.
      - EG: Has-fur \land Long-Teeth \land Scary \to Lion
    - Here, we only consider propositional theories, but we can also have
      first-order domain theories.

    Terminology for propositional logic:
    - Literal: proposition or negation
    - Clause: disjunction of literals
    - Horn clause: a clause with at most one unnegated literal
      - This is special because you can take a horn clause and write it as a
        logical implication: \(x_1 \lor \lnot x_2 \lor \lnot x_3\) becomes \(x_2
        \land x_3 \to x_1 \)

*** KBANN Algorithm

    - First, create a network (with sigmoid functions) that perfectly fits the
      domain theory.
      - Suppose we have a Horn Clause \(x_2 \land x_3 \to C\):
        - We can create a network consisting of a single perceptron.  We give a
          weight of 1 on x_1 and x_2, and we set our activation threshold just
          below two.
        - In theory, we could set the weights to be the whatever we'd like, and
          we could scale the threshold accordingly.  The actual value of the
          weights doesn't matter for creating a "perfect fit".
        - The weights do matter for the later backprop step.  If you have large
          weights, the effect of the examples on the network will be minimal.
          So you can think of the overall weight assignment as a confidence
          measurement for the rule.  The more confident you are in a rule, the
          larger the weights, and the less the training examples can change this
          aspect of the learned concept.
      - Suppose we have a more complex domain theory: \(x_2 \land x_3 \to C\),
        \(x_1 \land x_2 \to C\).  Any of these clauses evaluating to true means
        the example is labeled with C.
        - We can create a network with a hidden layer.
        - The hidden layer contains a perceptron for each clause as above.
        - The output layer takes the output of the two clauses and acts like an
          "or gate".  In terms of its weights, this means that the activation
          threshold is around \(0.5W\), where \(W\) is the weight of its inputs.
          So, it activates when any of its inputs are activated.
      - Suppose we have a domain theory which has variables that aren't
        observable.  EG: \(x_1 \land h \to C\), \(x_3 \land x_4 \to h\)
        - In this case, we need to have more layers.  The variable \(h\) is
          "defined" as the output of a perceptron in an earlier layer, and then
          used in the later layers as we've discussed.
        - This is equivalent to "substituting" the definition for the variable,
          but it is likely to simplify the task of the learner.
    - As an intermediate step, KBANN will take the structure derived above, and
      it will add in *every* missing edge from the output of a previous layer to
      the input of the next, with 0 weight.  This is important because our
      domain theory is assumed to be imperfect, and we would like to give our
      learner the flexibility to modify the concept beyond what the domain
      structure initially said.
      - An interesting question is whether 0 weights give you a chance to encode
        your confidence in the fact that a given input has no impact on that
        proposition.  I think it's the difference between 0 and $W$, but you
        could ask KBANN explicitly not to include edges if you really don't
        think they should be there.  This is just heuristic.
    - Then, use backpropagation to refine this network to fit the training
      examples.
      - If your prior knowledge was completely consistent with examples,
        backpropagation will not change anything.  This is because the loss
        function will be 0, so any backpropagation updates will be multiplied
        by 0.
      - If not, this will be a good initial point to fit your data.  The weights
        chosen previously will affect how "quickly" the input data will make the
        network converge to the correct concept.

    Refining Prior Knowledge:
    - You can take the final trained network, and go back and figure out which
      parts of the prior knowledge were wrong.
    - Unfortunately, you can't always interpret a network structure and weights
      as a meaningful horn clause anymore.
      - There are algorithms to approximately do this (eg TREPAN).

*** KBSVM Algorithm

    This algorithm incorporates prior knowledge into (you guessed it) SVMs.
    (Fung, Mangasarian, Shavlik 2002)

    - How do we think of prior knowledge in terms of SVMs?  Well, we could think
      of them as "regions" of the input space that you know belong to one class.
    - We can define "polyhedrons", regions bounded by hyperplanes.  (Not
      necessarily completely bounded).

**** Knowledge Sets

     Each region is a knowledge set defined by \(C \cdot \vec{x} \le c\), and it
     defines an implication: \(C \cdot \vec{x} \le c \to \vec{w} \cdot \vec{x} + b \ge 1\).

     These can be added as constraints into the QP.

* 2015-10-29 Thursday

** Weighting

   Earlier we were talking about boosting, which is a way to "focus attention"
   on examples that a classifier misclassifies.  In order to do this, we need te
   weight each example as we train a classifier.

   For pretty much any classifier, you can incorporate a weight parameter into
   its formulation.  Especially for ANNs and Logistic Regression, where you then
   simply take the gradient of the loss function, and the weights are baked into
   the parameter update.

** Why Boosting (Adaboost) Works

   - People aren't really sure - this is an active area of research.
   - One idea: when you do boosting, you shift the decision surface to emphasize
     separating certain examples.  Then, if you put multiple boosted classifiers
     into an ensemble, they will create a much more complex decision surface
     capable of separating all the examples.
   - Another idea: Adaboost could instead be viewed as a margin maximization
     algorithm.  This means that as you continue to boost classifiers, you get
     more and more generalizable ones.

** Another Perspective of Adaboost

   You can also look at Adaboost as a process that minimizes the exponential
   loss of an ensemble classifier.

   If you replace the exponential loss function with another one (say, logistic
   loss), you actually get logistic regression, where each learner in the
   ensemble will represent a particular feature.

** Other Boosting

   Gradient boosting.  Minimizes squared loss of the gradient (not of the
   classifier).  This is like cascade correlation in ANNs.

** Mixture of Experts

   - Look at each classifier as an expert in a certain area of the input.
   - Do weighted votes proportional to how likely the example is to belong to
     each expert's responsible area.

* 2015-10-22 Thursday

  Today, Burr Settles from Duolingo will be talking about some of the stuff he
  did during his PhD.  We won't be tested on this stuff, but it'll still be
  interesting.

  - Text annotation example (for abstracts).
  - Wants to extract information from citations? (seems like something that
    could be parsed)
  - Active learning can help reduce the size of the needed training set size.
    - Interestingly, it doesn't really reduce annotation time.
    - Probably because the examples the algorithm "asks for help" on are the
      hardest to annotate.
  - Interactive learning:
    - Human gives "advice" (some rules or generalizations to start from).
    - ML attempts to "self-train" on unlabeled data (semi-supervised learning)
    - ML asks for help occasionally

** Active Feature Labeling

   Conditional Random Fields (CRFs).  They take sequential information and learn
   from them.  (Apply logistic regression to markov processes)

   Each word is something like a feature.  Humans can provide rules to label
   features.  The algorithm attempts to minimize the Kullback-Liebler Divergence
   between the feature label distribution and the expected label distribution
   (from the examples).

   The system chooses features it would like labeled (active learning).  Instead
   of choosing the ones that maximize some exact information theoretic quantity
   that encodes the amount of information it holds, they chose to approximate
   this with the "uncertainty" (i.e. entropy), multiplied by the log of the
   frequency (so that we choose the common, uncertain features).

** DUALIST: Interactive Classification

   Downsides of the previous algorithm were that it wasn't truly interactive.
   It was slow, and the user couldn't "volunteer" information and rules they
   just came up with.  DUALIST (which is pretty neat) did text classification
   and allowed the user to add words to each label, and also presented words it
   wanted to know that the user could label as well.  It retrained really
   quickly, which made it pretty nice and interactive.

   https://github.com/burrsettles/dualist

   Results fairly positive, but a lot of failure cases were due to human error.

** Human Behavior

   Some of the problems with DUALIST were due to human errors.  They used Amazon
   Mechanical Turk to get larger (n=30) samples for their examples.  They got
   results that were not positive.  However, it turned out that volunteering
   feature labels to the model is actually the most important part of getting a
   good model.  So it takes some good interface design to encourage good input
   to the model.

* 2015-10-13 Tuesday

** Part 2: Issues in Machine Learning

   We're entering the second part of the course!  We've covered algorithms,
   evaluation, and comparison.  Now we cover extensions and enhancements, and
   implementation issues.

*** Handling Missing Attribute Values

    - Frequently you don't have complete data due to cost, noise, or just
      inconsistencies on behalf of the person doing the measuring.
    - How do we handle it?

    Strategies:

    - Preprocess to remove missing  values.
    - Algorithm specific strategies.
    - A general strategy for generative probabilistic models (expectation
      maximization)

**** Preprocessing

     - Easiest strategy: remove all examples with missing examples.
       - Sometimes this is the sensible thing to do - especially if only a few
         examples are missing data.
     - Next easiest: replace with a new value that indicates "unknown"
       - This can cause unforeseen consequences!  (gene expression leukemia
         story)
     - Third strategy: fill in the missing values somehow, based on the observed
       data.
       - (this is called imputation)
       - It's a learning algorithm within a learning algorithm.
       - Machine Learning-ception

***** Mean Imputation
     - Suppose the \(j\)th feature is missing from the \(i\)th example is
       missing.
     - Substitute with the average value for the \(j\)th feature taken over all
       examples with the same label for which $j$ is observed.

       \begin{equation}
         \hat{x}_{ij} = \frac{\sum_{\{k|x_{kj}\text{ is observed}\}} x_{kj} I(y_k = y_i)}{\sum_{\{k|x_{kj}\text{ is observed}\}} I(y_k=y_i)}
       \end{equation}

       This is useful for training.  What about prediction?
     - Suppose the \(j\)th feature for new example, $x$, is missing.
     - Substitute with the average value of the \(j\)th feature taken over all
       examples for which $x_j$ is observed.

       \begin{equation}
         \hat{x}_j = \frac{\sum_{\{k|x_{kj}\text{ is observed}\}} x_{kj}}{\text{# where }x_{jk}\text{ is observed}}
       \end{equation}

***** Model-based Imputation

      - Suppose you have example $i$, which is missing some attributes, in
        particular $j$.
      - Let $O$ be the set of observed attributes for $i$.
      - Find the set of examples $E$ which (i) have the same label as $j$, (ii)
        have at least both $O$ and $x_j$ observed.
      - Learn a model to predict $x_j$ using $O$, trained on $E$.
      - Use this model to impute $x_{ij}$.
      - *Very costly:* You must do this for every single cell that is missing!

***** Imputation Recap

      - If you need to do some sort of imputation, Dr. Ray recommends you start
        with mean-based imputation.  It works "surprisingly well".

**** Algorithm-Dependent Strategies

     The above preprocessing solutions ignore which classification algorithm you
     are going to use with the data.
     - "Generally not a good idea"

     Some algorithms have tailored approaches to deal with missing values.

***** Decision Trees
      *Prediction:*

      - When training, maintain fractions of training examples that went down
        each branch, at every node.
      - When you encounter a node that asks you to test an attribute that your
        example doesn't have, send it down "both" (or all) paths.
        - You'll get back a prediction with a weight.
        - Add up all the fractions for each label, and pick the label with the
          biggest fraction.
      - If you have lots of incomplete data, then your classification time is
        like $b^k$, where $b$ is your branching factor and $k$ is your number of
        missing attributes.  So it's pretty inefficient.

      *Learning:*
      - Key question: how to compute $IG(X)$ if there are examples with $X$
        missing?
      - Idea: when partitioning the data to get $H(Y|X)$, send "fractions" of
        these examples to all buckets for $X$.
      - For the \(i\)th bucket, the fraction is equal to the proportion of the
        \(i\)th value of $X$ in /other/ examples at this node, where $X$ is
        observed.
      - On the slides, he has a table with an example, that really helps the
        understanding.  I'd check it out if this is confusing.
      - When you go down a node, you need to remember what proportion of each
        example is "present" in this node.
      - His slides example:
        - You have an example that is missing "color".  \(\frac{3}{8}\) of the
          other examples have color=red, so when you split a node on color, you
          say that there are 3 and \(\frac{3}{8}\) examples where color=red.
        - When you go down to the next node, and that examples is present, you
          need to remember that there is only \(\frac{3}{8}\) of that example
          present.
        - If that example is missing another attribute shape, and
          \(\frac{1}{3}\) of the other examples in that node have shape=circle,
          then you compute that \(1 + (1/3) * (3/8) = 1 \frac{1}{8}\).

***** Neural Networks

      People typically just use preprocessing, because modifying ANN's is too
      difficult.

***** Generative Models

      Expectation maximization.

      - For a missing feature in an example, create a random variable $S_i$.

        \begin{align*}
          \hat{\theta} &= \arg \max_{\theta} \sum_{i} \ln p(\vec{x}_i, y_i; \theta) \\
          &= \arg \max_{\theta} \sum_{i \in NoMissing} \ln p(\vec{x_i}, y_i; \theta) + \sum_{i \in Missing} \ln \sum_{s_i} p(\vec{x}_i, s_i, y_i; \theta) \\
          &= MLE_{\text{NoMissing}} + EM
        \end{align*}

      Steps associated with EM:

      - Initialize the parameters of the probabilistic model randomly.
      - EM is a two stage optimization process:
        - E(xpectation): estimate the values of the hidden variables given the
          data and the current model.
        - M(aximization): find the MLE estimates of the parameters, given the
          now complete data.
      - Iterate until convergence.
        - Fortunately, this procedure will not cycle, and it will reach an
          optimal value.  But, there's no guarantee that it will be a global
          optimum.

* 2015-10-08 Thursday

  Naive Bayes Classifier

  The naive bayes classifier implements a linear decision boundary.

** Why Does Naive Bayes Work Well?

   - Theoretically, it should not.
   - The independence assumptions made by naive bayes are nearly always wrong.
   - Strangely, it still works well.
   - Turns out, it works quite well for *classification*, but the probability
     estimates are usually bad.

** Tree Augmented Naive Bayes

   Augments Naive Bayes so that there is a tree structure over the attributes,
   which is unknown.

   - Instead of each $X_i$ node having \(p(X_i |Y)\), you have \(p(X_i | X_j and
     Y)\).
   - Makes fewer independence assumptions -> better performance.
   - Has optimal algorithm to learn the structure:

   *ALGORITM*

   - Create a complete graph over the attributes.
     - Edges are weighted by \(I(U, V| Y)\) (conditional mutual information).
   - Find the maximal weighted spanning tree of this graph.
     - Just negate edge weights and apply Prim's Algorithm.
   - Set the parameters with maximum likelihood estimation.

   It turns out that this is the tree structure that maximizes the likelihood of
   the data.  (ask for paper)

** Logistic Regression

   The simplest discriminative model.  Models "log odds" as a linear function:

   \begin{equation}
   \log \frac{p(Y = 1 | \vec{x})}{p(Y = -1| \vec{x})} = \vec{w} \cdot \vec{x} + b
   \end{equation}

   If you manimulate this equation (in slides) you get the following:

   \begin{equation}
     p(Y=1|\vec{x}) = \frac{1}{1 + e^{-(\vec{w} \cdot \vec{x} + b)}}
   \end{equation}

   To do classification, you simply compute the above probability and classify
   as positive if it's greater than 0.5, otherwise negative.

   To do learning, you can use maximum likelihood estimation.  In the previous
   example, we used the likelihood of the data.  In this example, we maximize
   the conditional likelihood of the data.

   \begin{align*}
     \vec{w}, b &= \arg \max \Pi_i p(Y_i = p_i | \vec{x}_i) \\
     &= \arg \max \sum_{i\in pos} \log p(Y_i = 1 | \vec{x}_i) \sum_{i\in neg} \log p(Y_i = -1 | \vec{x}_i)
   \end{align*}

   You can also add a term to control for overfitting.  Typically
   \(\frac{1}{2} ||\vec{w}||^2\).  Then you have to switch the signs of the rest
   of the function and do a minimization instead.  You can do this min/max
   problem using gradient descent or many other optimization methods.  It's very
   robust, works well in many practical settings, and is easy to code.

   Geometry - classify as positive iff \(\vec{w} \cdot \vec{x} + b > 0\).  So it's
   linear again.

* 2015-09-29 Tuesday

** Support Vector Machines

   - SVM's are a rather new method in machine learning.
   - Produced by multiple groups of people in ML, Statistics, and Operations
     Research who basically converged on this idea.
   - Three fundamental ideas:
     - Linear discriminants (we saw this with a perceptron)
     - Margins
     - Kernels

*** Linear Discriminants

    What is?
    - \(sign(5x_1 + 3x_2 - 4)\)
    - \(sign(x_2 - 4x_1^2)\)
    - \(sign(x_2 - e^{-x^2})\)

    This was a trick question.  They all are.  In machine learning, we aren't
    particularly interested in the $x$ variables -- they are given to us.  We are
    interested in the coefficients $w$.  So when we talk about linear
    discriminants, we mean linear in terms of $w$, not $x$.
    - In general, we talk about a linear discriminant in this form:
    - \(\vec{w} \cdot \phi(\vec{x}) + b = 0\)

    When we talked about perceptrons not being able to discriminate XOR, it was
    not entirely true.  If you make a transformation $\phi$ of the variables, you
    can define a line that discriminates XOR perfectly.  This transformation is
    \(\phi(x_1, x_2) = x_1 + x_1 x_2 \).  You can see the diagram in the slides.

*** Margins

    Given a training sample, you can frequently define tons of linear
    discriminants that cleanly separate the training sample.

    What is the best one?  Probably the one that is mostly in the middle of the
    training positives and negatives.

    We define *margins* as the amount you could slide the discriminant in until
    you reach a point.  With this, we can say that we would define our best SVM
    classifier as the one with the /maximum margin/.

    When you are in the "/input feature space/", (i.e. \(\phi(\vec{x})=\vec{x}\)),
    this is called a "linear SVM" (which is a bit confusing, but this means
    linear in the feature space, as opposed to the coefficient space -- in
    essence, it's a linear, linear SVM).

** Why it makes sense

   - Computationally easy to come up with.
   - Requires relatively few data points.

** Calculating the Margin

   If you have a classifier \(\vec{w}\cdot\vec{x} + b = 0\), and your first positive
   datapoint is at \(\vec{w}\cdot\vec{x} + b = 1\), and your first negative point is
   \(\vec{w}\cdot\vec{x} + b = -1\).  Cool.

   Note that $\vec{w}$ is perpendicular to \(\vec{w}\cdot\vec{x} + b = 0\),
   - Why? Pick any $\vec{u}$, $\vec{v}$.
   - \(\vec{w}\cdot(\vec{u}-\vec{v}) = \vec{w}\cdot\vec{u} - \vec{w}\cdot\vec{v} = (-b) -
     (-b) = 0\)
   - Also, $\vec{w}$ is perpendicular to the plus and minus planes.

   There's some stuff here that I didn't follow.  Essentially, he got to the
   point where maximizing the margin is equivalent to maximizing
   \(\frac{2}{||\vec{w}||}\), which is equivalent to $\frac{||\vec{w}||^2}{2}$.

** Problem Formulation

   What we want to do:
   - While respecting the labels of training examples:
     - \(\vec{w}\cdot\vec{x_i} + b \ge 1 \text{ if } y_i = 1\)
     - \(\vec{w}\cdot\vec{x_i} + b \le -1 \text{ if } y_i = -1\)
   - Or, more compactly:
     - \(y_i(\vec{w}\cdot\vec{x_i} + b) \ge 1\)

   Problem formulation:
   - \(\min \frac{1}{2} ||\vec{w}||^2\), s.t.
   - \(y_i(\vec{w}\cdot\vec{x}_i + b) \ge 1\)

   Whoa, it's quadratic programming!  There are algorithms to do this, and since
   the constraints are linear, the feasible region is convex, and therefore we
   have a global minimum!

   Unfortunately, if the data are not linearly separable, then our quadratic
   programming algorithm will come back saying that the problem is not feasible.
   So, we need to allow for misclassification by adding slack variables:

   - \(\min \frac{1}{2} ||\vec{w}||^2\), s.t.
   - \(y_i(\vec{w}\cdot\vec{x}_i + b) + \xi_i \ge 1\)
   - \(\xi_i \ge 0\)

   Sadly, this doesn't solve the problem.  A trivial solution that always
   exists: \(\vec{w}=\vec{0}\), and \(\xi_i=0\)!  We need to add into the objective
   function a term to simultaneously minimize the number of errors in the
   classifier.  Unfortunately, the number of errors is not a differentiable
   quantity, but \(\sum_i \xi_i\) is a good proxy and is differentiable.  Hooray!

   - \(\min \frac{1}{2} ||\vec{w}||^2 + C\sum_i \xi_i \), s.t.
   - \(y_i(\vec{w}\cdot\vec{x}_i + b) + \xi_i \ge 1\)
   - \(\xi_i \ge 0\)

   In this program, $C$ is a constant that quantifies the "tradeoff" between the
   generalization and the error.  The value tends to be problem and dataset
   specific, so you need to determine it via cross validation in your code.

   Notice that in an optimal solution \(\xi_i = \max(0,1-y_i(\vec{w}\cdot\vec{x}_i +
   b))\).  We can lift this into the objective function to remove the
   constraints:

   \begin{equation}
     \min \frac{1}{2} ||\vec{w}||^2 + C\sum_i [(1 - y_i(\vec{w}\cdot\vec{x}_i + b))]^2
   \end{equation}

   We don't need to use quadratic programming now, we can just solve an
   unconstrained problem, which is much simpler and more efficient.  Yay us.

** Course Project Notes

   Sign up sheet: "I want to do project", "I have an idea for a project."

   Email with idea for project by 10/9.  Official project start date 10/23.

   Significant work - treat like a real research project.  Start with a
   hypothesis of what you'd like to show.  You may be algorithm-specific, or
   application specific:
   - I have a better version of algorithm X.
   - My algorithm X is better for application Y.

   Preferably, you should integrate the project with your own research.  You may
   also be able to integrate the project with other course projects as well.

   In terms of grading, the project is 35% of your grade:
   - 25% is a writeup, in a conference format, of your problem, experiments,
     observations, and interesting directions.
   - 10% is a presentation to the class on what you did, during finals week.

   Website will have example projects, and a document with requirements for the
   writeup document.

   *The writeup is due Dec 10!  /No extensions!!/*

* 2015-09-24 Thursday

  Recall: we want to compare ML algorithms.  Our steps for comparing test data:
  1. Determine the sampling distribution.
  2. Estimate parameters using MLE.
  3. Use an approximation distribution if necessary.
  4. Come up with a C% confidence interval.

  Our "Issue #1" was "what do we know about the true performance of a
  classification algorithm, given that we've tested it on a test set?"
  - We determined that we could come up with a confidence interval for an
    observed test statistic.

** Issue 2.1

  Our next issue, #2.1, is that we have a conjecture "classifier A is better
  than algorithm B on a certain type of data."  We would like to evaluate
  whether this conjecture is true.  We can do this with statistical hypothesis
  testing.

  We want to look at the random variable \(F = Err_{C_1} - Err_{C_2}\).  What can we
  say about the sampling distribution of $F$?  Assuming that the error
  distributions are Gaussian, the distribution of $F$ is going to be Gaussian
  also.  The MLE parameter estimates:
  - \(E(F) = e_{S,C_1} - e_{S,C_2} = \left(\frac{r_1}{n_1} - \frac{r_2}{n_2}\right)\)
  - \(V(F) = V(Err_{C_1}) + V(Err_{C_2}) = \frac{e_{S,C_1}(1-e_{S,C_1})}{n_1} + \frac{e_{S,C_2}(1-e_{S,C_2})}{n_2}\)

  Comparing classifiers: hypothesis testing:
  - Establish your "null hypothesis."
    - You will reject this hypothesis with high probability.
    - You presume it is true until the test shows otherwise.

  This test assumes that the two classifiers were evaluated on independent test
  data.

  Example:
  - On a test set with (*0 examples a decision tree misclassifies 20 examples.
    on the same test set, a neural network misclassifies 25 examples. Are these
    two classifiers actually different on this problem?
  - \(F=\frac{r_1}{n_1} - \frac{r_2}{n_2} = 0.05\)
  - \(V(F) = 0.2(1-0.2)/100 + 0.25 (1-0.25) / 100 = 0.0016 + 0.001875 = 0.0034\)
  - Standard deviation is about 0.05.
  - 0 is definitely within the 95% confidence interval, so we cannot reject the
    null hypothesis (that they are the same)

** Issue 2.2

   This issue is critically different from 2.1.  In 2.1, we had two classifiers
   -- who knows where they came from, but we want to compare those two
   classifiers' performances.  In this case, we have two learning algorithms,
   and we want to compare the performance of the *algorithms*, not a particular
   *classifier* produced by an algorithm.

   In order to do so, we must find the expected value of an algorithm's error
   rate.  To do this, we must take the average over all classifiers, produced by
   all possible training sets.  We usually estimate this by doing $n$-fold
   validation instead of actually finding all possible training sets from the
   population.

   We can do /pair testing/, where we evaluate the algorithms on the same folds,
   and then compare the difference between their error rates.  Or, we can do it
   independently, on separate folds, and compare their error rates.  But this
   method gives you a bigger variance.

   When you compare the difference of error rates, you want to know what the
   sampling distribution is.  The sampling distribution looks Gaussian, but not
   quite.  Instead, it's a $t$ distribution.  If $k$ (the number of folds) was
   very large, we could use the Gaussian, but instead we have to use $t$
   distribution, with $k-1$ degrees of freedom.  Here are parameter estimates:
   - Mean (\delta): the average difference of error rates across $k$ folds.
   - Standard Deviation:
     \begin{equation}
       s = \sqrt{\frac{\sum_{i=1}^k (\delta_i - \delta)^2}{k(k-1)}}
     \end{equation}
   - The standard deviation is adjusted to make the distribution narrower, and
     put more mass in the tails!

** One-way ANOVA

   - For comparing >2 algorithms.
   - Why not just do pairwise hypothesis tests?
     - The results may not be consistent (i.e. transitive)
     - Multiple hypotheses result in lost confidence, so you'd need to correct
       your P-value/confidence interval.
       - EG: with 10 95% CI's, you only have 60% confidence that the true values
         of all 10 parameters are within the range.
   - ANOVA looks at the "between means" variance.
   - Essentially, it seems like a generalized $t$ test (gives the same result as
     a $t$ test for two distributions).

** Sign Test

   - Simpler than $t$ test with fewer assumptions.
   - For each fold, note which algorithm had better performance.
   - Use binomial null hypothesis, where p=0.5.

** Mann-Whitney-Wolcoxon Signed-Rank Test

   - What if a classifier produces confidence estimates?
   - If we can rank the predictions, we can calculate a $U$ statistic based on
     the ranks:
     \begin{equation}
       U_1 = \sum_i R_{1,i} - \frac{n_1(n_1 + 1)}{2}
     \end{equation}
   - For large enough samples, you can approximate $U$ with a normal
     distribution.
   - AUC is actually a normalized version of $U$!

** Bootstrap

   - All previous methods relied on knowing the sampling distribution of the
     statistic we are interested in.
   - The bootstrap is a procedure where we get the properties of the statistic
     using /empirical resampling/ from the observations.

   Example:
   - Suppose we have a set of iid examples and we want to get a C1 for F1 score.
   - Repeatedly draw an equal sized sample (with replacement) from our test
     examples, and measure F1.
   - This creates an empirical sampling dsitribution.
   - Then, go back to the original data, measure F1, and ask how unusual that is
     in the empirical distribution.

   Weird... ¯\_ (ツ)_/¯

   Pros: very easy, few assumptions, good for complex things.

   Cons: finite sample behavior is not very well understood.

** Is there a best learning algorithm?

   - No
   - No Free Lunch theorem!
     - In the expectation over all learning algorithms, they will perform
       equally.
     - Wolpert 1996: "The lack of a priori distinctions between learning
       algorithms."
     - For any specific application, you can have a "best" algorithm.  But
       overall, no.

* 2015-09-22 Tuesday

** ANNs, Continued

*** Cascade Correlation (Learning the Structure of an ANN)

    - No textbook sections on this, ask for paper.
    - Start with a single perceptron.  Train and find "residuals".
    - Now, add a new perceptron that feeds into the original one.
    - Train it to feed the "residuals" into the original perceptron.
      - Hold the original perceptron's training constant.
    - Continue adding perceptrons that correct for the "residual" of the
      previous iteration.
    - This essentially does a Taylor series approximation of the underlying
      function.
    - It is an instantiation of a more general technique called "gradient
      boosting".

*** Interpretation of Hidden Units

    - Unlike Decision trees, the ANN structure is very opaque.
    - Difficult to interpret what it is doing.
    - One way is to look at the last layer of the ANN (the last perceptron) and
      see what it's doing.  You could even assign labels to each of the inputs
      for whatever concept you may believe they represent.

*** How Many Hidden Units?

    - Some work shows that it is better to start with a network that is too big.
      - Train until the error on the validation set grows.
      - Then look at the weights associated with edges, and prune the hidden
        units that don't actually contribute.

*** Recurrent ANNs

    - So far, we've looked at ANNs that feed forward.
    - There are also networks with loops, called "recurrent neural networks"
      - This gives ANNs a "memory" of previous inputs.
      - They are much more of a dynamic structure
    - A recurrent ANN architecture with /rational weights/ has computational
      power equivalent to a Universal Turing Machine!!!!!
      - However, this is ridiculously hard to train (ya don't say...)
      - Very prone to overfitting.

*** Pros/Cons of ANNs

    Pros:
    - Very expressive hypothesis space
    - Very useful for classification, regression, density estimation
    - Builds useful representations "automatically"

    Cons:
    - Easy to overfit.
    - Slow to train, require many examples.
    - Doesn't easily handle nominal data.
    - Opaque

** Comparing Learning Algorithms

*** Issue 1
    - Suppose we collect test data and evaluate a classifier.  Accuracy=$x$.
    - Then, someone repeats the experiment with another set of test data from the
      same problem, independent of the first set.
      - What can we say about the accuracy here?

*** Issue 2
    - Suppose now we have two different classifiers $A$ and $B$.  We measure
      their accuracies on a test set, and get $x$ and $y$, and $x > y$.  Does
      this mean $A$ is better than $B$ in this problem?
    - Or how about doing this with completely different algorithms?
      - If we repeated this experiment, we would get new $x'$ and $y'$.  Would
        we find that $x' > y'$ again?

*** It's All Just Statistics!

    - We're just looking at estimating the "true value" of a metric on the basis
      of a small sample.
    - Just like statistics!
    - *Definition:* Data Distribution: assume there is an unknown, underlying
      probability distribution, $D$, from which /unlabeled/ examples ($x$) are
      being sampled without replacement.
      - I.I.D.
    - *Definition:* Sample Error Rate: The fraction of examples in our test
      sample on which the learned classifier disagrees with the target concept.
      \begin{equation}
        e_s = \frac{1}{n} \sum_x \delta(y_x, \hat{y}_x)
      \end{equation}
    - *Definition:* True Error Rate: The probability that the learned classifier
      will make a mistake on a random example drawn from $D$.:
      \begin{equation}
        e_D = Pr_{x~D}(y_x \ne \hat{y}_x)
      \end{equation}
    - For problem #1, we want to know how are $e_S$ and $e_D$ related.
    - *Definition:* Sampling Distribution:
      - Suppose we perform a random experiment lots of times and record the
        outcome.
      - Call the random variable associated with the outcome $O$.
      - Suppose we then plot a frequency histogram of $O$.
      - something something something (see slides)
    - We'd like to get at the sampling distribution of the "error rate" r.v.,
      but we'll start with something easier.
    - Let $R$ be an rv denoting the number of errors in an evaluation
      experiment: (he changed slides too quick)
      - Sampling Distribution of $R$
        - Suppose we run $k$ experiment with test samples of size $n$
        - In the \(i\)th experiment our learned classifier makes $R=r_i$ errors.
        - We'll pot a frequency histogram of $R$.
        - What will it look like for $k$ large?
        - We have a Binomial distribution.  In the limit, this actually
          converges to a normal distribution.
        - This means we can infer the error rate $e_D$ (since $\mu=np$, $\sigma =
          ne_D(1-e_d)$)
      - If we do one trial and find that there are $r$ errors on $n$ examples, a
        good parameter estimate for $e_D$ is $\frac{r}{n}$.  Why?
      - This is a maximum likelihood estimation.  It is the parameter that
        maximizes the probability of the data.
    - *Definition:* Estimation Bias: Estimation bias of an estimator $Y$ for
      parameter $p$ is $E(Y)-p$.
      - If it has 0 bias, it converges asymptotically to the true value.
      - MLE has 0 estimation bias.
    - This is getting to some good math, but I can't summarize it in my notes
      right now if I want to understand it.  See slides.
    - Summary for Issue 1:
      - Determine sampling distribution of measure.
      - Estimate sampling distribution parameters using MLE on test set.
        - If necessary, approximate using standard distribution such as
          Gaussian.
      - Use tables to determine C% CI.
        - Usually use C=95
        - The true measure will lie in that interval with C% probability.

* 2015-09-17 Thursday

** Tradeoffs of Neural Networks

   - Lots of DoF!
     - Topology
     - Parameters
   - Easy to overfit.

** Training ANN

   We'll pretend that the network topology is already decided.  Here is the
   setup:

   \begin{equation}
     D = \left( \begin{array}{ccccc}
           x_{11} & \cdots & x_{1n} & -1 & y_1 \\
           \vdots & & \vdots & \vdots & \vdots \\
           x_{m1} & \cdots & x_{mn} & -1 & y_m
         \end{array} \right)
   \end{equation}

   - Want to find parameters $\vec{w} = (w_1, w_2, \cdots, \sigma)$.
   - Such that we minimize the "loss" function $L(\vec{w})$.
   - We can't use the sign function because it's not differentiable.
   - We can't use the dot product approximation.
   - Instead we use a sigmoid function $y = (1 - e^{x})$ I think.

   To train, we use Backpropagation!  This is gonna be fun.
   - Feed examples forward through the network.
   - Do layer-wise gradient descent starting at the output layer.

*** Backpropagation

    - Let $x_{ji}$ be the ith input to unit j.
    - Let $w_{ji}$ be the parameter associated with $x_{ji}$.
    - Let $n_j = \sum_i something$
    - Next up is the derivation of the derivative of the loss function for the
      output layer.  It's easy to follow, and I can't keep up with typing the
      math.  Check the slides!

    Backpropagation for hidden layers.
    - A perceptron $j$ only can affect the output from its downstream
      perceptrons, which we denote as $Downstream(j)$.
    - We can compute the derivative of the loss function with respect to the
      inputs of this perceptron, $\frac{dL}{dn_j}$, by computing the sum of
      $\frac{dL}{dn_k} \frac{dn_k}{dn_j}$ for all the $k\in{}Downstream(j)$.
      Excitingly, we already have $\frac{dL}{dn_k}$, since $k$ is dowstream of
      $j$,
    - The math is on the slides again, cause I'm not typing this stuff.  Still
      pretty easy to follow.

*** Example

    Consider a neural network with 2 input units, 2 hidden units, and 1 output
    unit, and all weights initialized to 1, with the bias set to zero.  Using
    squared loss, show the weights after the first backpropagation update with
    these examples.

    We have the inputs labelled 1 and 2, and then the two internal nodes labeled
    3 and 4, and the output node labeled 5.  Weights and x's are labeled
    accordingly.

    | x_1 | x_2 | f | $\hat{f}$ |
    | 0  | 0  | 0 | 0.731     |
    | 0  | 1  | 1 | 0.812     |

    Now that we have the initial outputs of the network, we can compute the
    derivatives for each example, and once we have all the derivatives we add up
    all the derivatives and compute the next step.

    Example 1:
    - Output layer: \(\frac{dL}{dw_{53}} = (0.731) (1 - 0.731) (0.5) (0.731 - 0) = 0.0719\)

    Example 2:
    - Output Layer: \(\frac{dL}{dw_{53}} = (0.812) (1 - 0.812) (0.731) (0.812 - 1) = -0.021\)

    Update:
    - \(w_{53}' = 1 - \eta (0.0719 - 0.021) = 0.949\) (assuming $\eta = 1$ for example).

** Overfitting in ANNs

   - They are very prone to overfitting, due to the large amount of parameters.
   - Can create very nonlinear decision surfaces.
   - You can impose a simple structure on the network, but then the network may
     not be capable of representing the true decision boundary.
   - Some strategies for controlling overfitting:

*** Weight Decay

    Add a "weight decay term" to keep the weights from growing:

    \(L_{OC}(\vec{y}, \hat{\vec{y}}, \vec{w}) = L(\vec{y}, \hat{\vec{y}}, \vec{w}) + \gamma \sum_i \sum_j w_{ji}^2\)

    If you have a large $\gamma$, your solution will tend to $w_{ji}$'s will tend toward
    zero, to minimize the effect of $\gamma$.  So it seems careful choice of $\gamma$ is
    pretty important.

** Implementation Issues

   You should standardize your inputs to zero mean, unit variance, so that your
   units don't have a massive effect on the network.

   Nominal features: you need to re-encode it.  You could do 1 of N input units.
   Or you could do logarithmic encoding, where each input is a binary code.

* 2015-09-15 Tuesday

** Famous Dead People

   - George Boole - father of Boolean algebra.
   - Someone else - neuroscience.
   - Frank Rosenblatt (may not be dead) - artificial neurons.

** History

   - We want "artificial intelligence."
   - Human brain is intelligent.
   - Try to simulate the structure of the brain to achieve intelligence

** Perceptron / Linear Threshold Unit

   - Has weighted ($w_i$) inputs ($x_i$).
   - Has Activation Threshold $\sigma$
   - Activation function is:

     \begin{equation}
       h(\vec{x}; \vec{w}, \sigma) = \left\{
       \begin{array}{ll}
         +1 & \text{if } \vec{w} \cdot \vec{x} \ge \sigma \\
         -1 & \text{else} \\
       \end{array}
       \right.
     \end{equation}

   - The parameters of the perceptron are $\vec{w}$ and $\sigma$.
     - There aren't really parameters of the decision tree algorithm, just the
       structure of the tree.

   - Example evaluation for perceptron $\vec{w}=(1,2)$, $\sigma=0.5$:

     | $x_1$ | $x_2$ |  h |
     |    0 |    0 | -1 |
     |    0 |    1 |  1 |

   - So, the question remains, how do we train them?

*** Training a Perceptron

    - Loss function: $L(\vec{w},\sigma)$
    - Measures the difference between the current estimates of $y$ ($\hat{y}$),
      and the true $y$ (which is known), over all training examples.
    - Our goal is to minimize the loss function with respect to $(\vec{w}, \sigma)$.
    - Notations:
      - Training data: (he changed the slide too quick)
    - Common loss function is "squared loss":
      \begin{equation}
        L(\vec{w}) = \frac{1}{2} \sum_{i=1}^m (y_i - \hat{y}_i)^2
                   = \frac{1}{2} \sum_{i=1}^m (y_i - sign(\vec{w}\cdot\vec{x}_i))^2
      \end{equation}
    - Sign function is not differentiable, so we'll replace it by dot product.
      \begin{equation}
        L(\vec{w}) = \frac{1}{2} \sum_{i=1}^m (y_i - \vec{w}\cdot\vec{x}_i)^2
      \end{equation}
    - Calculate gradient wrt $\vec{w}$
      \begin{equation}
        \frac{dL}{d\vec{w}} = \sum_{i=1}^m (y_i - \vec{w} \cdot \vec{x}_i)(-\vec{x}_i)
      \end{equation}
    - Parameter Update:
      \begin{equation}
        \vec{w} \gets \vec{w} - \eta \frac{dL}{d\vec{w}}
      \end{equation}
    - We can use gradient descent
      - Loss function is differentiable.
      - Loss function is bounded below by 0.
      - Loss function is convex (proof???)
      - This means there is a well-defined minimum for the loss function.
      - And, gradient descent will find it!
    - However, just cause the gradient descent converges, doesn't mean that it
      will converge to 0, since the true concept is not necessarily linear.
    - Stochastic G.D:
      \begin{array}{l}
        \frac{dL}{d\vec{w}} = (y_i - \vec{w} \cdot \vec{x}_i)(-\vec{x}_i) \\
        \vec{w} \gets \vec{w} - \eta \frac{dL}{d\vec{w}}
      \end{array}
      - This is done for each example instead of as a group.
      - Since the loss function is convex, it will converge to the same thing in
        the limit.
      - But the stochastic procedure will procede differently and maybe converge
        at a different speed.
      - Stochastic seems to give initial examples more "weight" in the direction
        of the search.
      - Stochastic is better for "online" learning, and for very large datasets.

*** More on Perceptrons

    - Geometry of the perceptron:
      - In one dimension, it is a step function.
      - In two dimensions, the separating surface is a line.
      - In three dimensions, the separating surface is a plane.
      - So, in general, the decision surface is a hyperplane.
    - Loss function is 0 when the surface completely separates the examples with
      no errors.  It is non-0 when there are some wrong ones.
    - Linear separability is whether or not a dataset can be separated by a
      linear function without error.
    - The perceptron is not nearly as powerful as a decision tree (can't
      separate things like exclusive or).
    - So, it is more resistant to overfitting.  (which we will quantify later)
    - It can do some logic:
      - Conjunctions:
        \begin{array}{l}
          x_1 \land x_2 \land x_3 \leftrightarrow y \\
          1 \cdot x_1 + 1 \cdot x_2 + 1 \cdot x_3 \ge 3
        \end{array}
      - At least $m$-of-$n$:
        \begin{array}{l}
          (x_1 \land x_2) \lor (x_1 \land x_3) \lor (x_2 \land x_3) \leftrightarrow y \\
          1 \cdot x_1 + 1 \cdot x_2 + 1 \cdot x_3 \ge 2
        \end{array}
    - But not all:
      - Complex disjunctions
      - Exclusive or!!
    - Can fix this by using more perceptrons hooked up to each other.
    - The neural network for exclusive or looks remarkably similar to the logic
      gate circuit for XOR :D
    - It involves a "hidden" layer that isn't part of the output.

*** Feedforward Network Topology

    - Essentially, a directed acyclic graph of perceptrons.
    - But, it may be that you have to follow the layer structure.
    - Representation ability
      - Every boolean function can be represented by a network w/ one hidden
        layer.
      - Every bounded continuous function can be represented by a network with
        one hidden layer.
      - Every function in R^n can be represented by a network with two hidden
        layers.
      - Woah.
    - This gives you a tradeoff...
      - You end up with the possibility for a lot of overfitting (many degrees
        of freedom and high representation ability).
      - It also takes a long time to train these networks if they are complex.

* 2015-09-10 Thursday

** Evaluation Methods and Metrics

   How do you figure out if your algorithm is "good"?

   Goal: find a measure *expected future performance* of the learning algorithm
   for some problem.  How?

   Idea:
   - Separate available data into sets for training and evaluation.
   - The examples for evaluation will be new to the learned classifier.
   - Do this lots of times to get reliable estimates.
   - The sets should be "separate" at least in the sense of independently
     chosen, if not disjoint examples.

*** n-fold Cross Validation

    - Generally, the number of examples is limited.
    - Want to train on sets that are as large as possible.
    - Divide set into $n$ separate sets.
      - For each set, withhold it for testing, and train on the remaining sets.
      - Then evaluate the classifier on the testing sets.
    - Special case of $n$-fold cross validation: Leave-one-out
      - $n$ examples, $n$ folds.
      - Only really useful if you have a few examples.
      - Called "jackknife" in statistics literature.
    - Stratified cross validation
      - Same as $n$-fold cross validation, but you sample folds such that the
        proportions of class labels is preserved in each fold.
      - More stable performance estimates.
      - Implementation:
        - Put $pos$ positive examples in one list, and $neg$ negative examples
          in another.
        - Randomly shuffle the lists.
        - Put the first $pos/n$ positives in fold 1, the next into fold 2, etc.
        - Repeat for negatives.
        - Assign leftover examples randomly.

*** Metrics for Classification

    Contingency Table

    |              | Positive (TC)                | Negative (TC)                |
    | Positive (C) | True Positive (TP)           | False Positives (FP, Type I) |
    | Negative (C) | False Negative (FN, Type II) | True Negative (TN)           |

    Can compute all metrics from the contingency table.

    - Accuracy: most commonly used measure for comparing algorithms.
      \begin{equation}
        \text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}
      \end{equation}
      - Simply the fraction of examples that are correctly classified.
      - There are many problems with accuracy.
        - Skewed class distribution: eg, if 99% animals aren't lions, a
          classifier with 99% accuracy would just predict "not lion".  And it
          would kill you next time you see a lion.
        - Differential misclassification costs: some types of errors (FP or FN)
          are more serious for an application than others (eg screening for a
          disease).  Accuracy treats them equally.
    - Weighted Accuracy
      \begin{equation}
        \text{WAcc} = \frac{1}{2}\left(\frac{TP}{Allpos} + \frac{TN}{Allneg}\right)
                    = \frac{1}{2}\left(\frac{TP}{TP + FN} + \frac{TN}{TN + FP}\right)
      \end{equation}
      - First part is the "true positive rate" (how many positives are correctly
        identified)
      - Second part is the "true negative rate" (how many negatives are
        correctly identified)
    - Precision
      \begin{equation}
        \text{Precision} = \frac{TP}{TP + FP}
      \end{equation}
      - Sometimes, the "positive" case is all you're interested in.
      - This measures "of all the examples classified positive, how many were
        actually positive?"
    - Recall / True Positive Rate / Sensitivity
      \begin{equation}
        \text{Recall} = \frac{TP}{TP + FN}
      \end{equation}
      - This quantifies "of all the positive examples, how many were correctly
        classified?"
    - Specificity
      \begin{equation}
        \text{Specificity} = \frac{TN}{TN + FP}
      \end{equation}
      - Conterpart of recall for the negative class.
    - F1
      \begin{equation}
        \frac{1}{F1} = \frac{1}{2} \left( \frac{1}{Precision} + \frac{1}{Recall}\right)
      \end{equation}
      \begin{equation}
        F1 = \frac{2}{\left( \frac{1}{Precision} + \frac{1}{Recall}\right)}
      \end{equation}
      - Combines precision and recall into single measure.
      - Not necessarily a good idea, but widely used.

*** Learning Curves

    - Frequently it's useful to plot metrics as a function of sample size.
    - Provides insight into how many examples the algorithm needs to be
      effective.

*** Metrics with Confidence Measures

    - Many learning algorithms produce classifiers or models that provide
      estimates of how confident they are.
    - Can use this to create Precision/Recall curves or Receiver Operator
      Characteristic curves.
    - Precision/Recall curves:
      - plot precision, recall as you change threshold.
    - ROC graphs
      - plot FPR x , TPR y as you change threshold.
      - Random guessing is a diagonal line.
        - Also majority class classifier.
        - Good classifier mst be above the diagonal.
      - Monotonically increasing.
      - Can be misleading if class distribution is too skewed.
        - Use PR instead.
      - Frequently use AUC as statistic.
* 2015-09-08 Tuesday

** Review:

   - Decision trees: trees where internal nodes are tests on attributes, and
     leaves are class labels.
   - Construct them by choosing attributes which give the most information.
   - Measure this information with entropy, mutual information ("information
     gain").
   - ID3 algorithm is the formal algorithm for applying mutual information to
     constructing decision trees.

** Generalizing ID3

   - What about multiple valued attributes (more than 2-valued)?
     - Mutual information still applies to $v$-valued finite, discrete
       variables.
     - You simply have the internal node for that attribute have $v$ children
       instead of 2.
     - However, the maximum mutual information for a $k$ valued variable is
       $\log{k}$, so the IG function is biased towards attributes with many
       values.
     - Can normalize by dividing by $H(X)$, the entropy of the attribute itself.
       - *Question:* why is this better than dividing by $\log{|X|}$, e.g., the
         maximum overall entropy of $H(X)$?
       - In essence, this division gives you a quantity that answers the
         question "what fraction of this variable's entropy contributes
         information about the class label?"
   - Continuous Attributes
     - Continuous variables have entropy defined on them, but it's useless for
       making a decision in a tree.
     - Need to "bin" the attribute ($X \le v$ or $X \ge v$).
     - You only need to consider values for $v$ that separate different class
       labels in the training set.
       - This is still problematic for large training sets, as we'll see on our
         programming assignment.

   Example

   | Color | Area | Shape    | Class Label |
   | red   |  0.1 | circle   |           1 |
   | red   |  0.7 | square   |           0 |
   | red   |  0.4 | triangle |           1 |
   | blue  |  0.2 | triangle |           1 |
   | blue  |  0.6 | circle   |           0 |
   | blue  |  0.8 | square   |           0 |
   | green |  0.4 | square   |           0 |
   | green |  0.3 | triangle |           0 |
   | green |  0.3 | circle   |           0 |

   1. First, compute H(Y), which is $H(\frac{1}{3})$ (as a shorthand).
   2. Then, compute H(Y|Color):

      \begin{equation}
      H(Y|Color) = p(Color=red)H(Y|Color=red) + p(Color=blue)H(Y|color=blue) + p(Color=green)H(Y|Color=green)
      \end{equation}

      \begin{equation}
      H(Y|Color) = \frac{1}{3}H(\frac{1}{3}) + \frac{1}{3}H(\frac{1}{3}) + \frac{1}{3}\times 0
      \end{equation}

      \begin{equation}
      H(Y|Color) = \frac{2}{3}H(\frac{1}{3})
      \end{equation}

   3. We can use this to compute the information gain of Color.

      \begin{equation}
      IG(Color) = H(Y) - H(Y|Color) = \frac{1}{3} H(\frac{1}{3})
      \end{equation}

   4. Conveniently, this is the same as the information gain of Shape.

   5. For area, if we sort the training set by Area, we find the cutoffs 0.25,
      0.35, and 0.5.  Then we can compute H(Y|Area,v) for each cutoff v.

      $H(Y|Area\le0.25) = \frac{2}{9}\times 0 + \frac{7}{9} H(\frac{1}{7})$, so IG(Area\leq 0.25) = 0.4583

      etc for each cutoff

   6. You choose the best IG, and use that for the root node.  Then continue to
      do this for each child node.

** Overfitting

   - Given enough features, ID3 will create a tree that fits your data perfectly.
     - Enough features = enough that there are no contradictory examples.
   - Overfitting is an issue.

   - What is overfitting?  Making your model too specific to your training
     examples, and not general enough to be applied well to new data.

   - Strictly, if a concept $h$ has:

     - Higher performance on the training examples, but
     - Lower performance on the whole dataset

   - Than some other concept $h'$, then we say that $h$ has overfit the training
     data.

*** Controlling Overfitting

    - Can introduce a restriction on the hypothesis space, to prevent overly
      complex hypotheses from being learned.
    - Early Stopping
      - Standard ID3 algorithm stops when IG(X)=0 for all X.
      - Instead, stop when IG(X) \leq \epsilon, for some chosen \epsilon.
      - This is sensitive to your parameter choice for \epsilon.
      - It's easy to implement, but doesn't work well in practice.
    - Greedy post-pruning
      - Hold aside some training examples at the start.
      - Do your training procedure on the remainder (allowing it to overfit if
        it wants).
      - Then, do a /greedy pruning/ algorithm on your model.
* 2015-09-01 Tuesday

  HW1 due tonight at midnight.  HW 2 out today.  Read Ch. 3 in Mitchell.

** What is "Machine Learning?"

   - Machine = autonomous system, with no (or limited) human intervention.
   - Learning?
     - System changes after an experience, so that it can work more effectively
       next time it does the task.
     - We want the system to learn how to do /related/ tasks better too.
   - Specification for a learning system:
     - Given: Task goal, performance measure P, and examples E
     - Produce a *concept* that is good wih respect to P on /all/ examples of
       the task.
   - Example: learn to play chess
     - Perforance measure = games won/lost
     - Examples = games played
     - Concept?  Probably a function mapping a current board state to a move to
       play next.
   - Two phases: learning/training, and evaluation/testing
     - (In the evaluation phase, you want to evaluate on new examples that you
       haven't trained on).
   - Batch learning: one learning phase, with a large set of examples, followed
     by a testing phase.
   - Online learning: examples arrive one at a time (or in small groups);
     learning and evaluation phases iterate.
   - Learning systems need to have some sort of constraint.  Memorizing all the
     examples is probably the best strategy, but we know that this doesn't
     represent learning the underlying concept.

*** Inductive Generalization

    - In all learning problems, need to reason from specific examples to a
      general case.
    - (this is the reverse of deductive reasoning, where you reason from the
      general case to the specific case)
    - Target concept = the underlying concept that the system is trying to
      learn.  EG, Gary kasparov's head.
    - Typically, the performance measure quantifies the difference between
      current and target concepts.
    - Hypothesis space - all concepts the learning system will consider
      (e.g. all possible combinations of animal properties)
    - Hopefully, target concept is in the hypothesis space.
      - But can't include every possible hypothesis in your space.
      - The size would be huge.
      - You would end up memorizing, not learning.
    - This is the idea behind "No Tabula Rasa" (blank slate) learning.  There
      has to be some sort of restriction on hypothesis spaces.
    - Inductive Bias
      - Assumptions used to limit the hypothesis space are the inductive bias.
      - The more assumptions, the stronger the bias.
      - It can even be quantified (later)

*** Learning Settings

**** Supervised Learning

     - Examples are annotated by a teacheer or oracle.
     - Learning system just finds the concept to match the annotations.

**** Unsupervised Learning

     - No annotations
     - Goal is to find interesting patterns in the examples
     - System defines what is interesting.
     - Example: grouping images by content.

**** Semi-Supervised Learning

     - "*normal learning*" is really a combination of the two
     - You do unsupervised learning, and you occasionally get your
       "parent"/oracle to come in and teach you some labels.
     - You use those new concepts to help you organize your thoughts better.

**** Active Learning

     - A few examples are annotated with the target concept.
     - Learning system can "ask" the oracle to label something.
     - There is a cost of labelling that the system must optimize.

**** Transductive Learning

     - Learning system has some knowledge of possible examples it will be
       evaluated on.
     - Adjusts the system to do better on those examples.
     - EG - learn to play chess against Kasparov.

**** Reinforcement Learning

     - This is "sequential" learning.
     - Your environment provides feedback.
     - You take actions and use the consequences to learn.

**** Transfer Learning

     - Human learning is cumulative.
       - When we encounter a new problem, we don't just start from scratch.
       - We use prior knowledge and reasoning.
     - Transfer learning attempts to apply concepts learned in other problems to
       bias your search.

** When to use ML?

   - Shouldn't use ML to recognize geometric shapes.
   - In general, you don't need to learn if you have these things:
     - The concept is already accurately known.
     - It can be easily (and compactly) described
     - Unlikely to change
   - Learning is not free, requires computation and storage, and real world
     effort in labeling, etc.

** Example Representations

   - Internal representation of examples effects how you learn.
   - EG: When you recognize objects, you don't do it at the level of signals on
     your optic nerve.  You do it at the level of smaller parts that you've
     learned.  A chair has four legs, a flat surface, and usually a back.
   - In the same way, pixels aren't useful in object recognition.
   - This is an open area of research: we don't always know the best
     representation of examples.

*** Feature Vector Representation

    - Examples are vectors of values for a set of attributes.
    - Can be an n-by-m matrix

      |      | Attr 1 | Attr 2 | Attr 3 |
      | EG 1 | v_11    | v_12    | v_13    |
      | EG 2 | V_21    | V_22    | v_23    |
      | EG 3 | v_31    | v_32    | v_33    |

    - This is also called "propositional representation", because each example
      can be a logical conjunction.
    - Can represent all the examples as logic formula.

*** Relational Representation
    - Can use first order logic.

*** Multiple Instance Representation
    - Examples are represented by arbitrary sized sets of attribute-value pairs.
* 2015-08-27 Thursday

** Optimization

*** What is it?

    Find the extreme points of an objective function.

*** Types of Optimization Problems

    - Discrete vs Continuous - objective function is defined on discrete or
      continuous space.
    - Unconstrained vs constrained - whether there are additional constraints
      defining the feasible region.
    - In this class, we are interested in continuous problems, constrained and
      unconstrained.  We use tools from calculus and linear algebra.

*** Unconstrained Optimization

    - Function of one variable, eg minimum of x^2.  Typical method for solving
      this is to compute first and second derivative, find zeros of first
      derivative where second derivative is positive.
    - Fuctions of two variables, you find the same things, but in matrix form:
      - Jacobian \(J = (\frac{\delta{}f}{\delta{}x_i}) = 0\)
      - Hessian \(H = [\frac{\delta^{2}f}{\delta{}x_{i}\delta{}x_j}] > 0\) must be
        positive definite.
    - Can't always do this, due to computational constrains, and due to weird or
      unknown function.

*** Gradient Ascent

    A way of maximizing/minimizing a function.  From your current position
    $\vec{x}$, go in the direction that maximizes the increase.

    \(\vec{x}_{new} = \vec{x}_{old} - \alpha \Delta f_{\vec{x}_old}(\vec{x})\)

    Here, \alpha is the step size, and \Delta f is the function gradient
    evaluated at x_{old}.

    Downside of this is that the convergence rate is not very good.  Also, this
    procedure assumes linearity, where a quadratic function may be a better
    approximation.

*** Newton-Raphson Method

    In this, we use a quadratic approximation of f.  Then, instead of taking a
    linear step, we take a "Newton step".

    \(f(\vec{x}_{old} + u) = f(\vec{x}_{old}) + u^T \Delta f_{\vec{x}_{old}}(\vec{x}) + \frac{1}{2} u^T \Delta^2f_{\vec{x}_{old}}(\vec{x})u = g(u)\)

    More math, see slides.

    Properties:
    - Fast convergence close to solution.
    - Not guaranteed to converge if started far from solution, may cycle or
      diverge in this case.

*** Quasi-Newton Methods

    - Often, constructing the Hessian for a multivariate function is
      computationally difficult, because it takes O(n^2) space and time and has
      to be done over and over.
    - So, a number of methods exist that approximate the Hessian by using the
      Jacobian at nearby points.

*** Local and Global Optima

    - A *global minimum* for a function is a point x where f(x) \leq f(x+u) for
      all u.
    - A *local minimum* is an x where f(x) \leq f(x+u) for all |u|<\epsilon, for
      some positive \epsilon.
    - Every global minimum is a local min, but not the other way around.
    - There is no algorithm that is guaranteed to find the global maximum of an
      arbitrary function.

*** Convex Sets

    Take two points x_1 and x_2.  A point on the line segment between them is
    defined by \lambda x_1 + (1-\lambda) x_2, for 0 \leq \lambda \leq 1.

    A Convex Set is a set of points such that for any two points in the set,
    \lambda x_1 + (1-\lambda) x_2 is also in the set (for 0 \leq \lambda \leq
    1).  Basically, you can visualize these sets on the plane as "shapes that
    don't have holes in them".

*** Convex Functions

    If you look at all the points that are "above" a function - {(x,y)|y \geq
    f(x)}, if that set is convex, then f is a convex function.

    JENSEN'S INEQUALITY (yaaaaaaas)!

    f(\lambda x_1 + (1-\lambda) x_2) \leq \lambda f(x_1) + (1-\lambda) f(x_2)

    Jensen's inequality seems to apply for any convex function.  It just says
    that the points on the segment between f(x_1) and f(x_2) have to be above
    the the function itself.  Pretty cool.

    For a convex function, every local optimum is also a global optimum!  That's
    a pretty nice property to have.

*** Constrained Optimization

    - Minimize a function of x such that some constraints on x are satisfied.
      The constraints define a feasible region on of in which the solution must
      lie.

*** Linear Programming

    Linear Programming is a *special case* of *constrained optimization*, in
    which both the objective function and the constraints are linear!
    Typically, we write all the constraints and objective function as functions
    of matrices and vectors, for compactness.

    When you apply all these linear constraints, you have a feasible region that
    is a "polyhedron" (because it is bounded by a bunch of "hyperplanes").  It's
    possible that one side of the feasible region is open, (so not completely
    bounded).

    If you have a linear objective function, you can say for certain that an
    optimal point is on one of the vertices.

*** Simplex Algorithm

    - Around the polyhedron we go.
    - From any feasible vertex, walk along the edges of the polyhedron,
      following the vertices.
    - Once you are at a vertex where the neighboring vertices have higher f
      values, stop.
    - You've found a local optimum, which happens to be a global optimum since
      the linear function is convex.

    Properties of this algorithm:

    - Very simple, and easy to implement, and works well in practice.
    - It works by traversing vertices, and there may be exponentially many
      vertices for n constraints.  So, in the worst case, runtime is
      exponential.
      - Average case under various distributions has been shown to be
        polynomial, which is useful.
    - Other algorithms exist, such as "interior point methods", which have
      polynomial bounds*

*** Duality in Linear Programming

    From any "primal" LP, we can derive a "dual" LP.  Say we have a primal LP:

    - min_x c^T x, such that
    - A x \geq b
    - x \geq 0

    We could create a dual like this:

    - max_u b^T u, such that
    - A^T u \leq c
    - u \geq 0

    The nice properties of this are:

    - The primal has a solution iff the dual has a solution.
    - Further, the dual LP is a lower bound on the primal LP.
      - That is, if we pick any feasible x and any feasible u, we always havve
        c^T x \geq b^T u.
    - From the relationship between primal and dual LPs, we can derive a set of
      conditions that characterize the solutions for a primal/dual pair, called
      the Karush-Kuhn-Tucker conditions.
    - Essentially, the conditions are that at the optimal solution, x and u are
      feasible and the objective functions c^T x and b^T u are equal (and some
      other stuff).
    - Soumya says if this doesn't make sense now, that's ok.  Which is good,
      because he lost me at the dual being a lower bound on the primal.

*** Summary of Optimization

    - Types of optimization problems.
    - Unconstrained optimization - gradient ascent/descent, Newton Raphson
      methods.
    - Convex sets and functions
    - Constrained optimization:
      - Linear programming
      - Simplex method
      - Duality
      - KKT conditions

** The Simplex Algorithm

   He says we should know how it works.

   Let us consider the following linear program:

   - minimize (with respect to x_1, x_2) f(x) = 3x_1 - 6x_2, such that
   - x_1 + 2x_2 \geq -1
   - 2x_1 + x_2 \geq 0
   - -x_2 + x_1 \geq -1
   - -4x_2 + x_1 \geq -15
   - -4x_1 + x_2 \geq -23
   - x1, x_2 \geq 0

   Steps:
   1. Standardize so everything is in [variables] \geq [constant] form.
   2. Introduce "slack variables".  Essentially, these are the gap in the
      conditions.  These have to be greater than or equal to 0:
      1. x_3 = x_1 + 2x_2 + 1
      2. x_4 = 2x_1 + x_2
      3. x_5 = -x2 + x_1 + 1
      4. x_6 = -4x_2 + x_1 + 15
      5. x_7 = -4x_1 + x_2 + 23
   3. We can put this stuff into tableu form:

      |     | x_1 | x_2 |    |
      | x_3 |   1 |   2 |  1 |
      | x_4 |   2 |   1 |  0 |
      | x_5 |   1 |  -1 |  1 |
      | x_6 |   1 |  -4 | 13 |
      | x_7 |  -4 |   1 | 23 |
      | 2   |   3 |  -6 |  0 |

   4. Assume that zero is feasible.  Pick the variable that will decrease the
      objective function (the most?), and change it accordingly.  In this case,
      we choose x_2.  Then, we write out the constraints, holding x_1 to be 0.
      We find the smallest positive constraint value for x_2, and choose that.
      Whatever variable caused that constraint, we swap it with x_2, and make a
      new tableau.

      In this case, x_5 is the blocking constraint, so we pick it.

      |     | x_1 | x_5 |  1 |
      | x_3 |   3 |  -2 |  3 |
      | x_4 |   3 |  -1 |  1 |
      | x_2 |   1 |  -1 |  1 |
      | x_6 |  -3 |   4 |  9 |
      | x_7 |  -3 |   1 | 24 |
      | z   |  -3 |   6 | -6 |

   5. The value of the function is now -6.  We can see that the right variable
      to decrease now is x_1.  So, we do the constraints again.  Here, the
      blocking constraint is x_6, so then we get this tableau:

      |     |  x_6 | x_5 |   1 |
      | x_3 |   -1 |   2 |  12 |
      | x_4 |   -1 |   3 |  10 |
      | x_2 |  1/3 | 1/3 |   4 |
      | x_1 | -1/3 | 1/3 |   3 |
      | x_7 |    1 |  -5 |  15 |
      | z   |    2 |   1 | -15 |

      The stopping condition is when both variables on top of the columns have
      coefficients that are positive, so you can't improve the function value.

   If you have more than one variable that will decrease the function, you can
   choose any variable to decrease, and you will always get to the correct
   solution.  However, some choices will be faster than others.
