<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2015-11-18 Wed 16:24 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title>EECS 477 Notes</title>
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Stephen Brennan" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://orgmode.org/mathjax/MathJax.js"></script>
</head>
<body>
<div id="content">
<h1 class="title">EECS 477 Notes</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline4">1. 2015-11-18 Wednesday</a>
<ul>
<li><a href="#orgheadline1">1.1. "Math Facts"</a></li>
<li><a href="#orgheadline2">1.2. Back to Hitting Set</a></li>
<li><a href="#orgheadline3">1.3. Maximum Satisfiability</a></li>
</ul>
</li>
<li><a href="#orgheadline10">2. 2015-11-16 Monday</a>
<ul>
<li><a href="#orgheadline5">2.1. Hitting Set</a></li>
<li><a href="#orgheadline6">2.2. Primal Dual Algorithm for HS</a></li>
<li><a href="#orgheadline7">2.3. Example</a></li>
<li><a href="#orgheadline8">2.4. Approximation</a></li>
<li><a href="#orgheadline9">2.5. Randomized Approximation Algorithm</a></li>
</ul>
</li>
<li><a href="#orgheadline12">3. 2015-11-11 Wednesday</a>
<ul>
<li><a href="#orgheadline11">3.1. Game Tree Evaluation (cont)</a></li>
</ul>
</li>
<li><a href="#orgheadline18">4. 2015-11-09 Monday</a>
<ul>
<li><a href="#orgheadline17">4.1. Game Tree Evaluation</a>
<ul>
<li><a href="#orgheadline13">4.1.1. Pruned Depth First Search</a></li>
<li><a href="#orgheadline14">4.1.2. Game Theory Approach to Game Tree Evaluation</a></li>
<li><a href="#orgheadline15">4.1.3. Randomized Algorithms</a></li>
<li><a href="#orgheadline16">4.1.4. Worst Case</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline22">5. 2015-11-06 Friday</a>
<ul>
<li><a href="#orgheadline21">5.1. Two Person Zero Sum Games (cont)</a>
<ul>
<li><a href="#orgheadline19">5.1.1. Yao's Principle</a></li>
<li><a href="#orgheadline20">5.1.2. Example Time</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline26">6. 2015-11-04 Wednesday</a>
<ul>
<li><a href="#orgheadline23">6.1. Two Person Zero Sum Games</a></li>
<li><a href="#orgheadline24">6.2. Saddle Points for Mixed Strategies</a></li>
<li><a href="#orgheadline25">6.3. Something Else</a></li>
</ul>
</li>
<li><a href="#orgheadline31">7. 2015-11-02 Monday</a>
<ul>
<li><a href="#orgheadline27">7.1. Dr. Liberatore's Homework</a></li>
<li><a href="#orgheadline30">7.2. Our Homework</a>
<ul>
<li><a href="#orgheadline28">7.2.1. MCNF and Max Flow</a></li>
<li><a href="#orgheadline29">7.2.2. Successive Shortest Paths</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline36">8. 2015-10-30 Friday</a>
<ul>
<li><a href="#orgheadline32">8.1. Games (strategic form)</a></li>
<li><a href="#orgheadline33">8.2. Mixed Strategies</a></li>
<li><a href="#orgheadline34">8.3. Games (Behavioral Form)</a></li>
<li><a href="#orgheadline35">8.4. Two person zero sum game</a></li>
</ul>
</li>
<li><a href="#orgheadline41">9. 2015-10-28 Wednesday</a>
<ul>
<li><a href="#orgheadline38">9.1. Capacity Scaling (cont)</a>
<ul>
<li><a href="#orgheadline37">9.1.1. Runtime</a></li>
</ul>
</li>
<li><a href="#orgheadline39">9.2. Max Flow Min Cut</a></li>
<li><a href="#orgheadline40">9.3. Game Theory!</a></li>
</ul>
</li>
<li><a href="#orgheadline44">10. 2015-10-26 Monday</a>
<ul>
<li><a href="#orgheadline42">10.1. Runtime of Successive Shortest Paths</a></li>
<li><a href="#orgheadline43">10.2. Capacity Scaling</a></li>
</ul>
</li>
<li><a href="#orgheadline47">11. 2015-10-21 Wednesday</a>
<ul>
<li><a href="#orgheadline46">11.1. MCNF Algorithms</a>
<ul>
<li><a href="#orgheadline45">11.1.1. Successive Shortest Paths</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline48">12. 2015-10-16 Friday</a></li>
<li><a href="#orgheadline53">13. 2015-10-14 Wednesday</a>
<ul>
<li><a href="#orgheadline49">13.1. Back to Equipment Replacement</a></li>
<li><a href="#orgheadline50">13.2. Back to Uniform Cost Parallel Machines</a></li>
<li><a href="#orgheadline51">13.3. Midterm Questions</a></li>
<li><a href="#orgheadline52">13.4. Final Exam</a></li>
</ul>
</li>
<li><a href="#orgheadline55">14. 2015-10-12 Monday</a>
<ul>
<li><a href="#orgheadline54">14.1. Homework 3 Review</a></li>
</ul>
</li>
<li><a href="#orgheadline58">15. 2015-10-09 Friday</a>
<ul>
<li><a href="#orgheadline56">15.1. Uniform Parallel Machines</a></li>
<li><a href="#orgheadline57">15.2. Equipment Replacement</a></li>
</ul>
</li>
<li><a href="#orgheadline61">16. 2015-10-07 Wednesday</a>
<ul>
<li><a href="#orgheadline59">16.1. Max Flow Reduction: Matrix Rounding</a></li>
<li><a href="#orgheadline60">16.2. Max Flow Reduction: Job Scheduling</a></li>
</ul>
</li>
<li><a href="#orgheadline65">17. 2015-10-05 Monday</a>
<ul>
<li><a href="#orgheadline62">17.1. Max Flow</a></li>
<li><a href="#orgheadline63">17.2. Feasible Flow Problem</a></li>
<li><a href="#orgheadline64">17.3. Reductions</a></li>
</ul>
</li>
<li><a href="#orgheadline69">18. 2015-10-02 Friday</a>
<ul>
<li><a href="#orgheadline66">18.1. Shortest Paths</a></li>
<li><a href="#orgheadline67">18.2. Dynamic Programming!!!!!</a></li>
<li><a href="#orgheadline68">18.3. Circulation</a></li>
</ul>
</li>
<li><a href="#orgheadline71">19. 2015-09-30 Wednesday</a>
<ul>
<li><a href="#orgheadline70">19.1. MCNF Duality</a></li>
</ul>
</li>
<li><a href="#orgheadline72">20. 2015-09-28 Monday</a></li>
<li><a href="#orgheadline74">21. 2015-09-25 Friday</a>
<ul>
<li><a href="#orgheadline73">21.1. MCNF Duality</a></li>
</ul>
</li>
<li><a href="#orgheadline81">22. 2015-09-23 Wednesday</a>
<ul>
<li><a href="#orgheadline78">22.1. MCNF: Transformations</a>
<ul>
<li><a href="#orgheadline75">22.1.1. Removing lower bounds</a></li>
<li><a href="#orgheadline76">22.1.2. Removing upper bounds</a></li>
<li><a href="#orgheadline77">22.1.3. Node splitting</a></li>
</ul>
</li>
<li><a href="#orgheadline79">22.2. Residual Network</a></li>
<li><a href="#orgheadline80">22.3. MCNF as LP</a></li>
</ul>
</li>
<li><a href="#orgheadline82">23. 2015-09-21 Monday</a></li>
<li><a href="#orgheadline85">24. 2015-09-18 Friday</a>
<ul>
<li><a href="#orgheadline83">24.1. Integrality</a></li>
<li><a href="#orgheadline84">24.2. Minimum Cost Network Flow</a></li>
</ul>
</li>
<li><a href="#orgheadline91">25. 2015-09-16 Wednesday</a>
<ul>
<li><a href="#orgheadline86">25.1. Problem Setup</a></li>
<li><a href="#orgheadline87">25.2. Complementary Slackness Proof</a></li>
<li><a href="#orgheadline88">25.3. Lagrangian Relaxation</a></li>
<li><a href="#orgheadline89">25.4. Easily Finding the Dual</a></li>
<li><a href="#orgheadline90">25.5. More Examples</a></li>
</ul>
</li>
<li><a href="#orgheadline95">26. 2015-09-14 Monday</a>
<ul>
<li><a href="#orgheadline92">26.1. Homework Stuff</a></li>
<li><a href="#orgheadline93">26.2. Duality</a></li>
<li><a href="#orgheadline94">26.3. Complementary Slackness</a></li>
</ul>
</li>
<li><a href="#orgheadline97">27. 2015-09-11 Friday</a>
<ul>
<li><a href="#orgheadline96">27.1. Duality</a></li>
</ul>
</li>
<li><a href="#orgheadline99">28. 2015-09-09 Wednesday</a>
<ul>
<li><a href="#orgheadline98">28.1. Linear Programming</a></li>
</ul>
</li>
<li><a href="#orgheadline108">29. 2015-09-02 Wednesday</a>
<ul>
<li><a href="#orgheadline101">29.1. Review of LP</a>
<ul>
<li><a href="#orgheadline100">29.1.1. Example</a></li>
</ul>
</li>
<li><a href="#orgheadline102">29.2. Reducing Vertex Cover to ILP</a></li>
<li><a href="#orgheadline103">29.3. "Slicing" Linear Programs</a></li>
<li><a href="#orgheadline104">29.4. Semi Definite Programming</a></li>
<li><a href="#orgheadline105">29.5. LP reduces to SDP</a></li>
<li><a href="#orgheadline106">29.6. Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li><a href="#orgheadline107">29.7. Back to Linear Programming</a></li>
</ul>
</li>
<li><a href="#orgheadline113">30. 2015-08-31 Monday</a>
<ul>
<li><a href="#orgheadline109">30.1. Linear Programming (LP)</a></li>
<li><a href="#orgheadline110">30.2. Integer Linear Programming</a></li>
<li><a href="#orgheadline111">30.3. Mixed Integer Linear Programming</a></li>
<li><a href="#orgheadline112">30.4. Next Time, on Advanced Algorithms:</a></li>
</ul>
</li>
<li><a href="#orgheadline120">31. 2015-08-28 Friday</a>
<ul>
<li><a href="#orgheadline114">31.1. Last Time:</a></li>
<li><a href="#orgheadline115">31.2. Proof</a></li>
<li><a href="#orgheadline118">31.3. Reduction</a>
<ul>
<li><a href="#orgheadline116">31.3.1. Optimal Message Passing</a></li>
<li><a href="#orgheadline117">31.3.2. Choosing your reduction</a></li>
</ul>
</li>
<li><a href="#orgheadline119">31.4. GNU Octave</a></li>
</ul>
</li>
<li><a href="#orgheadline125">32. 2015-08-26 Wednesday</a>
<ul>
<li><a href="#orgheadline121">32.1. Optimization Problems (cont.)</a></li>
<li><a href="#orgheadline122">32.2. Maximization problems</a></li>
<li><a href="#orgheadline123">32.3. Approximation Algorithms</a></li>
<li><a href="#orgheadline124">32.4. Approx Vertex Cover</a></li>
</ul>
</li>
<li><a href="#orgheadline128">33. 2015-08-24 Monday</a>
<ul>
<li><a href="#orgheadline126">33.1. Asymptotics</a></li>
<li><a href="#orgheadline127">33.2. Optimization Problems</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline4" class="outline-2">
<h2 id="orgheadline4"><span class="section-number-2">1</span> 2015-11-18 Wednesday</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-orgheadline1" class="outline-3">
<h3 id="orgheadline1"><span class="section-number-3">1.1</span> "Math Facts"</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Fact #1:
</p>

\begin{equation}
  1 - x \le \left(1 - \frac{x}{k}\right)^k \le e^{-x} \:\:\: \forall x \in R, \forall k \in Z^+
\end{equation}

<ul class="org-ul">
<li>in the limit as \(k\to\infty\), the right side is equality.</li>
<li>when \(k=1\), the left side is equality.</li>
</ul>

<p>
Fact #2:
</p>

<p>
Let \(x_1, x_2, \dots, x_n \in [0,1]\), s.t. \(\sum_{i=1}^n x_i \ge \zeta \ge 0\).  Then:
</p>

\begin{equation}
  \prod_{i=1}^n (1-x_i) \le \left(1 - \frac{\zeta}{n}\right)^n
\end{equation}

<p>
We'll "sketch" the proof without fully doing it.  Let's look at the case
\(n=2\), \(\zeta=1\).  So, we have:
</p>
<ul class="org-ul">
<li>\(x_1 + x_2 \ge 1\)</li>
<li>Or, \(x_2 \ge 1 - x_1\).</li>
</ul>
<p>
The expression we're interested in is:
</p>

\begin{align*}
  (1 - x_1)(1 - x_2) \le x_2(1-x_2) = f(x_2)
\end{align*}

<p>
You can maximize \(f\) by looking at its derivative, which will give you \(x_2 =
   \frac{1}{2} = x_1\).  So, plugging that back in:
</p>

\begin{align*}
  (1 - x_1)(1 - x_2) \le x_2(1-x_2) \le \left(1 - \frac{1}{2}\right)^2
\end{align*}

<p>
Tada!  You can do this generally by induction on \(n\), but we won't.
</p>
</div>
</div>

<div id="outline-container-orgheadline2" class="outline-3">
<h3 id="orgheadline2"><span class="section-number-3">1.2</span> Back to Hitting Set</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Here's the ILP relaxation of the hitting set problem:
</p>

<ul class="org-ul">
<li>\(\min \sum_{e \in E} c_e x_e\), (we'll call this \(c(x)\)) s.t.</li>
<li>\(\sum_{e \in T_i} x_e \ge 1\)</li>
<li>\(0 \le x_e \le 1\)</li>
</ul>

<p>
Earlier, we interpreted \(x_e\) as \(Pr[e\in A]\).  Then, we defined a randomized
algorithm to "solve" this problem.  Except it didn't solve it, because its
solutions weren't necessarily feasible.  But hey, it performed well!  So,
let's look at the probability that this algorithm produces an infeasible
solution.  We'l start by looking at the probability that a single set \(T_i\)
isn't "covered":
</p>

\begin{align*}
  Pr[T_i \cap A \ne \emptyset] &= 1 - Pr[\text{any member is in A}] \\
  &= 1 - \prod_{e \in T_i} Pr[e \not\in A] \\
  &= P - \prod_{e \in T_i} (1 - x_e^*)
\end{align*}

<p>
For each \(T_i\), we have \(\sum_{e \in T_i} x_e^* \ge 1\).  Which means that we can
say:
</p>

\begin{align*}
  Pr[T_i \cap A \ne \emptyset] &= P - \prod_{e \in T_i} (1 - x_e^*) \\
  &\ge 1 - \left(1 - \frac{1}{|T_i |}\right) \\
  &\ge 1 - \frac{1}{e} > 0
\end{align*}

<p>
Now, suppose we repeat the randomized algorithm \(t\) times.  Now, we know that
\(e\in A\) iff \(\exists\) one trial in which \(e\in A\).  Looking at the expressions
above, we know that then:
</p>

\begin{align*}
  Pr[T_i \cap A = \emptyset] \le \frac{1}{e^t} \le \frac{1}{cp}
\end{align*}

<p>
That last bit holds if we choose a \(t \in \Theta(\log p)\).  Now, let's define an
indicator random variable \(\gamma_i\) which is 1 when \(T_i\) is not covered.  Since
it's an indicator RV, its expectation is just the probability of its event,
which means:
</p>

\begin{align*}
  E[\gamma_i ] \le \frac{1}{cp}
\end{align*}

<p>
Now, we can talk about the expected number of sets not hit after \(t\) trials:
</p>

\begin{align*}
  E[\sum_{i=1}^p \gamma_i ] \le \frac{1}{c}
\end{align*}

<p>
This is true due to the linearity of expectation.  It is not true that the
\(\gamma_i\) are independent.
</p>

<p>
So, repeating the algorithm can allow us to bound the number of sets not
covered and make it arbitrarily small.  But, this ability comes at a cost.
Let's look at the expected cost of the new solution:
</p>

\begin{align*}
  E[c(A)] = t * c(x^*) \le t* \text{OPTIMAL}
\end{align*}

<p>
This leads to the theorem that this repeated randomized algorithm is an
\(O(\log p)\)-apx algorithm, and it returns a hitting set with probability
\(\ge 1 - \frac{1}{c}\).
</p>

<p>
Let's summarize our algorithms for Hitting Set:
</p>
<ul class="org-ul">
<li>Primal/Dual algorithm: apx \(\max |T_i |\) (for vertex cover, this is 2)
<ul class="org-ul">
<li>both simple and deterministic</li>
</ul></li>
<li>Repeated Randomized algorithM: apx \(O(\log p)\)
<ul class="org-ul">
<li>also simple, but randomized and has a nonzero probability of failure.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline3" class="outline-3">
<h3 id="orgheadline3"><span class="section-number-3">1.3</span> Maximum Satisfiability</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li>Have \(n\) decision variables, \(x_1, x_2, \dots, x_n \in \{T, F\}\).</li>
<li>Have \(c_i\), a bunch of clauses over \(x_1, \dots, x_n\).
<ul class="org-ul">
<li>For example, this could be \(x_1 \lor x_2\), \(x_1 \lor \lnot x_3\), or
\(\not x_1 \lor x_2 \lor \lnot x_3\).</li>
<li>In general, a clause may only be a disjunction of variables (or their
negations).</li>
</ul></li>
<li>We would like to find a truth assignment to \(x_1, x_2, \dots, x_n\) that maximizes
the number of true clauses.</li>
</ul>

<p>
Let's imagine right now that each clause contains \(\ge k = 2\) literals.
There is an incredibly easy approximation algorithm here: just assign \(x_i =
   T\) with probability \(\frac{1}{2}\).  The probability of a clause not being
satisfied is less than or equal to \(\frac{1}{4}\).  The expected number of
clauses being satisfied is \(\frac{3}{4}p\), (where \(p\) is the number of
clauses), and you can't do better than \(p\).  So this is a \(\frac{3}{4}\) apx
algorithm.  But it's not so good when \(k\) is 1.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline10" class="outline-2">
<h2 id="orgheadline10"><span class="section-number-2">2</span> 2015-11-16 Monday</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-orgheadline5" class="outline-3">
<h3 id="orgheadline5"><span class="section-number-3">2.1</span> Hitting Set</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Given \(E\), \(c_e \ge 0 \:\: \forall e \in E\), \(T_1, T_2, \dots, T_p \subseteq E\)</li>
<li>Find \(A \subseteq E\) w/ min \(c(A) = \sum_{e \in A} c_e\)
<ul class="org-ul">
<li>such that \(A \cap T_I \ne \emptyset, \:\: (i=1,2,\dots,p)\)</li>
</ul></li>
<li>An example of this problem is vertex cover.</li>
</ul>

<p>
An ILP for this:
</p>
<ul class="org-ul">
<li>\(x_e\) is 1 if \(e \in A\), or 0 otherwise.</li>
<li>Minimize \(c_{opt}\) subject to constraints based on the \(T\) sets.</li>
</ul>

<p>
The linear relaxation would allow \(x_e\) to take any value between 0 and 1.
The dual involves some variables \(y_i\) for \(i=1,2,\dots,p\).  Complementary
slackness conditions:
</p>

<ul class="org-ul">
<li>\(y_i \left(\sum_{e \in T_I} x_e - 1\right) = 0\)</li>
<li>\(x_e \left(\sum_{i,e\in T_i} y_i - c_e \right) = 0\)</li>
<li>And we want our solutions to be integral.</li>
</ul>

<p>
Unfortunately, this problem is not unimodular, so we can't expect all the
conditions to be satisfied.  So I guess we're just going to try to enforce
the bottom two conditions.  Furthermore, there is something we get when we
apply integrality to the second CS condition.  I guess it's nice cause we
know that the value of \(x\) must be 0 or 1, which makes the conditions a bit
stronger.
</p>
</div>
</div>

<div id="outline-container-orgheadline6" class="outline-3">
<h3 id="orgheadline6"><span class="section-number-3">2.2</span> Primal Dual Algorithm for HS</h3>
<div class="outline-text-3" id="text-2-2">
<p>
We start with an integral primal and a feasible dual, which are relataed by
complementary slackness.  In MCNF, we could start with \(x=0\), and \(\pi=0\).
</p>

<ul class="org-ul">
<li>\(A \gets \emptyset\) <i>i.e. x=0</i></li>
<li>\(y_i \gets 0 \:\:\: (i=1,2, \dots, p)\)</li>
<li><b>while</b> \(\exists K: T_k \cap A = \emptyset\)  <i>(that is, while x is still infeasible)</i>
<ul class="org-ul">
<li>(comment) Change x so that it's slightly "less infeasible".  <i>(In MCNF,
this was pushing flow from excess to deficit).</i></li>
<li>(comment) Update y so as to ensure complementary slacknesss. <i>(In MCNF, &pi;
       &larr; &pi;-d)</i>.</li>
<li>Increase \(y_k\) until \(\exists e: \sum_{i,e\in T_i} y_i = c_e\).</li>
<li>\(A \gets A \cup \{e\}\) (which means \(x_e=1\)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline7" class="outline-3">
<h3 id="orgheadline7"><span class="section-number-3">2.3</span> Example</h3>
<div class="outline-text-3" id="text-2-3">
<p>
We have a graph with nodes \(a,b,c,d\), which have costs \(2,3,4,5\)
respectively.  We have edges \((a,b), (a,c), (a,d), (b,c), (c,d)\).  It's a
rectangle with a diagonal.  The \(y_i\) values exist for each edge, and they are
all initialized to 0.
</p>

<p>
We start with the \(T_k\) associated with the edge \((a,c)\).  We increase \(y_k\)
(you might say instead \(y_{ac}\)) to 2, because this is the first value that
will make a vertex's CS condition switch.  This causes vertex $a$'s condition
to be true, so we can add \(a\) to the cover.
</p>

<p>
Next iteration, we see that there are still two edges (\((b,c)\) and
\((c,d)\)). that haven't been covered yet.  We wil choose \(T_k\) associated
with \((b,c)\).  When we increase \(y_{bc}\), the first value that makes a
constraint tight is 2, making the constraint on \(c\) tight (since it already
has 2 in its sum, and its cost is 4).  So, we add \(c\) to the cover, which
covers all the edges and terminates the algorithm.
</p>
</div>
</div>

<div id="outline-container-orgheadline8" class="outline-3">
<h3 id="orgheadline8"><span class="section-number-3">2.4</span> Approximation</h3>
<div class="outline-text-3" id="text-2-4">
<p>
<b>Theorem:</b> Primal Dual is a max \(|T_i|\)-apx algorithm for HS.
</p>

<p>
<b>Corollary:</b> PD is a 2-apx algorithm for VC.
</p>

<p>
<b>Proof of Theorem:</b> The \(y_i\) values start at 0 and never decrease, and are
always feasible (since &sum; y<sub>i</sub> &lt; c<sub>e</sub>\).  \(e\) is added to \(A\) when \(\sum_{i:e\in T_i}
   y_i = c_e\).
</p>

\begin{align*}
  c(A) &= \sum_{e \in A} c_e = \sum_{e \in A} \sum_{i:e\in T_i} y_i \\
       &= \sum_{i=1}^p y_i |A \cap T_i | \\
       &\le \max_{1 \le i \le p} |T_i | \cdot \sum_{i=1}^p y_i \\
       &\le \max_{1 \le i \le p} |T_{i}| \cdot c(y^*) \\
       &\le \max_{1 \le i \le p} |T_{i}| \cdot c(x^*) \\
       &\le \max_{1 \le i \le p} |T_{i}| \cdot c(x_{int}^*) \\
\end{align*}
</div>
</div>

<div id="outline-container-orgheadline9" class="outline-3">
<h3 id="orgheadline9"><span class="section-number-3">2.5</span> Randomized Approximation Algorithm</h3>
<div class="outline-text-3" id="text-2-5">
<p>
LP:
</p>
<ul class="org-ul">
<li>\(\min \sum_{e \in E} c_e x_e\), s.t.</li>
<li>\(\sum_{e \in T_i} x_e \ge 1\)</li>
<li>\(0 \le x_e \le 1\)</li>
</ul>

<p>
Randomized algorithm idea v0.1:
</p>
<ul class="org-ul">
<li>Solve the linear relaxation above.  This gets you an x.</li>
<li>Add \(e\) to \(A\) with probability \(x_e\).</li>
</ul>

<p>
Sadly, the glaringly obvious issue here is that in general, a solution
generated by this algorithm is not guaranteed to be feasible.  But how would
it perform?  Well, it turns out that \(E[c(A)] = c(x^*) \le c(x_{int}^*)\).  Which
is really good.  Like, potentially better than the actual solution to the
ILP.  Which seems problematic :P
</p>

<p>
The problem that needs to be solved is making this algorithm feasible.  And
the approach will be selecting a number \(t\) of trials where we will choose
whether to add \(e\) to \(A\).  This will allow us to reduce the probability of
infeasibility, (which sadly increases the expected cost, but ya gotta do what
you gotta do).  There will never be a point where the probability of
infeasibility of 0.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline12" class="outline-2">
<h2 id="orgheadline12"><span class="section-number-2">3</span> 2015-11-11 Wednesday</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-orgheadline11" class="outline-3">
<h3 id="orgheadline11"><span class="section-number-3">3.1</span> Game Tree Evaluation (cont)</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Same problem setup - binary tree, alternating min/max, and 0/1 leaves.
Recall from last time:
</p>
<ul class="org-ul">
<li>Not proven, but intuitive: for any deterministic algorithm, I can find a
labelling that will force the algorithm to visit every single leaf node.</li>
<li>We viewed this as a game situation where the algorithm and the input were
adversaries.  In this view, a randomized algorithm could be viewed as a
mixed strategy as opposed to any single deterministic (i.e. pure) strategy.</li>
<li>Finally, we were attempting to quantify the expected worst case number of
leaves visited, by defining it recursively.  We're looking at a tree with
"min" at the top of each round.</li>
</ul>

<p>
Expected visits for the max layer:
</p>
<ul class="org-ul">
<li><b>If max evaluates to 0:</b> then E[# leaves] = \(2W(k-1)\).</li>
<li><b>If max evaluates to 1:</b> then E[# leaves] \(\le \frac{3}{2} W(k-1)\).</li>
</ul>

<p>
Now we need to try to quantify \(W(k)\), so we look at the min layer (above
the max layer).
</p>
<ul class="org-ul">
<li><b>If min evaluates to 0:</b> then <i>at least one child</i> max evaluates to 0.
<ul class="org-ul">
<li>With at least 0.5 probability, the algorithm finds it:</li>
<li>\(W(k) \le \frac{1}{2}[2(W(k-1))] + \frac{1}{2}[2W(k-1) + \frac{3}{2}W(k-1)]\)</li>
<li>This is \(< W(k-1)\)</li>
</ul></li>
<li><b>If min evaluates to 1:</b> then we know both children evaluated to 1.
<ul class="org-ul">
<li>\(W(k) \le 2 * \frac{3}{2} W(k-1) = 3 W(k-1)\)</li>
</ul></li>
</ul>

<p>
In all cases, \(W(k) \le 3W(k-1)\le 3^k = 3^{\log_4 n} = n^{\log_4 3} = n^{0.793} = o(n)\).
So by randomizing, we have improved from linear in the number of leaves, to
sub linear.  Yay us.
</p>

<p>
Now, we would like to show that there is not a better randomized algorithm
for this problem.  To do this, we will use Yao's principle:
</p>

\begin{equation}
  \min_y \max_i H(i,y) \ge \min_j H(x,j) \forall x
\end{equation}

<p>
In our case, \(H(i,y)\) is the expected number of leaves visited by
randomized algorithm \(y\) on input tree \(i\).  Similarly, \(H(x,j)\) is the
expected number of leaves visited by deterministic algorithm \(j\) on a tree
labelled by probability distribution \(x\).
</p>

<p>
What Yao's principle gives us here is that our best randomized algorithm has
a lower bound determined by the minimum number of leaves expected to be
visited by a deterministic algorithm on any input probability distribution
\(x\).  Interesting.
</p>

<p>
We will label a leaf 1 with probability \(p\) and 0 with probability \(1-p\).
Doesn't matter what \(p\) is.  In this case, we have \(p=\frac{3-\sqrt{5}}{2}\)
</p>

<p>
Also, instead of doing mins and maxes, since we're guaranteed to have 0's and
1's, we know that max nodes are like or's, and min nodes are like ands.  If
we replace all nodes with nor's, we get an equivalent tree (viewed from the
top).  This is because \(\lnot(\lnot(a \lor b) \lor \lnot (c \lor d)) = (a \lor b) \land (c \lor
   d)\).  Let's look at the truth table of a nor:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-right">a</td>
<td class="org-right">b</td>
<td class="org-right">\(\lnot(a \lor b)\)</td>
<td class="org-left">probability</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-left">\(1-p)^2 = p\)</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-left">\(1-(1-p)^2\ = 1-p\)</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-left">same</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-left">same</td>
</tr>
</tbody>
</table>

<p>
Since Dr. Liberatore cleverly solved a quadratic equation, we have a nice
property that each node in the whole tree is 1/0 with the exact same
probability distribution: \(p\) and \(1-p\).  Woah.
</p>

<p>
Ok, so now we're looking at a tree of depth two, min at the top, max at the
next layer, leaves labeled left to right a,b,c,d.  Let a(b,c,d) be the first
(second, third, fourth) grandchild(ren) visited by a deterministic algorithm.
</p>

<p>
Let's look at cases:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Case</th>
<th scope="col" class="org-left">\(W(k)\)</th>
<th scope="col" class="org-left">Pr</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(a=0 \to b=0\)</td>
<td class="org-left">\(2W(k-1)\)</td>
<td class="org-left">\((1-p)^2 = p\)</td>
</tr>

<tr>
<td class="org-left">\(a=0 \to b=1 \to c=0 \to d=?\)</td>
<td class="org-left">\(4W(k-1)\)</td>
<td class="org-left">\((1-p^2)p = p^2\)</td>
</tr>

<tr>
<td class="org-left">\(a=0 \to b=1 \to c=1\)</td>
<td class="org-left">\(3W(k-1)\)</td>
<td class="org-left">\(p^2(1-p)\)</td>
</tr>

<tr>
<td class="org-left">\(a=1 \to c=0 \to d=?\)</td>
<td class="org-left">\(3W(k-1)\)</td>
<td class="org-left">\(p(1-p)\)</td>
</tr>

<tr>
<td class="org-left">\(a=1 \to c=1\)</td>
<td class="org-left">\(2W(k-1)\)</td>
<td class="org-left">\(p(1-p)\)</td>
</tr>
</tbody>
</table>

<p>
To find the expected value of \(W(k)\), we can just sum up each case times
its probability, plug it into your favorite numerical/symbolic computing
system, and get back an answer.  Or, we could be lazy, and try to avoid that.
Guess what we're going to do?  Be lazy!
</p>

<p>
We know that the final recurrence will be of the form \(W(k) = c W(k-1)\).
"Your book is fearless, and actually calculated the value of \(c\)." -
Dr. Liberatore.
</p>

<p>
However, we see that 3 is sort of the "mean" value, and the probability of 2
is greater than the probability of 4.  So we know that \(c<3\), but more than
two.  If you get the exact value of \(c\) as the book did, you would find that
\(W(k) = \Omega(n^{0.694})\).  The exponent is \(\log_4 c\), \(c \approx 2.62\).  Our estimation
was right on.
</p>

<p>
This tells us that a randomized algorithm needs to examine at least that many
nodes.  Our randomized algorithm examines more, and it's an open problem to
try to close the gap.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline18" class="outline-2">
<h2 id="orgheadline18"><span class="section-number-2">4</span> 2015-11-09 Monday</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-orgheadline17" class="outline-3">
<h3 id="orgheadline17"><span class="section-number-3">4.1</span> Game Tree Evaluation</h3>
<div class="outline-text-3" id="text-4-1">
<p>
(continued)
</p>

<p>
Recall:
</p>
<ul class="org-ul">
<li>We have a game tree, with internal nodes labeled "min" and "max"
alternating layers.  The leaves have payoff values of either 0 or 1.</li>
<li>Some definitions:
<ul class="org-ul">
<li>\(n\): number of leaves (\(=4^K\))</li>
<li>\(k\): number of rounds</li>
<li>\(h\): height (\(=2k\))</li>
</ul></li>
<li>Input: \(k\), labeling of leaves.</li>
<li>We were doing a "game theoretic" approach to this problem, where:
<ul class="org-ul">
<li>One player tries to minimize the number of node visits by designing a
smarter algorithm.</li>
<li>The other player tries to maximize the number of node visits by designing
a devious tree.</li>
<li>We realized that really any deterministic algorithm will have an input
that will force it to visit every node.</li>
<li>We realized that a randomized algorithm was equivalent to a "mixed"
strategy, and it may present a better expected runtime</li>
</ul></li>
</ul>
</div>

<div id="outline-container-orgheadline13" class="outline-4">
<h4 id="orgheadline13"><span class="section-number-4">4.1.1</span> Pruned Depth First Search</h4>
<div class="outline-text-4" id="text-4-1-1">
<p>
This is essentially a depth first search where we avoid expanding branches
if we already know the parent's value.  Without loss of generality, any
algorithm we design will be of this form.  That is (according to the book):
"for any algorithm that examines \(L\) leaves, there exists a pruned DFS that
examines no more than \(L\) leaves."  So it's a bit simpler to think of
everything in pruned DFS terms anyway.
</p>
</div>
</div>

<div id="outline-container-orgheadline14" class="outline-4">
<h4 id="orgheadline14"><span class="section-number-4">4.1.2</span> Game Theory Approach to Game Tree Evaluation</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
<b>Question:</b> Here's a deterministic algorithm.  Show that it examines all
leaves.
</p>

<p>
<b>Answer:</b> A labeling that forces the examination of all leaves.
</p>

<p>
We have two players:
</p>
<ul class="org-ul">
<li>The Input (worst-case).  Objective: maximize \(n\).
<ul class="org-ul">
<li>Strategies: \(S^1 = \{(0,0,0,0), \dots, (1,1,1,1)\}\)</li>
<li>Some are not useful at all (like the first and last)</li>
<li>So strictly less than 16 (or 15) strategies in the case of \(k=1\).</li>
</ul></li>
<li>The Algorithm: Objective: minimize \(n\)
<ul class="org-ul">
<li>Strategies: \(S^1 = \{(L,L,L), \dots, (R,R,R)\}\).</li>
<li>Each is an assignment of the "first" direction to each internal node.</li>
<li>These are all unique and potentially useful strategies, so exactly 8.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline15" class="outline-4">
<h4 id="orgheadline15"><span class="section-number-4">4.1.3</span> Randomized Algorithms</h4>
<div class="outline-text-4" id="text-4-1-3">
<p>
RA's base their decisions on a random number generator.
</p>

<p>
Examples:
</p>
<ul class="org-ul">
<li>for Game Tree Evaluation, choose a strategy in S<sup>2</sup> uniformly at random, and
use it.</li>
<li>always use the first strategy (an RA doesn't <i>have</i> to use its RNG).</li>
<li>choose a strategy in S<sup>2</sup> at random, with <i>any</i> probability distribution
over S<sup>2</sup>.</li>
</ul>

<p>
This already feels quite similar to mixed strategies!
</p>

<p>
<b>Note:</b> random &ne; arbitrary.  In depth first search, we arbitrarily select an
outgoing edge to expand.  That doesn't mean we're doing it at random.  An
arbitrary selection is just a decision made ahead of time, and a random with
arbitrary decisions will always run the same, and may have a worst case
input.
</p>

<p>
Anyway, now that we're looking at this problem as a game, we should also
note that the number of strategies of either player is exploding
exponentially as a function of the number of rounds.  So we can't expect to
be able to use a linear program to come up with a maximized worst case mixed
strategy.  But we can still use game theory stuff (such as Yao's principle)
to help.
</p>
</div>
</div>

<div id="outline-container-orgheadline16" class="outline-4">
<h4 id="orgheadline16"><span class="section-number-4">4.1.4</span> Worst Case</h4>
<div class="outline-text-4" id="text-4-1-4">
<p>
Ok, our previous initial randomized algorithm was to select a strategy from
\(S^1\) uniformly at random.  This is equivalent to, when visiting an
internal node, go L/R with probability 0.5.
</p>

<p>
Let \(W(k)\) be the worst case number of leaves examined by randomized
algorithm in the expectation (\(k\): number of rounds).
</p>

<ul class="org-ul">
<li>Let's look at a "max" node, which will finally evaluate to 0.  We know
that both nodes must have been evaluated, so \(W(k) = 2 W(k-1)\).</li>
<li>Now let's look at a "max" node, which will finally evaluate to 1.  This
means that at least one of the children will evaluate to 1.  With at least
0.5 probability, we will find it, and have \(W(k) = W(k-1)\), and with no
more than 0.5 probability, we won't find it, and get \(W(k) = 2W(k-1)\).
So, the expected worse case is \(W(k) \le \frac{3}{2} W(k-1)\).</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline22" class="outline-2">
<h2 id="orgheadline22"><span class="section-number-2">5</span> 2015-11-06 Friday</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-orgheadline21" class="outline-3">
<h3 id="orgheadline21"><span class="section-number-3">5.1</span> Two Person Zero Sum Games (cont)</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Last time, we defined TPZSGs, and saddle points, and we talked about mixed
strategies.  We also formulated the problem of designing a mixed strategy to
maximize a player's worst case payoff:
</p>

\begin{align*}
  \max v, &\text{ s.t.} \\
  \sum_{i=1}^n H(i,j) x_i &\ge v \:\:\: j=1,2,\dots,m \\
  \sum_{i=1}^n x_i &= 1 \\
  x_i &\ge 0 \:\:\: i=1,2,\dots,n
\end{align*}

<p>
The dual:
</p>

\begin{align*}
  \min w, &\text{ s.t.} \\
  \sum_{j=1}^m H(i,j) y_j &\le w \:\:\: i=1,\dots,n\\
  \sum_{i=1}^m y_i &= 1 \\
  y_i &\ge 0 \:\:\: j=1,\dots,m\\
\end{align*}

<p>
Would you look at that!  The dual is the exact same thing except for the
other player.  An due to duality, we know that \(v^* = w^*\), so they will
result in the same value.  This is <i>Loomis's Lemma</i> (this guy Loomis proved
it before we knew about LP duality, so good for him).
</p>

<p>
We can take the constraints of the dual and sum them up like this to get a
new expression (for any \(x\)).
</p>

\begin{align*}
  \sum_{i=1}^n x_i \sum-{j=1}^m H(i,j) y_j^* &\le \sum_{i=1}^n x_i w^* \\
  \sum_{i=1}^n \sum_{j=1}^m x_i H(i,j) y_j^* &\le w^*
\end{align*}

<p>
And similarly for the primal:
</p>

\begin{align*}
  \sum_{j=1}^m \sum_{i=1}^n x_i^* H(i,j) y_j \ge v^*
\end{align*}

<p>
Combining these (and substituting in \(x^*\) and \(y^*\) respectively, we have:
</p>

\begin{equation}
  v^* \le \sum_{i=1}^n \sum_{j=1}^m x_i^* H(i,j) y_j^* \le w^*
\end{equation}

<p>
By definition, that quantity in the summation is \(H(x^*, y^*)\), and by
duality, we know that \(H(x^*, y^*) = v^* = w^*\).  This leads us to the
inequality:
</p>

\begin{equation}
  H(x,y^*) \le H(x^*, y^*) \le H(x^*,y)
\end{equation}

<p>
This leads to the theorem (Von Neuman): Every 2 person zero sum game has a
(mixed) saddle point.
</p>
</div>

<div id="outline-container-orgheadline19" class="outline-4">
<h4 id="orgheadline19"><span class="section-number-4">5.1.1</span> Yao's Principle</h4>
<div class="outline-text-4" id="text-5-1-1">
\begin{equation}
  \max_x \min_j H(x,j) = \min_y \max_i H(i,y) \le \max_i H(i,y) \forall y
\end{equation}

<p>
This is a restatement of Loomis's lemma, with a rather obvious addendum.
And then we put this into two inequalities that look a bit less obvious.
</p>

\begin{align*}
  \max_x \min_j H(x,j) &\le \max_i H(i,y) \forall y\\
  \min_y \max_i H(i,j) &\ge \min_j H(x,j) \forall x
\end{align*}

<p>
And I guess that is Yao's Principle.  It is useful for when you want to put
an upper bound on the \(v^*\) the optimal payoff.  You create a \(y\) that has
\(\max_i H(i,y) \le b\).
</p>
</div>
</div>

<div id="outline-container-orgheadline20" class="outline-4">
<h4 id="orgheadline20"><span class="section-number-4">5.1.2</span> Example Time</h4>
<div class="outline-text-4" id="text-5-1-2">
<p>
Game Tree Evaluation:
</p>
<ul class="org-ul">
<li>Input: game tree.  The game tree root is a maximizing node, its children
are minimizing, and the next level are maximizing, etc.  The leaves are
labeled with 0 or 1.  The tree has degree \(d\), and \(k\) rounds (which means
that the height of the tree is \(2k\)).  In our example, \(d=2\).</li>
<li>Output: \(x\), the value at the root node.</li>
</ul>

<p>
We would like an algorithm to evaluate this tree, and we would ilke it to
minimize the number of leaves visited (which is roughly equivalent to
minimizing runtime).
</p>

<p>
The total number of leaves in this tree is \(4^k\).
</p>

<p>
Depending on the tree, there could be a node that will allow you to skip
visiting other nodes.  But with a deterministic algorithm, you can always
design a worst-case input that will "hide" that information for as long as
possible.
</p>

<p>
This already is similar to game theory stuff.  We are one player (trying to
find a worst-case input), and our algorithm is the other player (trying to
terminate as quickly as possible).
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline26" class="outline-2">
<h2 id="orgheadline26"><span class="section-number-2">6</span> 2015-11-04 Wednesday</h2>
<div class="outline-text-2" id="text-6">
</div><div id="outline-container-orgheadline23" class="outline-3">
<h3 id="orgheadline23"><span class="section-number-3">6.1</span> Two Person Zero Sum Games</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Recall: a TPZSG is a game with 2 players, and the payoff of player 1 is the
opposite of the payoff of player 2.  We will define \(H(i,j)\) to be the
payoff for player 1 of strategies \(i\), \(j\).
</p>

<p>
<b>Definition:</b> A saddle point is strategies \(i^*\) (first player), \(j^*\)
(second player) s.t.:
</p>
<ul class="org-ul">
<li>\(\forall i \in S^1\), \(\forall j \in S^2\): \(H(i, j^*) \le H(i^*, j^*) \le H(i^*, j)\)</li>
</ul>

<p>
That is, when player 1 knows player 2 is using \(j^*\), no strategy will do
better than \(i^*\).  And vice versa.
</p>

<p>
Example:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">&#xa0;</td>
<td class="org-right">1:</td>
<td class="org-right">2:</td>
<td class="org-right">3:</td>
</tr>

<tr>
<td class="org-right">1:</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-right">3</td>
</tr>

<tr>
<td class="org-right">2:</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>

<p>
Saddle point: \(i^* = 1\), \(j^* = 1\), with a payoff of 3.
</p>

<p>
There is not always a saddle point!
</p>

<p>
Assume &exist; saddle point \((i^*, j^*)\):
</p>

<p>
\(\forall \min_j H(i,j) \le H(i,j^*) \le H(i^*,j^*)\)
</p>

<p>
\(\max_i \min_j H(i,j) \le H(i^*, j^*) \le \min_j \max_i H(i,j)\)
</p>

<p>
When there is a saddle point, these inequalities are actually strict
inequalities!  Since you would simply choose \(i^*\) and \(j^*\).
</p>
</div>
</div>

<div id="outline-container-orgheadline24" class="outline-3">
<h3 id="orgheadline24"><span class="section-number-3">6.2</span> Saddle Points for Mixed Strategies</h3>
<div class="outline-text-3" id="text-6-2">
<p>
<b>Definition:</b> A saddle point \((x^*, y^*)\) has the property that \(\forall x,y H(x,
   y^*) \le H(x^*, y^*) \le H(x^*, y)\).
</p>

<p>
This is pretty much exactly the same as previously, except \(H\) is now an
expectation over a probability distribution.
</p>
</div>
</div>

<div id="outline-container-orgheadline25" class="outline-3">
<h3 id="orgheadline25"><span class="section-number-3">6.3</span> Something Else</h3>
<div class="outline-text-3" id="text-6-3">
<p>
From the point of view of player 1, I expect the worst (i.e. minimizing my
playoff) pure strategy from player 2.  I would like to formulate a mixed
strategy to play the game that maximizes my (expected) "profit".  Let \(x_1,
   x_2, \dots, x_n\) be probabilities of strategy \(i \in S^1\).  If player 2 plays \(j\) (I
don't know that he will!):
</p>

<p>
\(H(x^*, j) = \sum_{i=1}^n x_i H(i,j)\)
</p>

<p>
My worst-case payoff for any strategy that player 2 can play is:
</p>

<p>
\(\min_j H(x^*, j) = \min_j \sum_{i=1} x_i H(i,j)\)
</p>

<p>
I would like to maximize my own worst-case payoff (since I know player 2 will
try to minimize my payoff).  That is:
</p>

<p>
\(x^* = \arg \max_x \min_j \sum_{i=1}^n x_i H(i,j)\)
</p>

<p>
How do we find this?  Linear programming.  We create a constraint for every
strategy \(j\) can have: \(\sum_{i=1}^n x_i H(i,j) \ge v\).  We want to maximize \(v\), our
minimum payoff.  We also need to ensure that \(\sum_{i=1}^n x_i = 1\)., and \(x_i \ge
   0 \forall i\).  The overall LP:
</p>

<ul class="org-ul">
<li>\(\max v\), s.t.</li>
<li>\(-\sum_{i=1}^n x_i H(i,j) + v \le 0 \:\: \forall j\)</li>
<li>\(\sum_{i=1}^n x_i = 1\)</li>
<li>\(x_i \ge 0 \:\: \forall i\)</li>
</ul>

<p>
Dual:
</p>

<ul class="org-ul">
<li>\(\min w\), s.t.</li>
<li>\(-\sum_{j=1}^m H(i,j) y_j + w \ge 0\)</li>
<li>\(\sum_{i=1}^m y_i = 1\)</li>
<li>\(y_j \ge 0\)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline31" class="outline-2">
<h2 id="orgheadline31"><span class="section-number-2">7</span> 2015-11-02 Monday</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-orgheadline27" class="outline-3">
<h3 id="orgheadline27"><span class="section-number-3">7.1</span> Dr. Liberatore's Homework</h3>
<div class="outline-text-3" id="text-7-1">
<p>
Last time we talked about games in Behavioral (Extensive?) form, and
translating them into Strategic form.  We are still looking at the game tree
where Player 1 has choices L and R, and then Player 2 chooses L or R after
that.
</p>

<ul class="org-ul">
<li>\(S^1 = \{L, R\}\)</li>
<li>\(S^2 = \{(L,L), (L,R), (R,L), (R,R)\}\)
<ul class="org-ul">
<li>where (x,y) means: if player 1 played L, then x.  If player 1 played R,
then y.  (I think I missed this part last lecture!)</li>
</ul></li>
</ul>

<p>
The probability you assign to \(S^2\) is the product of the conditional
probabilities of each move.  So say player 2 will move left with probability
\(1/2\) when player 1 moves left, and left with probability \(3/4\) if player 1
goes right.  In this case, the probability of strategy \(s^2=(L,L)\) is
\(\frac{3}{8}\).
</p>

<p>
Computing the probability of each of the payoffs can be done with this setup,
you just have to make sure that you add up probabilities for both strategies
that allow a certain branch to take.
</p>

<p>
Given a game in extensive form:
</p>
<ol class="org-ol">
<li>Wlog the payoff leaves are distinct (if not, add a player who never gets
to move and whose payoff is different from leaf to leaf).</li>
<li><p>
\(Pr[H(l)]\).  If you have a path from the root to a payoff, then the probability is:
</p>

\begin{equation}
  Pr[H(l)] = \prod_{e \in \text{Path root} \to l} p(e)
\end{equation}

<p>
Here, \(p(e)\) is the probability that edge \(e\) is chosen.  We can
subdivide this into terms for each player:
</p>

\begin{equation}
  Pr[H(l)] = \prod_{e \in \text{Path root} \to l} p(e) = \prod_{i=1}^n \prod_{e \in \text{Path root} \to l, e \text{chosen by player } i} p(e)
\end{equation}

<p>
When you transform the game to strategic form, a strategy set will be the
set of all possible choices for all the players.  To compute the
probability of getting to a certain payoff, you do the following:
</p>

\begin{equation}
  Pr[H(l)] = \prod_{i=1}^n Pr[\text{player }i\text{ contemplates }e_1, e_2, \dots]
\end{equation}

<p>
Which evaluates to the same probability.
</p></li>
</ol>
</div>
</div>

<div id="outline-container-orgheadline30" class="outline-3">
<h3 id="orgheadline30"><span class="section-number-3">7.2</span> Our Homework</h3>
<div class="outline-text-3" id="text-7-2">
</div><div id="outline-container-orgheadline28" class="outline-4">
<h4 id="orgheadline28"><span class="section-number-4">7.2.1</span> MCNF and Max Flow</h4>
<div class="outline-text-4" id="text-7-2-1">
<p>
Can't have negative upper bounds!  For the nurse problem, you had to
maximize the number of unused nurses, not try to maximize the negative of
the number of nurses used.
</p>
</div>
</div>

<div id="outline-container-orgheadline29" class="outline-4">
<h4 id="orgheadline29"><span class="section-number-4">7.2.2</span> Successive Shortest Paths</h4>
<div class="outline-text-4" id="text-7-2-2">
<p>
Negative reduced costs should not happen, they indicate an error in your
work.  That's fine if you encounter that and recognize it, but it sounds
like a lot of people got negative reduced costs.
</p>

<p>
Each iteration, you have \((c_{ij}^\pi, r_{ij})\) for each arc, and \((e_i, \pi_i)\)
for each node \(i\).
</p>

<ul class="org-ul">
<li>You compute the shortest path using the reduced costs.  This assigns \(d_i\)
      values.</li>
<li>You determine the shortest path and augment flow.</li>
<li>You draw a new network.  The $&pi;<sub>i</sub>$'s are updated by \(\pi_i - d_i\).</li>
<li>The topology is changed to to the augmentation.</li>
<li><b>However</b>, the \(c_{ij}^\pi\) are always recomputed with \(c_{ij}^\pi = c_{ij} -
      \pi_i + \pi_j\), not using the old \(c_{ij}^\pi\)</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline36" class="outline-2">
<h2 id="orgheadline36"><span class="section-number-2">8</span> 2015-10-30 Friday</h2>
<div class="outline-text-2" id="text-8">
</div><div id="outline-container-orgheadline32" class="outline-3">
<h3 id="orgheadline32"><span class="section-number-3">8.1</span> Games (strategic form)</h3>
<div class="outline-text-3" id="text-8-1">
<ul class="org-ul">
<li>\(N=\{1,2,\dots,n\}\) players</li>
<li>\(S^i\) = set of pure strategies of \(i\)th player.</li>
<li>Payoff function \(H: S^1 \times S^2 \times \dots \times S^N \to R^N\)</li>
<li>\(\vec{s} \in S^1 \times S^2 \times \dots \times S^N\) strategy profile</li>
<li>\(H_i(s)\): payoff of \(i\)th player.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline33" class="outline-3">
<h3 id="orgheadline33"><span class="section-number-3">8.2</span> Mixed Strategies</h3>
<div class="outline-text-3" id="text-8-2">
<ul class="org-ul">
<li>Mixed strategy for \(i\)th player:
<ul class="org-ul">
<li>\(x^i\): probability distribution over \(S^i\).</li>
</ul></li>
<li>Mixed strategy profile:
<ul class="org-ul">
<li>\(\vec{x} = (x^1, x^2, \dots, x^n)\)</li>
</ul></li>
<li>Strategy profile \(\vec{s} = (s_1, s_2, \dots, s_n)\) has probability
\(\vec{x}(\vec{s}) = x^1(s_1)x^2(s_2) \dots x^n(s_n)\).</li>
<li>Expected payoff:
<ul class="org-ul">
<li>\(H(\vec{x}) = \sum_{s\in S^1 \times S^2 \times \dots \times S^N} \vec{x}(\vec{s}) H(\vec{s})\)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline34" class="outline-3">
<h3 id="orgheadline34"><span class="section-number-3">8.3</span> Games (Behavioral Form)</h3>
<div class="outline-text-3" id="text-8-3">
<p>
There is a tree that shows what the players can do.  Say, the first level is
player one, and there is a branch for each move player one can do.  Then, the
next level is for the next player, and at each "child node" game state, there
is a branch for each move the player can make.
</p>

<p>
At the leaves of the tree you have the payoff function.
</p>

<ul class="org-ul">
<li>The behavioral form is probably a bit more intuitive.  It represents
players' moves as sequential, whereas the strategic form doesn't impose
that.</li>
<li>The book has a proof that the behavioral form and strategic form are
equivalent.  But we won't be doing much with behavioral form, so we'll just
do an example of one.</li>
</ul>

<p>
Our example of behavioral form was a binary tree with two levels, where
player one would first choose between left and right, and then player two
would choose between left and right as well.
</p>

<p>
We had a bit of a disagreement on how to represent player two's set of pure
strategies, and its probability distribution.  Dr. Liberatore represented
\(S^2 = \{LL, LR, RL, RR\}\), and created a probability distribution over
that.  I disagree; I think that you should represent \(S^2=\{L, R\}\), and
that \(x^2\) is just conditioned on \(x^1\).  But, we spent a lot of time on
this dispute without getting anything done, so we moved on.
</p>
</div>
</div>

<div id="outline-container-orgheadline35" class="outline-3">
<h3 id="orgheadline35"><span class="section-number-3">8.4</span> Two person zero sum game</h3>
<div class="outline-text-3" id="text-8-4">
<ul class="org-ul">
<li>\(n=2\)</li>
<li>\(H_1(\vec{s}) = -H_2(\vec{s})\)</li>
</ul>

<p>
This means that the first playe'rs maximization is the second player's min.
</p>

<ul class="org-ul">
<li>There is/(may be) a saddle point \((i^*, j^*) \in S^1 \times S^2\).</li>
<li>\(H(i,j^*) \le H(i^*,j^*) \le H(i^*,j)\), \(\forall i \in S^1, j \in S^2\)
<ul class="org-ul">
<li>If player two plays \(j^*\), then player 1 wlog plays \(i^*\).</li>
<li>If player one plays \(i^*\), its payoff is guaranteed to be \(\ge H(i^*,
       j^*)\).  It could be more if player two makes a mistake.</li>
</ul></li>
<li>There doesn't have to be a saddle point, I guess.  For instance, the
matching pennies game doesn't have a saddle point.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline41" class="outline-2">
<h2 id="orgheadline41"><span class="section-number-2">9</span> 2015-10-28 Wednesday</h2>
<div class="outline-text-2" id="text-9">
</div><div id="outline-container-orgheadline38" class="outline-3">
<h3 id="orgheadline38"><span class="section-number-3">9.1</span> Capacity Scaling (cont)</h3>
<div class="outline-text-3" id="text-9-1">
<p>
The algorithm:
</p>
<ul class="org-ul">
<li>First, set a flow \(x \gets 0\), \(\pi \gets 0\)</li>
<li>\(\Delta \gets 2^{\lfloor\log U\rfloor}\)</li>
<li><b>while</b> \(\Delta \ge 1\)
<ul class="org-ul">
<li>Here, we maintain the invariant:</li>
<li><b>foreach</b> \((i,j) \in E\):
<ul class="org-ul">
<li>if \(c_{ij}^\pi < 0\), then arc reversal</li>
</ul></li>
<li>SSP(\(\Delta\))</li>
<li>\(\Delta \gets \Delta / 2\)</li>
</ul></li>

<li>The algorithm holds the loop invariant that \(c_{ij}^\pi \ge 0 \:\:\: \forall
     (i,j) \in E(x,\Delta)\).</li>
<li>SSP(\(\Delta\)) terminates when either \(E(\Delta) = \emptyset\) or \(D(\Delta) = \emptyset\).  So either
\(0 \le e_i < \Delta\) or \(0 \ge e_i > -\Delta\).  This means that the total excess &le;
\(n\Delta\).</li>
<li>After we modify \(\Delta\), the total excess &le; \(2n\Delta\).</li>
<li>However, the arc reversals introduce a worst case total of \(2m\Delta\) excess,
so at the beginning of SSP, we get a total excess &le; \(2(n+m)\Delta\).</li>
<li>At \(\Delta=1\), \(G(x,\Delta) = G(x)\), so we are solving the original residuals.
<ul class="org-ul">
<li>We can think of CS as a logarithmic number of scaling phases to prepare
for \(SSP(1)\) (the original problem).</li>
<li>Kinda like Shell Sort!</li>
</ul></li>
</ul>
</div>

<div id="outline-container-orgheadline37" class="outline-4">
<h4 id="orgheadline37"><span class="section-number-4">9.1.1</span> Runtime</h4>
<div class="outline-text-4" id="text-9-1-1">
<p>
The original SSP took \(O(nUS)\).  It was based on the idea that you would
send (in the worst case) 1 unit of flow each time, so you would do \(nU\)
augmentations, multiplied by \(S\), the shortest path time.  We have changed
our SSP algorithm so that it pushes \(\Delta\) units of flow at each iteration.  We
said above that at the beginning of SSP, the total excess &le; \(2(n+m)\Delta\).  By
\(2(n+m)\) iterations, we should have excess set to 0.  So, the runtime is
\(O((n+m)S)\) for each SSP, and the total runtime is \(O((n+m)S\log U)\).
(for the record, \(m\) is the number of edges).  This means that capacity
scaling is in fact a polynomial time algorithm!
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline39" class="outline-3">
<h3 id="orgheadline39"><span class="section-number-3">9.2</span> Max Flow Min Cut</h3>
<div class="outline-text-3" id="text-9-2">
<p>
Suppose you have a source vertex, and a network without costs but with
capacities.  The maximum flow from \(s\) to \(t\) is equal to the min cut that
separates \(s\), \(t\).  A cut is defined as a partition of the set of vertices.
The min cut is the one that minimizes the value of the edges (in this case,
capacities) that span the partition.  It actually really makes sense that max
flow is equal to min cut, because the min cut is like the smallest "pipe
width" you can send flow through.
</p>

<p>
Unfortunately, in order to solve min cut you pretty much have to solve max
flow.  So there isn't a huge breakthrough due to this property.  It's just
interesting and sometimes useful.
</p>
</div>
</div>

<div id="outline-container-orgheadline40" class="outline-3">
<h3 id="orgheadline40"><span class="section-number-3">9.3</span> Game Theory!</h3>
<div class="outline-text-3" id="text-9-3">
<p>
We have a game where a professor puts a coin under a cup, and a student
guesses whether it's heads or tails.  If the student guesses correctly, they
get a point and the professor loses one.  Otherwise, the professor gets a
point and the student loses one.  To summarize:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">Student Guesses Heads</td>
<td class="org-left">Student Guesses Tails</td>
</tr>

<tr>
<td class="org-left">Professor Picks Heads</td>
<td class="org-left">Professor-1, Student+1</td>
<td class="org-left">Professor+1, Student-1</td>
</tr>

<tr>
<td class="org-left">Professor Picks Tails</td>
<td class="org-left">Professor+1, Student-1</td>
<td class="org-left">Professor-1, Student-1</td>
</tr>
</tbody>
</table>

<p>
Here is a theoretical definition of a game (in strategic terms):
</p>
<ul class="org-ul">
<li>Set \(N=\{1,2,\dots,n\}\) players.</li>
<li>Set \(S^i \) of pure strategies for the \(i\textsuperscript{th}\) player.</li>
<li>Payoff function \(H: S^1 \times S^2 \times \dots \times S^n \to R^n \)</li>
<li>A strategy profile: \((s_1, s_2, \dots, s_n)\)</li>
<li>\(H_i(s_1, s_2, \dots, \s_n)\) is the payoff for player \(i\)</li>
</ul>

<p>
In this coin flipping game, there are two players (player one is professor,
player two is student), and \(S^1=S^2=\{h,t\}\).  The table above summarizes
the payoff, but I'll do it again in terms of the players and strategies
again.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">s<sub>2</sub>=h</td>
<td class="org-left">s<sub>2</sub>=t</td>
</tr>

<tr>
<td class="org-left">s<sub>1</sub> = h</td>
<td class="org-left">(-1, +1)</td>
<td class="org-left">(+1,-1)</td>
</tr>

<tr>
<td class="org-left">s<sub>2</sub> = t</td>
<td class="org-left">(+1, -1)</td>
<td class="org-left">(-1,+1)</td>
</tr>
</tbody>
</table>

<p>
<b>Def:</b> A <i>mixed strategy</i> of player \(i\) is a probability distribution over
\(S^i \).
</p>
<ul class="org-ul">
<li>\(\vec{x}\): mixed strategy profile
<ul class="org-ul">
<li>\(\vec{x} = (x_1, x_2, \dots, x_n)\)</li>
</ul></li>
<li>\(Pr[s] = x_1(s_1)x_2(s_2)\dots x_n(s_n)\)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline44" class="outline-2">
<h2 id="orgheadline44"><span class="section-number-2">10</span> 2015-10-26 Monday</h2>
<div class="outline-text-2" id="text-10">
</div><div id="outline-container-orgheadline42" class="outline-3">
<h3 id="orgheadline42"><span class="section-number-3">10.1</span> Runtime of Successive Shortest Paths</h3>
<div class="outline-text-3" id="text-10-1">
<ul class="org-ul">
<li>Worst case complexity, as a function of \(n\), is sometimes tricky.</li>
<li>We talk about it in terms of "input size", with any <i>reasonably efficient
encoding</i>.</li>
<li>Some things that look polynomial (e.g. \(n^k \)) are not, because \(k\) is an
input to the problem.</li>
<li>Some things that don't look polynomial (e.g. \(n \log n\)) are, because
they are bounded above by a polynomial function (e.g. \(n^2 \)).</li>
<li>So, with that in mind, let's look at the runtime of SSP.</li>
</ul>

<p>
The runtime is \(O(n \times U \times S)\), where:
</p>
<ul class="org-ul">
<li>\(n\) is the number of vertices</li>
<li>\(U = \max_{i\in V} \{|b_i |\}\)</li>
<li>\(S\) is the shortest path runtime</li>
</ul>

<p>
This is not polynomial!  Let's look at the terms.  We'll assume \(S\) is fine.
To represent the graph, we need at least \(n\) bits (probably some constant
multiple of \(n\), for the edges and things).  So, \(n\) is an input size, and
the \(n\) term is polynomial.
</p>

<p>
However, \(U\) is not an <i>input size</i>.  It's an <i>input number</i>.  It's
completely unrelated to the storage of the graph and the input size.  You
could make it whatever you'd like.  Because of this, you only need \(\log_2
   U\) bits to store it.  Thus, \(U\) is exponential in the input size.  (A useful
counterexample - if \(U=2^n \), then it's \(O(n2^n S)\)).
</p>

<p>
It's a pseudo polynomial time algorithm.  If we represented \(U\) in unary, it
would be polynomial.  But that's not a <i>reasonably efficient encoding</i>.
</p>
</div>
</div>

<div id="outline-container-orgheadline43" class="outline-3">
<h3 id="orgheadline43"><span class="section-number-3">10.2</span> Capacity Scaling</h3>
<div class="outline-text-3" id="text-10-2">
<ul class="org-ul">
<li>\(\Delta = 2^{\lfloor\log U\rfloor},\dots, 2, 1\)</li>
<li>\(E(\Delta) = \{e_i \ge \Delta: i \in V\}\)</li>
<li>\(D(\Delta) = \{e_i \le -\Delta: i \in V\}\)</li>
<li>\(G(x,\Delta) = (V, E(x,\Delta))\) - the residual network w.r.t. \(x\), w/ edges \(r_{ij}
     \ge \Delta\).</li>
</ul>

<p>
The idea of this algorithm is that we want to solve the biggest supplies and
demands first.
</p>

<p>
SSP(&Delta;):
</p>
<ul class="org-ul">
<li><i>Input:</i> \(G(x, \Delta)\) s.t. \(c_{ij}^\pi \ge 0 \:\:\: \forall (i,j) \in E(x,\Delta)\)</li>
<li>\(E(\Delta) \gets \{e_i \ge \Delta: i \in V\}\)</li>
<li>\(D(\Delta) \gets \{e_i \le -\Delta: i \in V\}\)</li>
<li><b>while</b> \(E(\Delta), D(\Delta) \ne \emptyset\):
<ul class="org-ul">
<li>choose \(k \in E(\Delta)\), \(l \in D(\Delta)\)</li>
<li>find shortest path \(P: k \to l\) in \(G(x, \Delta)\) w/ \(c_{ij}^\pi \)</li>
<li>send \(\Delta\) units of flow along \(P\).</li>
<li>\(\pi \gets \pi - d\)</li>
<li>update \(x\), \(E(\Delta)\), \(D(\Delta)\), \(G(k,\Delta)\).</li>
</ul></li>
</ul>

<p>
Thinking about this algorithm, we see that each iteration of this will not
necessarily remove an edge from the residual.  It could just drop that node
out of \(E(x,\Delta)\), while still keeping its residual greater than 0.  So, we
need the capacity scaling algorithm below:
</p>

<p>
Capacity Scaling:
</p>
<ul class="org-ul">
<li>\(x \gets 0, \pi \gets 0\)</li>
<li>\(\Delta \gets 2^{\lfloor\log U\rfloor}\)</li>
<li><b>while</b> &Delta; &ge; 1:
<ul class="org-ul">
<li><b>for each</b> \((i,j) \in G(x)\):
<ul class="org-ul">
<li><b>if</b> \(c_{ij}^\pi < 0\) <b>then</b> arc reversal</li>
</ul></li>
<li>SSP(\(\Delta\))</li>
<li>\(\Delta \gets \Delta / 2\)</li>
</ul></li>

<li><b>Claim:</b> \(c_{ij}^\pi \ge 0 \:\:\: \forall (i,j) \in E(x,\Delta)\) throughout SSP(\(\Delta\)).</li>
<li><b>Proof:</b> similar to SSP but \(G(x, \Delta)\) &#x2026;
<ul class="org-ul">
<li>Sketch: if we have an edge that is in \(E(x, \Delta)\) but not \(E(x, 2\Delta)\),
we cannot guarantee the claim.  But an arc reversal will fix this&#x2026;</li>
</ul></li>
</ul>

<p>
<b>Runtime:</b> \(O(n \times \log U \times S)\)
</p>
<ul class="org-ul">
<li>How do we get that?  Let's look at the total excess right after we adjust
\(\Delta\).  Due to the termination condition of SSP, we have:
<ul class="org-ul">
<li>either \(\forall i \in V: 0 < e_i < 2\Delta\),</li>
<li>or \(\forall i \in V: 0 > e_i > -2\Delta\).</li>
</ul></li>
<li>These imply:
<ul class="org-ul">
<li>either: total excess \(\le 2n\Delta\)</li>
<li>or: total deficit \(\ge -2n\Delta\)</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline47" class="outline-2">
<h2 id="orgheadline47"><span class="section-number-2">11</span> 2015-10-21 Wednesday</h2>
<div class="outline-text-2" id="text-11">
</div><div id="outline-container-orgheadline46" class="outline-3">
<h3 id="orgheadline46"><span class="section-number-3">11.1</span> MCNF Algorithms</h3>
<div class="outline-text-3" id="text-11-1">
<p>
Assumptions
</p>
<ol class="org-ol">
<li>All data is integral (this implies that solutions are integral as well).
<ul class="org-ul">
<li>An "integral part" of why the solutions are all integral.</li>
</ul></li>
<li>\(c_{ij} \ge 0 \:\:\: \forall (i,j) \in E\)
<ul class="org-ul">
<li>If you have a path that contains cycles, and all weights are
non-negative, you're better off eliminating all cycles.</li>
</ul></li>
<li>\(\forall u,v \in V \:\:\: \exists P: u \to v\) (that is, there is a path from every node
to every other one).
<ul class="org-ul">
<li>Let \(|P| \le n - 1\).  Since \(c_{ij} \ge 0\):</li>
<li>cost of \(P \le n C = n \max_{(i,j)\in E} \{c_{ij}\}\)</li>
<li>I missed the lead up to this, but: "An optimal solution should not send
flow through \(z\) if \(\exists\) another path from \(u\) to \(v\)."</li>
</ul></li>
</ol>
</div>

<div id="outline-container-orgheadline45" class="outline-4">
<h4 id="orgheadline45"><span class="section-number-4">11.1.1</span> Successive Shortest Paths</h4>
<div class="outline-text-4" id="text-11-1-1">
<p>
Imagine that you have a number line of the objective value.  You take a
feasible flow (?), then take its dual and start improving the dual until
it's feasible, and then it's optimal (?).
</p>

<p>
<b>Definition:</b> pseudo-flow \(x\) satisfies \(u_{ij}\) but not necessarily \(b_i\).
</p>
<ul class="org-ul">
<li>EG: \(x=0\).  This satisfies the upper bounds, but usually not the supplies
and demands.</li>
<li>We can also let \(\pi=0\) in the dual.  This means that the reduced costs
\(c_{ij}^\pi = c_{ij} - \pi_i + \pi_j \ge 0\), and the residual is the original
network.  This means that \(x=0\) and \(\pi=0\) are related by complementary
slackness.  So this is a starting point for SSP.</li>
</ul>

<p>
<b>Definition:</b> \(e_i \): excess at vertex \(i\) w.r.t. pseudo flow \(x\):
</p>

\begin{equation}
  e_i = b_i + \sum_{(j,i) \in E} x_{ji} - \sum_{(i,j) \in E} x_{ij}
\end{equation}

<p>
Initially, (at \(x=0\)), \(e_i = b_i\).
</p>

<p>
Then, we define the:
</p>
<ul class="org-ul">
<li>Excess vertices: \(E = \{i \in V: e_i > 0\}\)</li>
<li>Deficit vertices: \(D = \{i \in V: e_i < 0\}\)</li>
</ul>

<p>
Assuming we have a feasible solution, \(E \ne \emptyset \leftrightarrow D \ne \emptyset\) (initially?).
</p>

<p>
So, if you have two vertices, one in \(E\) and one in \(D\), you want to send
some flow between them.  How do we update the \(\pi\) values when you come up
with the new \(x\)?
</p>

<p>
<b>Claim:</b> Let \(x\) be pseudo-flows and \(\pi\) be node potentials, satisfying
complementary slackness.  We also have \(G(x)=(V, E(x))\), the residual
(residuals only use upper bounds and flows, not supplies, so we can have
residuals for pseudo-flows).  Finally, let \(c_{ij}^\pi \) be the reduced costs
for \((i,j) \in E(x)\).
</p>
<ul class="org-ul">
<li>Define \(d_i\) to be the length of the shortest path from \(s \in V\) to \(i\)
(\(\forall i \in V\)) in the residual, with lengths \(c_{ij}^\pi \).  (\(s\) is
arbitrary but fixed as some special vertex \(s \in V\)).  Since \(c_{ij}^\pi \ge
      0\), this is well defined.</li>
</ul>

<p>
OK, two results of all these assumptions and notations:
</p>
<ol class="org-ol">
<li>\(x\), \(\pi'\) satisfy complementary slackness, where \(\pi'=\pi-d\)</li>
<li>\(c_{ij}^\pi = 0 \:\:\: \forall (i,j) \in\) shortest path tree</li>
</ol>

<p>
Example (initial \(\pi = 0\))
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">i</th>
<th scope="col" class="org-right">b<sub>i</sub></th>
<th scope="col" class="org-right">d<sub>i</sub></th>
<th scope="col" class="org-right">&pi;'<sub>i</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">4</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">0</td>
<td class="org-right">2</td>
<td class="org-right">-2</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">0</td>
<td class="org-right">2</td>
<td class="org-right">-2</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">-4</td>
<td class="org-right">2</td>
<td class="org-right">-3</td>
</tr>
</tbody>
</table>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">i</th>
<th scope="col" class="org-right">j</th>
<th scope="col" class="org-right">c<sub>ij</sub></th>
<th scope="col" class="org-right">u<sub>ij</sub></th>
<th scope="col" class="org-right">c<sub>ij</sub>^&pi;</th>
<th scope="col" class="org-right">c<sub>ij</sub><sup>&pi;'</sup></th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">4</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">3</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">4</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">2</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
<td class="org-right">5</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>

<p>
The shortest path tree is: (1,2), (1,3), (3,4).  The (new) reduced costs for
those edges are all 0.  In this case, they are the only edges that have
reduced costs 0, but that's not true in the general sense.
</p>

<p>
<b>Proof:</b>  (I doubt we'll have time to finish this bad boy)
</p>

<ol class="org-ol">
<li>Proof of the first part (\(x\), \(\pi'\) satisfy complementary slackness):
<ul class="org-ul">
<li><p>
\(d_j \le d_i + c_{ij}^\pi \:\:\: \forall (i,j) \in E(x)\) (I believe this is
a Bellman-Ford inequality)
</p>
\begin{align*}
  c_{ij}^{\pi'} &= c_{ij} - \pi_{i}' + \pi_{j}' \\
       &= c_{ij} - \pi_i + d_i + \pi_j - d_j \\
       &= c_{ij}^\pi + d_i - d_j \ge 0
\end{align*}
<p>
Well, that was neat.
</p></li>
</ul></li>

<li>Proof of second part \((i,j) \in SPT\), \(c_{ij}^\pi = 0\)
<ul class="org-ul">
<li>If \((i,j) \in SPT\), then&#x2026;</li>
<li>We have $d<sub>j</sub> = d<sub>i</sub> + c<sub>ij</sub>^&pi;</li>
<li>And therefore when you do the computation above, you end up with
\(c_{ij}^{\pi'} = \dots = c_ij^\pi + d_i - d_j = 0\).  So that was also easier
than expected.</li>
</ul></li>
</ol>

<p>
Now we're talking about CS:
</p>
<ul class="org-ul">
<li>\(\alpha_{ij} (u_{ij} - x_{ij}) = 0\)</li>
<li>\(x_{ij} (c_{ij}^\pi + \alpha_{ij}) = 0\)</li>
</ul>

<p>
When you have \(c_{ij}^\pi = 0\), \(\alpha_{ij} = 0\), and so both CS conditions are
satisfied (for any flow \(x\)).
</p>

<p>
You can send any amount of flow along this SPT, and it will be optimal!  Our
job is just to find a flow that satisfies the upper bounds.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline48" class="outline-2">
<h2 id="orgheadline48"><span class="section-number-2">12</span> 2015-10-16 Friday</h2>
<div class="outline-text-2" id="text-12">
<p>
Midterm was today.
</p>
</div>
</div>

<div id="outline-container-orgheadline53" class="outline-2">
<h2 id="orgheadline53"><span class="section-number-2">13</span> 2015-10-14 Wednesday</h2>
<div class="outline-text-2" id="text-13">
<p>
<b>Update:</b> Problem 5 on the homework we went over on Monday did not actually
have a solution that involved shortest paths.  Oops!
</p>

<p>
Friday is the midterm.  (I'm a bit scared).  So I guess we'll do some review.
</p>

<ul class="org-ul">
<li>The "Reading List" syllabus on Blackboard contains all the topics.</li>
<li>There will be four questions.</li>
<li>Easier than the homework (but very similar)</li>
<li>Doubtful that we will have enough time to solve them all.  We won't be
graded out of 100% completion.</li>
<li>We should have enough time to "get a start" on them all and "write something
meaningful".</li>
<li>Doubtful that we will need to write anything in Octave.</li>
<li>Open books, open notes, open electronics (w/o internet).  Of course, this
isn't the most helpful to you if you bring everything, because you won't
have time to use it in 50 minutes.</li>
</ul>
</div>

<div id="outline-container-orgheadline49" class="outline-3">
<h3 id="orgheadline49"><span class="section-number-3">13.1</span> Back to Equipment Replacement</h3>
<div class="outline-text-3" id="text-13-1">
<ul class="org-ul">
<li>\(c_{ij}\): cost of operating machine from year \(i\) to year \(j\).  \(1 \le i < j
     \le n\).</li>
<li>Minimize the total cost s.t.
<ul class="org-ul">
<li>At every time, the number of running machines &ge; 1.</li>
</ul></li>
<li>Last time, we got to the point where we had a graph with nodes 1 through
\(n\).  Each node had an arc to all the future years, with the \(c_{ij}\)'s
labeled on it.</li>
<li>Put a supply of 1 on 1, and a supply of -1 on n.  This seems like it would
solve the problem, but it doesn't allow for us to have multiple machines
operating at one time.
<ul class="org-ul">
<li>One solution would be to have a single "source" node that connects
forward and backward with each year node.  But this allows us to send
machines "forward" in time with zero cost.  Bad.</li>
<li>My initial solution to that was to have a set of nodes 1' through n',
with edges (i',1) through (i', i-1), and then (i, i').  This allows only
backward flow.  This does solve the problem, but wih quadratic arcs.</li>
<li>The next improvement on that was to have each "prime" node connected
backward to the previous one, instead of all of the previous regular
nodes.  This improved it to linear number of arcs.</li>
<li>Then we realized that each node could just be directly connected to its
backward neighbor with cost of zero.  Oops.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline50" class="outline-3">
<h3 id="orgheadline50"><span class="section-number-3">13.2</span> Back to Uniform Cost Parallel Machines</h3>
<div class="outline-text-3" id="text-13-2">
<ul class="org-ul">
<li>\(n\) jobs, length \(l_i \le T\), \(\sum_{i=1}^n l_i \le TM\), where \(T\) is the
time unit, \(M\) the number of machines.</li>
<li>We had a max flow solution to assign job segments into time slots, now we
need to fill up the machines so that there is no overlap.
<ul class="org-ul">
<li>To do this: assign the first job into the first machine.  Assign the next
job to the remainder of the machine's time.  Then put that job's
remainder onto the next machine.  You know it won't overlap because the
segment must be &le; T.</li>
<li>Yay.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline51" class="outline-3">
<h3 id="orgheadline51"><span class="section-number-3">13.3</span> Midterm Questions</h3>
<div class="outline-text-3" id="text-13-3">
<p>
Now we talk about the midterm.
</p>

<ul class="org-ul">
<li>Mostly "solving problems"</li>
<li>They will be "like" homework problems.  Not necessarily "same question,
different numbers".  But similar in spirit.</li>
<li>There could be reductions to linear programs.</li>
<li>There could be reductions to shortest paths</li>
<li>Should probably review your linear algebra so it's easier to</li>
<li>(This is only slightly terrifying)</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline52" class="outline-3">
<h3 id="orgheadline52"><span class="section-number-3">13.4</span> Final Exam</h3>
<div class="outline-text-3" id="text-13-4">
<ul class="org-ul">
<li>30-40 minute oral exam!!!</li>
<li>Will be plenty of talk on concepts, explaining yourself, etc.</li>
<li>(This is actually a lot terrifying)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline55" class="outline-2">
<h2 id="orgheadline55"><span class="section-number-2">14</span> 2015-10-12 Monday</h2>
<div class="outline-text-2" id="text-14">
</div><div id="outline-container-orgheadline54" class="outline-3">
<h3 id="orgheadline54"><span class="section-number-3">14.1</span> Homework 3 Review</h3>
<div class="outline-text-3" id="text-14-1">
<p>
<b>Problem 1:</b> when converting nodes with capacities into pairs of nodes
without capacities, you assign the supplies to the input/output node by the
following rule:
</p>
<ul class="org-ul">
<li>If \(b_i > 0\), then assign it to the output node.</li>
<li>If \(b_i < 0\), then assign it to the input node.</li>
<li>Otherwise, it doesn't really matter, both nodes get 0.</li>
</ul>

<p>
Also on problem 1, make sure when you draw graphs you explain what your
numbers mean.  A good policy is to include a legend.
</p>

<p>
Finally, it's generally better to get rid of capacities first, because we
don't have solid rules on how to get rid of lower bounds with nodes that have
capacities.
</p>

<p>
For my case, I know I forgot to decrease the upper bound of \((c,d)\) when I
removed the lower bound.  My other error was putting the negative \(b_i\) on the
output node.
</p>

<p>
<b>Problem 4:</b> The strategies for finding \(\pi_i\) were varied.  Some wrote the
primal, and then found the dual, and then solved it with Octave, and then
used the dual solution.  Others guessed and checked.  Others used the \(c_{ij} -
   \pi_i + \pi_j \ge 0\) condition and found a feasible solution to that (which was
not too hard given the number of parallel arcs).  The other way was to pick a
starting node from which all other nodes are reachable, and then find the
shortest to each node.  The negative of these values are suitable to use as
node potentials.  Admittedly, in my solution, I didn't show any work for the
node potentials, but I used the shortest path method.
</p>

<p>
<b>Problem 5:</b> Here is one solution to a: 2 3 2 3 2.  Here, Alice always loses
no matter what she chooses.  My solution is similar to that (1 6 7 5 8 4 9 3
10 2).  This sort of solution may trick you into believing that if Alice has
any winning strategy, the best choice is to choose the max chip.  This is not
true.  Consider this arrangement: 10 1000 3 4.  Alice should choose 4 in
order to protect the 1000, forcing Bob to open it up for her to take on the
next turn.  This strategy requires that she remove the lower of the choices.
</p>

<p>
One way to solve part b is to formulate the solution recursively, and come up
with a dynamic programming algorithm.  Then do as in class and recast the
dynamic programming solution as a shortest path solution.
</p>

<p>
Another way to do it would be to do game trees, where you represent each node
as a series of decisions, and its children are the results of all the
possible decisions from that node.  But this has \(2^n\) nodes, which is pretty
bad.  There is also the problem that a shortest path solution will minimize
the strategy of Bob, which doesn't really do what we want.
</p>

<p>
Dr. Liberatore's solution is to formulate each node in a graph as a list of
chips, and make transitions a full round (Alice, Bob).  We identified a
similar issue to the previous strategy (where it minimizes Bob's strategy).
Hopefully next class we'll get it figured out.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline58" class="outline-2">
<h2 id="orgheadline58"><span class="section-number-2">15</span> 2015-10-09 Friday</h2>
<div class="outline-text-2" id="text-15">
</div><div id="outline-container-orgheadline56" class="outline-3">
<h3 id="orgheadline56"><span class="section-number-3">15.1</span> Uniform Parallel Machines</h3>
<div class="outline-text-3" id="text-15-1">
<p>
You have a set of \(n\) jobs:
</p>
<ul class="org-ul">
<li>\(s_i\): start time of job \(i\)</li>
<li>\(d_i\): deadline of job \(i\)</li>
</ul>

<p>
It seems like the idea is to schedule each job to a time interval on a machine
so that everything happens.  The claim is that this reduces to Max Flow.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Job</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
</tr>

<tr>
<td class="org-left">r<sub>i</sub></td>
<td class="org-right">1.5</td>
<td class="org-right">1.25</td>
<td class="org-right">2.1</td>
<td class="org-right">3.6</td>
</tr>
</tbody>
</table>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Interval</th>
<th scope="col" class="org-left">Jobs</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">1,3</td>
<td class="org-left">2</td>
</tr>

<tr>
<td class="org-left">3,4</td>
<td class="org-left">1,2,3</td>
</tr>

<tr>
<td class="org-left">4,5</td>
<td class="org-left">1,3</td>
</tr>

<tr>
<td class="org-left">5,7</td>
<td class="org-left">3,4</td>
</tr>

<tr>
<td class="org-left">7,9</td>
<td class="org-left">4</td>
</tr>
</tbody>
</table>

<p>
We create a network with a source, and a sink.  We create nodes for the jobs,
and we create nodes for each time interval.  We create arcs from the source to
each time interval, with capacity of \(M * (f_i - s_i)\).  Then we create nodes
for each job.  We create arcs from the jobs to the sink with capacity of \(r_i
  \).  Finally, we create arcs from the intervals to the jobs, with capacity
\(f_i - s_i \).
</p>

<p>
To go from a flow to a schedule:  If we have \(x_{(h,k), i} > 0\), we schedule
\(i\) in \([h,k]\) for \(x_{(h,k),i}\) time.  We may have to do some finagling to
ensure that everything fits, but we'll come back to that issue.  In any case,
we can also go from a schedule to a flow.
</p>

<p>
A schedule/flow is feasible if every job &rarr; sink link is at capacity.
</p>
</div>
</div>

<div id="outline-container-orgheadline57" class="outline-3">
<h3 id="orgheadline57"><span class="section-number-3">15.2</span> Equipment Replacement</h3>
<div class="outline-text-3" id="text-15-2">
<p>
When you buy a machine, it operates very nicely at the beginning, but then
deteriorates and you need to repair it.  You'll need to repair it more and
more and more as you go.  At some point, it becomes cheaper to replace it
with a new machine than to continue repair it.
</p>

<p>
So, say you buy a machine at time \(i\) and sell at time \(j\).
</p>

<ul class="org-ul">
<li>\(c_{ij}\): total cost of operating machine from time \(i\) to \(j\).  This
includes the cost of purchase, minus the sell price, plus repair prices.</li>
</ul>

<p>
At least one machine must be operating at all times.  The problem is to find
an optimal replacement schedule.  The claim is that this reduces to MCNF.  We
are given a discrete set of times 1 through \(n\) to buy and sell at.
</p>

<p>
You can create a network with a node for each time.  Each time \(i\) is
connected to each future node \(j\) with cost \(c_{ij}\).  You set the supply at \(1\)
to 1, and supply at \(n\) to -1.  The one problem is that this doesn't allow
for more than one machine to be running at a time.  So, we're trying to solve
that by adding a source and sink node, but things aren't working out
perfectly.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline61" class="outline-2">
<h2 id="orgheadline61"><span class="section-number-2">16</span> 2015-10-07 Wednesday</h2>
<div class="outline-text-2" id="text-16">
</div><div id="outline-container-orgheadline59" class="outline-3">
<h3 id="orgheadline59"><span class="section-number-3">16.1</span> Max Flow Reduction: Matrix Rounding</h3>
<div class="outline-text-3" id="text-16-1">
<p>
A government agency collects data via a census, in the following matrix \(A\):
</p>

\begin{bmatrix}
  3.1 & 6.8 & 7.3 \\
  9.6 & 2.4 & 0.7 \\
  3.6 & 1.2 & 6.5 \\
\end{bmatrix}

<p>
Where zip codes are along the x-axis, and tax brackets are along the y-axis.
Given enough such matrices to cross-reference, a nefarious individual would
be able to identify exactly which members of their community are in which
tax brackets. So, the agency would like to obfuscate the data in the matrix.
</p>

<p>
Create a new matrix \(B\) such that \(b_{ij}\) is either \(\lfloor{}a_{ij}\rfloor\) or
\(\lceil{}a_{ij}\rceil\) However, \(B\) should also obfuscate the sums of both the columns
and rows of \(A\) in the same manner (so, \(\sum_{i}b_{ij}\) should be either
\(\lfloor{}\sum_{i}a_{ij}\rfloor\) or \(\lceil{}\sum_{i}a_{ij}\rceil\), and so on, for each row and
column).
</p>

<p>
A ``consistent rounding'' satisfies all these constraints.
</p>

<p>
Question: Given matrix \(A\), find a consistent rounding.
Claim: Reduces to max flow.
</p>

<p>
See the (hopefully) included image.
</p>


<div class="figure">
<p><img src="./matrix_rounding_max_flow.jpg" alt="matrix_rounding_max_flow.jpg" />
</p>
<p><span class="figure-number">Figure 1:</span> (Hopefully) included image.</p>
</div>

<p>
Let \(r_i\) be the sum of row i, and \(c_j\) be the sum of column j.
</p>

<p>
The idea is to have a node for each element of \(A\), as well as nodes for the
column and row sums. Then, make an arc going from \(a_{ij}\) to \(c_j\), with lower
bound of \(\lfloor{}a_{ij}\rfloor_{}\) and \(\lceil{}a_{ij}\rceil\). Also, make an arc going
from \(r_i\) to \(a_{ij}\) with the same upper and lower bounds.
</p>

<p>
Now, make an arc going from each \(c_j\) to a sink node \(t\). Each arc will have
lower bound \(\lfloor{}c_j \rfloor\) and upper bound \(\lceil{}c_{j}\rceil\). Similarly,
make an arc going from a source node \(s\) to each \(r_i\). Each arc will have
upper bound \(\lfloor{}r_{i}\rfloor\) and upper bound \(\lceil{}r_{i}\rceil\).
</p>

<p>
This is sufficient to formulate matrix rounding as a max flow problem. Note
that there is actually a second formulation, which reverses the directions
of all the arcs.
</p>
</div>
</div>

<div id="outline-container-orgheadline60" class="outline-3">
<h3 id="orgheadline60"><span class="section-number-3">16.2</span> Max Flow Reduction: Job Scheduling</h3>
<div class="outline-text-3" id="text-16-2">
<p>
The remainder of class was spent talking about the job scheduling problem,
which appears to be described in full on the notes for 2015/10/09.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline65" class="outline-2">
<h2 id="orgheadline65"><span class="section-number-2">17</span> 2015-10-05 Monday</h2>
<div class="outline-text-2" id="text-17">
</div><div id="outline-container-orgheadline62" class="outline-3">
<h3 id="orgheadline62"><span class="section-number-3">17.1</span> Max Flow</h3>
<div class="outline-text-3" id="text-17-1">
<p>
<b><b>def</b></b> Max flow: G = (V, E), with \(\forall (i,j)\:u_{ij},\text{zero costs, supplies,
  and demands}\). One edge going from source to sink with no upper bound,
-1 cost. The optimal solution will maximize the flow from source to sink.
</p>
</div>
</div>

<div id="outline-container-orgheadline63" class="outline-3">
<h3 id="orgheadline63"><span class="section-number-3">17.2</span> Feasible Flow Problem</h3>
<div class="outline-text-3" id="text-17-2">
<p>
<b><b>def</b></b> Feasible flow: Given a network with supplies and upper bounds,
show that a flow exists.
</p>

<p>
Claim: Feasible flow reduces to Max flow.
</p>

<p>
Steps:
</p>
<ul class="org-ul">
<li>(remove lower bounds, if needed)</li>
<li>from all sinks, put edge to t with \(u_{it}=-b_{i}\)</li>
<li>to all sources, put edge from s with \(u_{si}=b_{i}\)</li>
<li>set supplies/demands of all nodes to zero</li>
<li>connect t to s, -1 cost, no upperbound</li>
<li>Solve max flow. If all upper bounds met going into t / out of s,</li>
</ul>
<p>
(i.e. final supplies/demands are 0) then there is a feasible flow.
</p>
</div>
</div>

<div id="outline-container-orgheadline64" class="outline-3">
<h3 id="orgheadline64"><span class="section-number-3">17.3</span> Reductions</h3>
<div class="outline-text-3" id="text-17-3">
<p>
To prove a reduction from one problem to another, show a 1-to-1 mapping
between solutions to each problem.
</p>

<p>
Ex. Show feasible flow reduces to max flow:
Proof: (\(feasible flow \Rightarrow max flow \)
</p>
<ul class="org-ul">
<li>Let x be a feasible flow. Let \(y_{ij}=\begin{cases}x_{ij}\text{, if}(i,j)\in E \\
                                                      u_{ij}\text{, if}(i,j)\notin E
                                        \end{cases}\)</li>
<li>We know the value of the max flow \(\le \sum u_{si}\) and the value of the y
flow \(&le; &sum; y<sub>si</sub> = &sum; b<sub>i</sub> \mathbb{1}<sub>bi &ge; 0</sub> &ge;) value of max flow. So,
If val(max flow) == val(y flow), y flow is optimal.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline69" class="outline-2">
<h2 id="orgheadline69"><span class="section-number-2">18</span> 2015-10-02 Friday</h2>
<div class="outline-text-2" id="text-18">
<p>
<b>MCNF: special cases and applications</b>
</p>

<p>
Taking problems and mapping them to MCNF is preferable because:
</p>
<ul class="org-ul">
<li>MCNF is faster that LP</li>
<li>MCNF is unimodular, so you can drop integrality constraints.</li>
<li>This makes it attractive.  How can we solve other problems with it?</li>
</ul>
</div>

<div id="outline-container-orgheadline66" class="outline-3">
<h3 id="orgheadline66"><span class="section-number-3">18.1</span> Shortest Paths</h3>
<div class="outline-text-3" id="text-18-1">
<p>
Given:
</p>
<ul class="org-ul">
<li>directed graph \(G=(V,E)\)</li>
<li>\(s \in V\)</li>
<li>lengths \(c_{ij}\) associated with \((i,j)\in E\).</li>
</ul>

<p>
Find the shortest path from \(s\) to \(u\).  \(u \in V - \{s\}\).
</p>

<p>
I feel like after EECS 340, I don't really need to copy down the graph we're
using for demonstration.  So, <i>shortest paths are a special case of MCNF</i>.
</p>
<ul class="org-ul">
<li>Set costs to the lengths of the edges.</li>
<li>Lower bounds 0</li>
<li>Upper bounds infinite</li>
<li>Set source node supply to \(|V-\{s\}|\), and all other nodes to \(-1\).</li>
</ul>

<p>
If you get fractional solution for a single solution, you could use either
flow.  Note that people really don't try to use MCNF to solve shortest paths;
rather, they sometimes use repeated Dijkstra's algorithm to solve MCNF
(instead of LP).
</p>

<ul class="org-ul">
<li>\(\min \sum_{(i,j)\in E} c_{ij} x_{ij}\), s.t.</li>
<li>\(\sum_{(s,j) \in E} x_{sij} - \sum_{(j,s) \in E} x_{js} = n - 1\)</li>
<li>\(\sum_{(i,j) \in E} x_{ij} - \sum_{(j,i) \in E} x_{ji} = -1\) for \(i &isin; V-\{s\})</li>
<li>\(x_{ij} > 0\), for \((i,j) \in E\)</li>
</ul>

<p>
The dual:
</p>

<ul class="org-ul">
<li>\(\max (n-1) \pi_s - \sum_{i \in V-\{s\}} \pi_i\), s.t.</li>
<li>\(c_{ij} - \pi_i + \pi_j \ge 0\), for \((i,j) \in E\)</li>
<li>&pi;'s unrestricted.</li>
</ul>

<p>
Let's look at the objective function and make a change of variable:
</p>

\begin{equation}
  (n-1) \pi_s - \sum_{i \in V-\{s\}} \pi_i = n \pi_s - \sum_{i\in V} \pi_i = \sum_{i \in V} (\pi_s - \pi_i) = \sum_{i\in V} d_i
\end{equation}

<p>
So, we change variable to \(d_i = \pi_s - \pi_i\).  Above is the new objective
function using the change of variable.  Now we'll rewrite the whole thing
using \(d_i\).
</p>

<ul class="org-ul">
<li>\(\max \sum_{i\in V} d_i \), s.t.</li>
<li>\(d_j \le d_i + c_{ij}\), for all \((i,j) \in E\)</li>
</ul>

<p>
The intuition is that \(d_j \) is the distance from the source to node \(j\).
We are maximizing, which seems funny, but it makes sense when you realize
that the constraints are holding back the whole thing.  The constraints here
are the Bellman-Ford optimality conditions, which are what Dijkstra's
Algorithm does at every step (and of course the Bellman-Ford algorithm).
</p>
</div>
</div>

<div id="outline-container-orgheadline67" class="outline-3">
<h3 id="orgheadline67"><span class="section-number-3">18.2</span> Dynamic Programming!!!!!</h3>
<div class="outline-text-3" id="text-18-2">
<p>
Dynamic programming is an algorithm design technique, not an algorithm in
itself.  However, we're going to make a sweeping claim here: "all dynamic
programming is a special case of network flow".  (<code>say-whaaaaat.gif</code>)
</p>


<div class="figure">
<p><img src="http://static2.therichestimages.com/wp-content/uploads/2014/06/yeahright-anim.gif" alt="yeahright-anim.gif" />
</p>
<p><span class="figure-number">Figure 2:</span> Say Whaaaaat?</p>
</div>

<p>
<b>Weighted Interval Scheduling</b>: You have \(n\) intervals \((s_i, f_i), v_i \).
Find the subset of non-overlapping intervals with maximum value.
</p>

<p>
Solution: \(M(i)\) is the maximum value from scheduling intervals 1, 2, &#x2026;,
i.  We want \(M(n)\).  We do know that \(M(0) = 0\).  Recursively:
</p>

\begin{equation}
  M(i) = \max \left\{ M(i-1), v_i + M(p(i))\right\}\
\end{equation}

<p>
Here, \(p(i)\) is the first predecessor interval that doesn't overlap with
\(i\).  The two conditions are either (1) we don't choose interval \(i\), or (2),
we do choose interval \(i\).  In the solution algorithm, we keep a vector of
\(M(i)\) values and compute it from \(0\) up to \(n\), and that value is your
optimal value.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-left">&#x2026;</td>
<td class="org-left">p(i)</td>
<td class="org-left">&#x2026;</td>
<td class="org-left">i-1</td>
<td class="org-left">i</td>
<td class="org-left">&#x2026;</td>
<td class="org-left">n</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">M(p(i))</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">M(i-1)</td>
<td class="org-left">M(i)</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">M(n)</td>
</tr>
</tbody>
</table>

<p>
Imagine that in this illustration, we have arrows going from the vector slot
\(i\) back to \(i-1\) and \(p(i)\).
</p>

<p>
Now, we start making broad, sweeping claims!  This is just a MCNF problem!
Make each element in the vector a node in a graph, and put edges from
\(p(i)\) to \(i\), and \(i-1\) to \(i\). The edges are labeled with the gain
you get \(-v_i\).  You can then run MCNF on this, and get an optimal solution.
You can do this with any Dynamic Programming algorithm that you have the
recursive formulation of, and are stored in a tabular format like this.
</p>

<p>
So essentially, the dynamic programming and recursive/network way of thinking
are pretty equivalent, and it usually depends on the person and how they've
been thinking previously.  But the solutions you come up with as a result may
have slightly different advantages and disadvantages.
</p>
</div>
</div>

<div id="outline-container-orgheadline68" class="outline-3">
<h3 id="orgheadline68"><span class="section-number-3">18.3</span> Circulation</h3>
<div class="outline-text-3" id="text-18-3">
<p>
Circulation problems are pretty much MCNF, except that all supplies are 0.
Sometimes, you have lower bounds, so you are forced to send some flow.  Of
course, then you could eliminate the lower bounds as we've discussed, and
then end up with a problem that has supplies and demands that are nonzero.
</p>

<p>
However, you can have problems with no supply/demand and no lower bounds,
which have an optimal flow that does send some flow.
</p>

<p>
<b>Example Network</b>
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">i</td>
<td class="org-right">j</td>
<td class="org-right">c<sub>ij</sub></td>
<td class="org-right">l<sub>ij</sub></td>
<td class="org-right">u<sub>ij</sub></td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">-1</td>
<td class="org-right">0</td>
<td class="org-right">3</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">+1</td>
<td class="org-right">2</td>
<td class="org-right">&#xa0;</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">5</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">-2</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">4</td>
</tr>

<tr>
<td class="org-right">5</td>
<td class="org-right">4</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
Next week is a regular week.  Then homework solutions and midterm review.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline71" class="outline-2">
<h2 id="orgheadline71"><span class="section-number-2">19</span> 2015-09-30 Wednesday</h2>
<div class="outline-text-2" id="text-19">
</div><div id="outline-container-orgheadline70" class="outline-3">
<h3 id="orgheadline70"><span class="section-number-3">19.1</span> MCNF Duality</h3>
<div class="outline-text-3" id="text-19-1">
<p>
Recall from last time:
</p>
<ul class="org-ul">
<li>&pi;<sub>i</sub>: node potential at node i &isin; V</li>
<li>&alpha;<sub>ij</sub> = max{0, -c<sub>ij</sub>^&pi;}, (i,j) &isin; E</li>
<li>where c<sub>ij</sub>^&pi; = c<sub>ij</sub> - &pi;<sub>i</sub> + &pi;<sub>j</sub></li>
</ul>

<p>
Here's his network from last time:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">Node</td>
<td class="org-right">Supply</td>
<td class="org-right">&pi;<sub>i</sub></td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">4</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">0</td>
<td class="org-right">-2</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">0</td>
<td class="org-right">-3</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">-4</td>
<td class="org-right">-4</td>
</tr>
</tbody>
</table>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">Start</td>
<td class="org-right">End</td>
<td class="org-right">Cost</td>
<td class="org-right">Upper</td>
<td class="org-right">c<sub>ij</sub>^&pi;</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">4</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">3</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">-1</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">4</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
<td class="org-right">5</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>

<p>
We have an (optimal) flow as follows:
</p>
<ul class="org-ul">
<li>2 units go 1 &rarr; 2 &rarr; 3 &rarr; 4</li>
<li>2 units go 1 &rarr; 3 &rarr; 4</li>
</ul>

<p>
Then he wrote the residual network, with &pi;<sub>i</sub> and c<sub>ij</sub>^&pi; copied down.  c<sub>ij</sub>^&pi;
can be copied for the arcs that still exist, but needs to be recalculated for
each new arc.  However, if a new arc is the opposite of an old one, you know
that \(c_{ji}^\pi = -c_{ij}^\pi\).  Here are the values for the residual network.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">Node</td>
<td class="org-right">Supply</td>
<td class="org-right">&pi;<sub>i</sub></td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">0</td>
<td class="org-right">-2</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">0</td>
<td class="org-right">-3</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">0</td>
<td class="org-right">-4</td>
</tr>
</tbody>
</table>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">Start</td>
<td class="org-right">End</td>
<td class="org-right">Cost</td>
<td class="org-right">Upper</td>
<td class="org-right">c<sub>ij</sub>^&pi;</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">1</td>
<td class="org-right">-2</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">-2</td>
<td class="org-right">2</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">2</td>
<td class="org-right">-1</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">4</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">3</td>
<td class="org-right">-1</td>
<td class="org-right">4</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>

<p>
The residual network can be a multigraph.  If you have node \(i\) and \(j\)
connected a forward and backward arc in the original network, you could have
each arc causing two in the residual.  If an edge \(e\) causes new edge \(f\) in
the same direction and \(g\) in the opposite, then you have that \(c_e^\pi =
   c_f^\pi = - c_g^\pi\).  We say \(e\) originates \(g\) and \(f\).
</p>

<p>
Ok.  We're kinda caught up to where we were at the end of the last MCNF
duality lecture.  Last time we conjectured that:
</p>

<p>
<b>Theorem:</b> x<sub>ij</sub>, &pi;<sub>i</sub>, (&alpha;<sub>ij</sub>) feasible:
</p>
<ul class="org-ul">
<li><p>
x<sub>ij</sub>, &pi;<sub>i</sub>, (&alpha;<sub>ij</sub>) optimal iff: \(c_{ij}^\pi \ge 0 \:\: \forall (i,j) \in E(x)\).
</p>

<p>
Here, \(E(x)\) is the arcs in the residual network.
</p></li>
</ul>

<p>
<b>Proof:</b> (&rarr;) x<sub>ij</sub>, &pi;<sub>i</sub> optimal.
</p>
<ul class="org-ul">
<li>This gives us:
<ul class="org-ul">
<li>wlog \(\alpha_{ij} = \max \{0, -c_{ij}^pi \}\)</li>
<li>complementary slackness!
<ul class="org-ul">
<li>(1): \(\alpha_{ij} (u_{ij} - x_{ij}) = 0\)</li>
<li>(2): \(x_{ij} (c_{ij}^\pi + \alpha_{ij}) = 0\)</li>
</ul></li>
</ul></li>
<li>\(\forall (h,k) \in E(x)\), \(\exists! (i,j) \in E\) that originates it.  \(\exists!\) means
"there exists a unique".
<ul class="org-ul">
<li>Case a: x<sub>ij</sub> = 0 originates (i,j) &isin; E(x)
<ul class="org-ul">
<li>By contradiction, c<sub>ij</sub>^&pi; &lt; 0, so a<sub>ij</sub> &gt; 0.</li>
<li>By the complementary slackness condition (1), \(x_{ij} = u_{ij}\), which is a
contradiction.</li>
</ul></li>
<li>Case b: x<sub>ij</sub> = u<sub>ij</sub> originates (j, i) &isin; E(k) (but not (i,j), since there is
no more flow to send forward).
<ul class="org-ul">
<li>By contradiction, c<sub>ji</sub>^&pi; &lt; 0, which means c<sub>ij</sub>^&pi; &gt; 0.</li>
<li>This means that \(c_{ij}^\pi + \alpha_{ij} > 0\) (since c<sub>ij</sub>^&pi; &gt; 0 and &alpha;<sub>ij</sub> &ge; 0).</li>
<li>According to complementary slackness condition (2), \(x_{ij} = 0\), which
is again a contradiction!</li>
</ul></li>
<li>Case c: 0 &lt; x<sub>ij</sub> &lt; u<sub>ij</sub> originates (i,j) and (j,i):
<ul class="org-ul">
<li>By contradiction, assume \(c_{ij}^\pi < 0\).  By the same argument in case
a, we get x<sub>ij</sub>=u<sub>ij</sub>, which is a contradiction.</li>
<li>By contradiction, assume \(c_{ji}^\pi < 0\), By the same argument in case
b, we get x<sub>ij</sub> = 0, which is a contradiction.</li>
</ul></li>
</ul></li>
<li>This completes the proof.  We use the fact that x<sub>ij</sub> and &alpha;<sub>ij</sub> are optimal to
get complementary slackness.  That's the key to the proof.</li>
</ul>

<p>
<b>Proof:</b> (&larr;) We know \(c_{ij}^\pi \ge 0\), \(\forall (i,j) \in E(x)\).  Need to
prove \(x_{ij}^\pi\) and \(\pi_i\) are optimal.
</p>
<ul class="org-ul">
<li>To do this, we would want to show that for each arc (h,k) &isin; E, we have
the complementary slackness conditions written above.</li>
<li>Case a: x<sub>hk</sub> = 0:
<ul class="org-ul">
<li>CS condition (2) follows immediately.</li>
<li>Also, since \(x_{hj}=0\), we know that \((h,k) \in E(x)\), which by our initial
assumption means that \(c_{ij}^\pi \ge 0\).  Since \(\alpha_{hk} = \max\{0,
       -c_{hj}^\pi\}\), we know \(\alpha_{hk}=0\), and therefore condition (1) is also
satisfied.</li>
</ul></li>
<li>Case b: x<sub>hj</sub> = u<sub>hk</sub>:
<ul class="org-ul">
<li>CS condition (1) follows immediately.</li>
<li>Since arc (h,k) is at capacity, we know that \((k,h)\in E(x)\).  So, we
know that \(c_{kh}^\pi \ge 0\).  This means that in the original problem,
\(c_{hk}^\pi \le 0\) (that's how we would have calculated \(c_{kh}\).</li>
<li>By our definition of \(\alpha_{hk} = - -c_{hk}^\pi\), which means that condition (2)
is satisfied!</li>
</ul></li>
<li>Case c: 0 &lt; x<sub>hk</sub> &lt; u<sub>hk</sub>:
<ul class="org-ul">
<li>Now we know that both \((h,k)\) and \((k,h)\) are in \(E(x)\).</li>
<li>This means \(c_{hk}^\pi, c_{kh}^\pi \ge 0\), which means \(c_{hk}^\pi = c_{kj}^\pi =
       \alpha_{hk} = 0\), which gives us both complementary slackness conditions!</li>
</ul></li>
<li>QED</li>
</ul>

<p>
As a recap, this proof gives us the approach of most of the MCNF algorithms.
They start with a feasible flow, find the violations in the \(c_{ij}^\pi \ge 0\)
condition in the residual, and update the flow to improve it.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline72" class="outline-2">
<h2 id="orgheadline72"><span class="section-number-2">20</span> 2015-09-28 Monday</h2>
<div class="outline-text-2" id="text-20">
<p>
<i>Looks like we're going over homework 2.</i>
</p>

<ul class="org-ul">
<li>It appears that the function <code>numpy.invert()</code> doesn't actually invert a
matrix.  The function for that is <code>numpy.linalg.inv()</code>.  Who names a
function <code>invert()</code>, when it doesn't actually invert a matrix?  Damn.</li>
<li>When you have a linear program with matrix \(A\), you can show that all BFS of
that program will be integer by showing that: \(A\) has full row rank!!, and
then that \(A\) is unimodular.
<ul class="org-ul">
<li>Turns out we needed to do all 8x8 bases, not 9x9 bases.  In which case,
checking all (10 choose 8) bases would be too much, so a script was
necessary here.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline74" class="outline-2">
<h2 id="orgheadline74"><span class="section-number-2">21</span> 2015-09-25 Friday</h2>
<div class="outline-text-2" id="text-21">
</div><div id="outline-container-orgheadline73" class="outline-3">
<h3 id="orgheadline73"><span class="section-number-3">21.1</span> MCNF Duality</h3>
<div class="outline-text-3" id="text-21-1">
<p>
Assume for simplicity, &forall; (i,j) &isin; E, 0 &lt; u<sub>ij</sub> &lt; &infin;.  This condition doesn't
provide any algorithmic restrictions, it just makes our notation simpler.
</p>

<ul class="org-ul">
<li>\(\min \sum_{(i,j)\in E} c_{ij} x_ij \), s.t.</li>
<li>\(\sum_{(i,j)\in E} x_{ij} - \sum_{(j,i)\in E} x_ji = b_i\)  for all i</li>
<li>\(-x_{ij} \ge -u_{ij}\) for all (i,j).</li>
<li>\(x_ij \ge 0\) for all (i,j)</li>
</ul>

<p>
The dual: this should be pretty automatic by now.  We will use variables &pi;<sub>i</sub>
for the top constraint, and &alpha;<sub>ij</sub> for the second constraint
</p>

<ul class="org-ul">
<li>\(\max \sum_{i\in V} b_i \pi_i - \sum_{(i,j) \in E} u_{ij} \alpha_{i,j}\), s.t.</li>
<li>\( \pi_i - \pi_j - \alpha_{ij} \le 0\), for all i,j
<ul class="org-ul">
<li>rewrite: \(c_{ij} - \pi_i + \pi_j + \alpha_{ij} \ge 0\)</li>
</ul></li>
<li>\(\alpha_{ij} \ge 0\)</li>
<li>&pi;<sub>i</sub> unrestricted</li>
</ul>

<p>
Now that we have the dual, the next step is to ask, "what does it mean"?  If
we figure out what the dual means, we can learn many properties about the
original problem and maybe come up with more efficient algorithms for it.
</p>

<p>
<b>Claim:</b> If the dual has an optimal solution, then &exist; optimal solution where
\(\alpha_{ij} = \max \{0, -c_{ij}^\pi \}\).
</p>
<ul class="org-ul">
<li><b>Definition:</b> \(c_{ij}^\pi = c_{ij} - \pi_i + \pi_j\)</li>
<li>Called the reduced costs for the arc (i,j).</li>
<li>But not the same as the reduced costs as its linear program (?).
<ul class="org-ul">
<li>That doesn't really matter to me since I never got the reduced costs thing
anyway.</li>
</ul></li>
</ul>

<p>
<b>Proof:</b> We already know that \(a_{ij} \ge 0\), and \(\alpha_{ij} \ge - c_{ij}^\pi\).
The claim states that this is a strict equality, and apparently this is true
becasue the profit of \(a_{ij}\) is \(-u_{ij}\), which means we need \(a_{ij}\) to be as
small as possible.
</p>

<p>
<b>Theorem:</b> (Complementary Slackness).  If \(x_{ij}\) is feasible, and \(\pi_i, a_{ij}\)
are feasible, then they are optimal iff:
</p>
<ul class="org-ul">
<li>\(\alpha_{ij}(u_ij - x_{ij}) = 0\)</li>
<li>\(x_{ij} (c_{ij}^\pi + a_{ij}) = 0\)</li>
</ul>

<p>
<b>Define:</b> Excess at \(i\): \(e_i = b_i - \sum_{(i,j)\in E} x_{ij} + \sum_{(j,i) \in E} x_{ij}\)
</p>
<ul class="org-ul">
<li>\(e_i=0\) in any feasible solution.</li>
</ul>

<p>
<b>Proof:</b> (of the complementary slackness statement above):
</p>
<ul class="org-ul">
<li>We write the LP complementary slackness conditions.</li>
<li>\(\pi_i e_i = 0\)</li>
<li>\(\alpha_{ij} (u_{ij}-x_{ij}) = 0\)</li>
<li>\(x_{ij} (c_{ij}^\pi + \alpha_{ij}) = 0\)</li>
<li>The first is always true (as we mentioned above with the excess)</li>
<li>The other two are the complementary slackness conditions from the theorem.</li>
</ul>

<p>
Example time (not pictured):
</p>
<ul class="org-ul">
<li>He gave a network and a feasible flow.</li>
<li>He claims it's optimal.</li>
<li>To prove it, he'll give a corresponding solution to the dual!</li>
<li>\(\pi_{j}\) is called the "node potential".</li>
<li>He gave us the values for \(\pi_i\) at each node.  The \(\alpha_{ij}\) follow from
complementary slackness, having the values of \(x_{ij}\) and \(\pi_i\).</li>
<li>We then computed the values for \(c_{ij}^\pi\), and used those to compute
\(\alpha_{ij}\), and showed that they satisfy complementary slackness, and are
feasible.</li>
<li>Then, we wrote the residual network.  Recall, you do this by replacing each
edge that has a flow with an edge in the same direction, same cost, and ub
is \(u_{ij}-x_{ij}\).  And, you add a reverse edge with negative cost and ub is
\(x_{ij}\)</li>
<li>Then, we mapped the reduced costs \(c_{ij}^\pi\) onto the residual network.  For
the new back edges \((i,j)\), their reduced costs are just \(-c_{ji}^\pi\).</li>
<li>We notice that \(c_{ij}^\pi \ge 0\) for all edges in the residual network.  We
denote that \(\forall (i,j) \in E(x)\).</li>
<li>We make the claim that \(x_{ij}\) and \(\pi_i\) are optimal iff \(c_{ij}^\pi \ge 0\) forall
edges in the residual network.</li>
<li>Also, we point out that if \(c_{ij}^\pi > 0\), then that arc is a binding
constraint on the flow (i.e., that arc is at capacity).</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline81" class="outline-2">
<h2 id="orgheadline81"><span class="section-number-2">22</span> 2015-09-23 Wednesday</h2>
<div class="outline-text-2" id="text-22">
</div><div id="outline-container-orgheadline78" class="outline-3">
<h3 id="orgheadline78"><span class="section-number-3">22.1</span> MCNF: Transformations</h3>
<div class="outline-text-3" id="text-22-1">
<p>
Have an instance of MCNF, and I want to transform it into another instance
which is equivalent.
</p>
</div>

<div id="outline-container-orgheadline75" class="outline-4">
<h4 id="orgheadline75"><span class="section-number-4">22.1.1</span> Removing lower bounds</h4>
<div class="outline-text-4" id="text-22-1-1">
<ul class="org-ul">
<li>If you have an arc from i to j, with b<sub>i</sub> and b<sub>j</sub>, with cost c<sub>ij</sub>, and bounds
(&ell;<sub>ij</sub>, u<sub>ij</sub>).</li>
<li>Create a new arc with bounds (0, u<sub>ij</sub>-&ell;<sub>ij</sub>), and you make node i
b<sub>i</sub>-&ell;<sub>ij</sub>, and node j has b<sub>j</sub>+&ell;<sub>ij</sub>.</li>
<li>Essentially, this is an equivalent problem where you disregard the required
flow.  The cost that's eliminated is constant, so the solution to this will
be the same to the solution of the original.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline76" class="outline-4">
<h4 id="orgheadline76"><span class="section-number-4">22.1.2</span> Removing upper bounds</h4>
<div class="outline-text-4" id="text-22-1-2">
<ul class="org-ul">
<li>Again, you have i and j, with supply b<sub>i</sub>/b<sub>j</sub>, cost c<sub>ij</sub>, and upper bound u<sub>iu</sub>.</li>
<li>i is flowing to j, with an amount x.</li>
<li>Create a new "node" that has supply -u<sub>ij</sub>.  Both i and j will flow into the
new node.  x will still flow from node i into the new node.</li>
<li>The cost of the flow from i to the new node will be c<sub>ij</sub></li>
<li>The cost of the flow from j into the new node will be 0.</li>
<li>The supply of node i is still b<sub>i</sub>.</li>
<li>The supply of j becomes b<sub>j</sub> + u<sub>ij</sub>.  Essentially, the flow from j to the new
node will be u<sub>ij</sub> - x, and when you subtract that from the supply, you will
get b<sub>ij</sub> + x, which is the flow from the original setup.</li>
</ul>

<p>
Example: he's doing an example I can't draw, but functionally he's taking a
MCNF problem with costs and upper bounds, and getting rid of all the upper
bounds.  The process is as follows (for the whole graph):
</p>

<ol class="org-ol">
<li>The whole network becomes a bipartite graph.  Each original node is on the
left, and on the right there is a node for each arc in the original
network.</li>
<li>For each arc on the original, you connect both nodes on the left to the
corresponding node on the right.</li>
<li>You set the supply on the right side node to be the opposite of the upper
bound.</li>
<li>The "supplying" node's arc has the same cost.</li>
<li>The "receiving" node's arc gets a cost of 0.</li>
<li>The receiving node has its supply increased by the upper bound.</li>
</ol>
</div>
</div>

<div id="outline-container-orgheadline77" class="outline-4">
<h4 id="orgheadline77"><span class="section-number-4">22.1.3</span> Node splitting</h4>
<div class="outline-text-4" id="text-22-1-3">
<p>
Sometimes you try to model a LP as an MCNF?  So to make the modeling better,
you may want to bound the amount flowing through a node (not just the net
"supply/demand").
</p>

<ol class="org-ol">
<li>Split the node into two: an input node containing all the edges going in,
and an output node containing all the outgoing edges.</li>
<li>Connect the nodes from the input to the output, set the cost to 0, and
upper bound it by the amount of flow you'd like to allow through the
terminal.</li>
<li>You set the input node's demand to 0, and the output node's demand to the
original.</li>
<li>Congrats, you've split the node!</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-orgheadline79" class="outline-3">
<h3 id="orgheadline79"><span class="section-number-3">22.2</span> Residual Network</h3>
<div class="outline-text-3" id="text-22-2">
<p>
If you have a feasible flow going through an instance of an MCNF problem, you
can create a new problem by changing the demands at each node to be what you
currently have satisfied, and creating arcs to either send back what you've
already got flowing, or to go up to the maximum flow between nodes.
</p>

<p>
This new network is called a "residual network" since it represents the
residual actions you can take to change the flow.  It's sort of a pivot in
the LP or maybe the dual.  It's an action frequently taken by algorithms.
</p>
</div>
</div>

<div id="outline-container-orgheadline80" class="outline-3">
<h3 id="orgheadline80"><span class="section-number-3">22.3</span> MCNF as LP</h3>
<div class="outline-text-3" id="text-22-3">
<p>
The MCNF is formulated as:
</p>

<ul class="org-ul">
<li>min c<sup>T</sup> x, s.t.</li>
<li>Nx = b</li>
<li>x &ge; 0</li>
</ul>

<p>
Where x is a vector of arcs, c<sup>T</sup> is a vector of costs for flows on each arc.
N is the node arc incidence matrix.  Its rows are the nodes, and its columns
are the arcs.  For arc (i,j), there is a +1 in row i, and a -1 in row j,
assuming the flow is from i to j.
</p>

<p>
We will call \(A\) the maximal subset of rows of \(N\) that are linearly
independent.  It has full row rank.  It has at least two non-zero elements,
which are &plusmn; 1, in each column.  Then, we have the problem:
</p>

<ul class="org-ul">
<li>min c<sup>T</sup> x, s.t.</li>
<li>Ax = b</li>
<li>x &ge; 0</li>
</ul>

<p>
If \(A\) is unimodular, then &forall; integer b, the optimal solution is an integer.
Which means that the solution to the ILP version of the \(A\) LP would be the
same as the solution to the LP, and integer.  I guess that would be pretty
cool.
</p>

<p>
<b>Definition:</b> A is totally unimodular iff &forall; square submatrices C of A, det C
&isin; {0, &plusmn; 1}.
</p>

<p>
<b>Claim:</b> Totally Unimodular &rarr; Unimodular.  (recall Unimodular is: &forall; bases B
of A, det B &isin; {&plusmn; 1}.)
<b>Proof:</b> Well, B is a non-singular square submatrix, so its determinant must
be &plusmn; 1.
</p>

<p>
So, we must prove that \(A\) is totally unimodular!
</p>
<ul class="org-ul">
<li>&forall; C square submatrices k &times; k.  We do induction on k.</li>
<li>Base case: k=1.  Each submatrix is 1 &times; 1, and either contains -1, 0, or 1,
so the determinant is either -1, 0, or 1.</li>
<li><p>
Induction:
</p>
<ul class="org-ul">
<li>Case 1: C has a column containing all 0's &rarr; det C = 0.</li>
<li>Case 2: in every column, there is a +1 and a -1.  If you sum up every
row, you get 0, so the matrix is singular, and det C = 0.</li>
<li>Case 3: Anything else.  Pick a column such that you have just a +1 or a
-1.  This is simply that entry (&plusmn; 1), times the determinant of the
submatrix that excludes that row and column.  By the inductive
hypothesis, the determinant of this submatrix is &isin; {0, &plusmn; 1}, so this
means that det C is &isin; {0, &plusmn; 1} as well!</li>
</ul>
<p>
MCNF is unimodular!
</p></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline82" class="outline-2">
<h2 id="orgheadline82"><span class="section-number-2">23</span> 2015-09-21 Monday</h2>
<div class="outline-text-2" id="text-23">
<p>
Same content as 2015-09-23.
</p>
</div>
</div>
<div id="outline-container-orgheadline85" class="outline-2">
<h2 id="orgheadline85"><span class="section-number-2">24</span> 2015-09-18 Friday</h2>
<div class="outline-text-2" id="text-24">
</div><div id="outline-container-orgheadline83" class="outline-3">
<h3 id="orgheadline83"><span class="section-number-3">24.1</span> Integrality</h3>
<div class="outline-text-3" id="text-24-1">
<p>
When you have a ILP, it's normally NP-hard.  However, if the optimal solution
to the LP is integral, then you have the solution in polynomial time.  Hooray
for you.  How can you check to find out if the optimal solution is integral?
Apparently, using a concept called <b>unimodularity</b>.
</p>

<p>
For this part of the lecture, \(A\) is a \(p \times q\) matrix, with integer values,
and \(rank(A)=p\).
</p>

<p>
<b>Definition:</b> A is unimodular iff \(\forall B\) basis, \(\det B = \pm 1\).
</p>

<p>
<b>Theorem:</b> \(A\) as above.  Equivalent:
</p>
<ul class="org-ul">
<li>(a) \(A\) is unimodular</li>
<li>(b) &forall; basic feasible solution, \(Ax=b\), \(x\ge 0\) s an integer. (b integer)</li>
<li>(c) \(\forall B\) basis, \(B^{-1}\) integer.</li>
</ul>

<p>
This theorem is cool because it will apply for any objective function, and
any integer \(\vec{b}\).  However, unimodularity is more strict than purely
figuring out whether a given problem has an integer solution that is optimal.
</p>

<p>
<b>Proof:</b> (a) &rarr; (b)
</p>
<ul class="org-ul">
<li>A basic feasible solution is \(x=(x_B, x_L)\) s.t. \(Bx_B = b\) and \(x_L = 0\).</li>
<li>For all basic feasible solutions, we have \(x_L = 0\), which is integer.  What
about \(x_B\)?  Let's figure out the ith component of \(x_B\).</li>
<li>Well, it turns out that by <b>Cramer's Rule</b>, \(x_i = \frac{\det B_i}{\det B}\).</li>
<li>We know that \(\det B_i =\) an integer (how?).</li>
<li>Since \(\det B = \pm 1\), we know that \(x_i\) must be an integer.  Yay (I guess).</li>
</ul>
<p>
<b>Proof:</b> (b) &rarr; (c)
</p>
<ul class="org-ul">
<li>basis &rarr; \(B\).</li>
<li>D<sub>j</sub>: jth column of \(B^{-1}\)</li>
<li>\(D_j = B^{-1} e_J\), where \(e_j\) is a vector of zeros except for index \(j\), which
is 1.</li>
<li>\((d_{ij}) = B^{-1}\)</li>
<li>\(a_i = \left\{ \begin{array}{ll} [-d_{ij}] & \text{ if } d_{ij} < 0 \\ 0 &
     \text{ if } d_{ij} \ge 0 \end{array} \right.\) (where [] is ceiling function).</li>
<li>This gives us a vector \(\vec{a}\).</li>
<li>\(D_j + a \ge 0\)</li>
<li>\(Bx = e_j + Ba\)</li>
<li>\(x = D_j + a\) is a solution</li>
<li>\(B(B^{-1} e_j + a) = e_j + Ba\)</li>
<li>The right hand side is an integer, and \(B^{-1} e_j + a\) is apparently also an
integer.</li>
<li>And I guess this proves it.  I'm totally lost here.</li>
</ul>
<p>
<b>Proof:</b> (c) &rarr; (a)
</p>
<ul class="org-ul">
<li>\(\det B \det B^{-1} = 1\)</li>
<li>This is because \(\det A \det B = \det AB\), and \(\det I = 1\).</li>
<li>So, \(\det B = \frac{1}{\det B^{-1}}\).  We know that \(B\) is integer (since
\(A\) is as above, integer).</li>
<li>We also know by our assumption (c) that \(B^{-1}\) is integer.  This means both
determinants are integers, and the only values for \(\det B^{-1}\) that make
this possible are -1 and 1.</li>
<li>So, \(A\) must be unimodular.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline84" class="outline-3">
<h3 id="orgheadline84"><span class="section-number-3">24.2</span> Minimum Cost Network Flow</h3>
<div class="outline-text-3" id="text-24-2">
<p>
Woo?
</p>

<p>
In minimum cost network flow, I have a graph \(G=(V,E)\) which is directed.  We
now call vertices nodes, and edges arcs.  You have a source node and a sink
node, with a certain amount of flow that needs to go through the network
(e.g., 3 units must go out of the source, and into the sink).
</p>

<p>
Each arc in the network from node \(i\) and \(j\) has a cost associated with it
\(c_{ij}\).  It also has an \(\ell_{ij}\) that is a lower bound of flow, and an \(u_{ij}\),
which is an upper bound of what can flow through the arc.  The cost of a flow
is the amount of flow through an arc times the cost of the arc.  All flows
are nonnegative.
</p>

<p>
The goal is to put the required amount of flow through the network, while
minimizing the cost.  The total cost is the sum for each arc of the amount of
flow times the \(c_{ij}\) for that arc.
</p>

<p>
The three next steps for this problem:
</p>
<ol class="org-ol">
<li>Make in into a linear program.</li>
<li>Find out whether it is unimodular.</li>
<li>Figure out its dual.</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-orgheadline91" class="outline-2">
<h2 id="orgheadline91"><span class="section-number-2">25</span> 2015-09-16 Wednesday</h2>
<div class="outline-text-2" id="text-25">
<p>
Intro: we're getting to the core of the class.  :D
</p>
</div>

<div id="outline-container-orgheadline86" class="outline-3">
<h3 id="orgheadline86"><span class="section-number-3">25.1</span> Problem Setup</h3>
<div class="outline-text-3" id="text-25-1">
<p>
Problem setup:
</p>

<ul class="org-ul">
<li>min c<sup>T</sup> x, s.t.</li>
<li>Ax &ge; b</li>
<li>x &ge; 0</li>
</ul>

<p>
Dual:
</p>

<ul class="org-ul">
<li>min b<sup>T</sup> &pi;, s.t.</li>
<li>A<sup>T</sup> &pi; &le; c</li>
<li>&pi; &ge; 0</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline87" class="outline-3">
<h3 id="orgheadline87"><span class="section-number-3">25.2</span> Complementary Slackness Proof</h3>
<div class="outline-text-3" id="text-25-2">
<p>
<b>Definition:</b> x, &pi; feasible are said to satisfy complementary slackness iff:
</p>

<ul class="org-ul">
<li>\(\pi_i \left( \sum_{i=1}^q a_{ij}x_{j} - b_i \right) = 0\), i=1,2,&#x2026;,p, (1)</li>
<li>\(x_j \left( \sum_{i=1}^p a_{ij}x_i - c_j \right) = 0\), j=1,2,&#x2026;,q, (2)</li>
</ul>

<p>
<b>Theorem:</b> x, &pi; feasible satisfy c.s. iff x, &pi; are optimal.
</p>

<p>
<b>Proof:</b> x, &pi; feasible &rarr; weak duality.
</p>
<ul class="org-ul">
<li>b<sup>T</sup> &pi; &le; &pi;<sup>T</sup> A x &le; c<sup>T</sup> x</li>
<li>First, prove cs &rarr; optimal
<ul class="org-ul">
<li>Assume x, &pi; satisfy cs.</li>
<li>Sum up equation (1):</li>
<li>You actually get \(\pi^T A x - \pi^T b = 0\), or \(\pi^T A x = \pi^T b\)</li>
<li>Sum up equation (2):</li>
<li>You similarly get \(\pi^T A x = c^T x\).</li>
<li>So, you have \(\pi^T b = c^T x\).</li>
<li>This means that x and &pi; are optimal.</li>
</ul></li>
<li>Next, the other way around.
<ul class="org-ul">
<li>It's basically the same proof in reverse.</li>
<li>\(c^T x = b^T \pi\) (by strong duality)</li>
<li>Due to the weak duality inequalities, we know \(c^T x = \pi^T A x = b^T \pi\).</li>
<li>Then we can take the left and right side of the above, and take them make
to summations:</li>
<li>\(\sum_{i=1}^p \pi_i \left(\sum_{j=1}^q a_{ij}x_j - b_i \right) = 0\)</li>
<li>(and similarly for the left side)</li>
<li>Since &pi;<sub>i</sub> &ge; 0 and the inner summation also &ge; 0, we know that each term
must be equal to 0.</li>
<li>So, this proves (1), and WLOG the other half of the equation proves (2).</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline88" class="outline-3">
<h3 id="orgheadline88"><span class="section-number-3">25.3</span> Lagrangian Relaxation</h3>
<div class="outline-text-3" id="text-25-3">
<p>
z<sup>*</sup> = min c<sup>T</sup> x, s.t. constraints
</p>

<ul class="org-ul">
<li>L(&pi;) = min c<sup>T</sup> x + &pi;<sup>T</sup> (b - Ax), s.t.
<ul class="org-ul">
<li>x &ge; 0</li>
</ul></li>
</ul>

<p>
Assume &pi; &ge; 0.  z* &ge; min c<sup>T</sup> x + &pi;<sup>T</sup> (b - Ax), s.t. Ax&ge; b, x&ge; 0.  This makes
sense because the feasible region is the same, the c<sup>T</sup> x part is the same, and
&pi;<sup>T</sup> (b - Ax) will be &le; 0.  We can then further expand this to say that the
right side is &ge; L(&pi;), since L(&pi;) expands the feasible region, meaning that
the optimum value is &le; the more constrained one.
</p>
</div>
</div>

<div id="outline-container-orgheadline89" class="outline-3">
<h3 id="orgheadline89"><span class="section-number-3">25.4</span> Easily Finding the Dual</h3>
<div class="outline-text-3" id="text-25-4">
<p>
We want to find the dual of every linear program, not just the form with
minimization, Ax&ge;b. and x&ge;0.  We could switch the problem into this form.
Let's call that plan B.  Let's do this instead:
</p>

<p>
min c<sup>T</sup> x s.t.
</p>
<ul class="org-ul">
<li>a<sub>i</sub><sup>T</sup> x = b<sub>i</sub>, (i&isin;M)</li>
<li>a<sub>i</sub><sup>T</sup> x &ge; b<sub>i</sub>, (i&not;&isin;M)</li>
<li>x<sub>j</sub> &ge; 0, (j&isin;N)</li>
<li>x<sub>j</sub> unconstrained, (j&not;&isin;N)</li>
</ul>

<p>
Dual:
</p>

<p>
max b<sup>T</sup> &pi;, s.t.
</p>
<ul class="org-ul">
<li>&pi;<sub>i</sub> unconstrained</li>
<li>&pi;<sub>i</sub> &ge; 0</li>
<li>A<sub>j</sub><sup>T</sup> &pi; &le; c<sub>j</sub></li>
<li>A<sub>j</sub><sup>T</sup> &pi; = c<sub>j</sub></li>
</ul>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Primal</td>
<td class="org-left">Dual</td>
</tr>

<tr>
<td class="org-left">min</td>
<td class="org-left">max</td>
</tr>

<tr>
<td class="org-left">c<sup>T</sup> x</td>
<td class="org-left">b<sup>T</sup> &pi;</td>
</tr>

<tr>
<td class="org-left">a<sub>i</sub><sup>T</sup> x = b<sub>i</sub></td>
<td class="org-left">&pi;<sub>i</sub> unconstrained</td>
</tr>

<tr>
<td class="org-left">a<sub>i</sub><sup>T</sup> x &ge; b<sub>i</sub></td>
<td class="org-left">&pi;<sub>i</sub> &ge; 0</td>
</tr>

<tr>
<td class="org-left">x<sub>j</sub> &ge; 0</td>
<td class="org-left">a<sub>j</sub><sup>T</sup> &pi; &le; c<sub>j</sub></td>
</tr>

<tr>
<td class="org-left">x<sub>J</sub> unconstrained</td>
<td class="org-left">A<sub>j</sub><sup>T</sup> &pi; = c<sub>j</sub></td>
</tr>
</tbody>
</table>


<p>
EG: min x<sub>1</sub> + x<sub>2</sub>, s.t.
</p>
<ul class="org-ul">
<li>x<sub>1</sub> - 2x<sub>2</sub> = 3</li>
<li>x<sub>1</sub>, x<sub>2</sub>, &ge; 0</li>
</ul>

<p>
Originally, we would have transformed it into this problem: min x<sub>1</sub> + x<sub>2</sub>, s.t.
</p>
<ul class="org-ul">
<li>x<sub>1</sub> - 2x<sub>2</sub> &ge; 3</li>
<li>-x<sub>1</sub> + 2x<sub>2</sub> &ge; -3</li>
<li>x<sub>1</sub>, x<sub>2</sub> &ge; 0</li>
</ul>

<p>
Then, we get the dual from the constraints: max 3&pi;<sub>1</sub> - 3&pi;<sub>2</sub>, s.t.
</p>
<ul class="org-ul">
<li>&pi;<sub>1</sub> - &pi;<sub>2</sub> &le; 1</li>
<li>-2&pi;<sub>1</sub> + 2&pi;<sub>2</sub> &le; 1</li>
<li>&pi;<sub>1</sub>, &pi;<sub>2</sub> &ge; 0</li>
</ul>

<p>
Finally, simplify to max 3y, s.t.
</p>
<ul class="org-ul">
<li>y &le; 1</li>
<li>-2y &le; 1</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline90" class="outline-3">
<h3 id="orgheadline90"><span class="section-number-3">25.5</span> More Examples</h3>
<div class="outline-text-3" id="text-25-5">
<p>
min 2x<sub>1</sub> + x<sub>2</sub>, s.t.
</p>
<ul class="org-ul">
<li>x<sub>1</sub> + 3x<sub>2</sub> &ge; 4   (&pi;<sub>1</sub>)</li>
<li>-x<sub>1</sub> + x<sub>2</sub> = 7   (&pi;)</li>
</ul>

<p>
Dual: max 4&pi;<sub>1</sub> + 7&pi;<sub>2</sub>, s.t.
</p>
<ul class="org-ul">
<li>&pi;<sub>1</sub> - &pi;<sub>2</sub> &le; 2 (x<sub>1</sub>)</li>
<li>3&pi;<sub>1</sub> + &pi;<sub>2</sub> = 1 (x<sub>2</sub>)</li>
<li>&pi;<sub>1</sub> &ge; 0</li>
</ul>

<p>
min x<sub>1</sub> + 2x<sub>2</sub> - 3x<sub>3</sub>, s.t.
</p>
<ul class="org-ul">
<li>x<sub>1</sub> +  + x<sub>3</sub> = 4 (&pi;<sub>1</sub>)</li>
<li>2x<sub>1</sub> - x<sub>2</sub> + 2x<sub>3</sub> &le; 5
<ul class="org-ul">
<li>-2x<sub>1</sub> + x<sub>2</sub> - 2x<sub>3</sub> &ge; -5 (&pi;<sub>2</sub>)</li>
</ul></li>
<li>3x<sub>1</sub> - 2x<sub>2</sub> + 3x<sub>3</sub> &ge; 7, (&pi;<sub>3</sub>)</li>
<li>x<sub>1</sub>, x<sub>3</sub> &ge; 0</li>
</ul>

<p>
&pi;<sub>1</sub> is unconstrained, due to the equality.  &pi;<sub>2</sub> and &pi;<sub>3</sub> are &ge; 0, due to the
inequality.  The dual: max 4&pi;<sub>1</sub> - 5&pi;<sub>2</sub> + 7&pi;<sub>3</sub>, s.t.
</p>
<ul class="org-ul">
<li>&pi;<sub>1</sub> - 2&pi;<sub>2</sub> + 3&pi;<sub>3</sub> &le; 1 (x<sub>1</sub>)</li>
<li>&pi;<sub>2</sub> - 2&pi;<sub>3</sub> = 2 (x<sub>2</sub>)</li>
<li>&pi;<sub>1</sub> - 2&pi;<sub>2</sub> + 3&pi;3 &le; -3 (x<sub>3</sub>)</li>
<li>&pi;<sub>2</sub>, &pi;<sub>3</sub> &ge; 0</li>
</ul>

<p>
We'll do one with the knapsack problem, where we don't have a definite number
of constraints/variables.
</p>

<p>
max &sum;<sub>j=1</sub><sup>q</sup> c<sub>j</sub> x<sub>J</sub>, s.t.
</p>
<ul class="org-ul">
<li>&sum;<sub>j=1</sub><sup>q</sup> w<sub>j</sub> x<sub>j</sub> &le; W</li>
<li>x<sub>j</sub> &le; 1, j=1&#x2026;q</li>
<li>x<sub>j</sub> &ge; 0, j=1&#x2026;q</li>
</ul>

<p>
For the dual, we'll take the one constraint, call it &alpha;, and the rest and call
the others &pi;<sub>i</sub>
</p>

<ul class="org-ul">
<li>min W &alpha;, s.t.</li>
<li>w<sub>j</sub> &alpha; + &pi;<sub>i</sub> &ge; c<sub>j</sub></li>
<li>&alpha;, &pi;<sub>i</sub> &ge; 0</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline95" class="outline-2">
<h2 id="orgheadline95"><span class="section-number-2">26</span> 2015-09-14 Monday</h2>
<div class="outline-text-2" id="text-26">
</div><div id="outline-container-orgheadline92" class="outline-3">
<h3 id="orgheadline92"><span class="section-number-3">26.1</span> Homework Stuff</h3>
<div class="outline-text-3" id="text-26-1">
<p>
Problem 3 had no solution.
</p>

<p>
In problem 7, you could find many examples of optimal solutions that are
actually convex combinations of two optimal basic solutions, which are not
themselves basic solutions.  This is not what the question asked for.  It
seems like the basic feasible solutions are always integer for this problem.
</p>
</div>
</div>

<div id="outline-container-orgheadline93" class="outline-3">
<h3 id="orgheadline93"><span class="section-number-3">26.2</span> Duality</h3>
<div class="outline-text-3" id="text-26-2">
<p>
Strong Duality Theorem: I have a program of the form:
</p>

<ul class="org-ul">
<li>min c<sup>T</sup> x, st</li>
<li>Ax &ge; b</li>
<li>x &ge; 0</li>
</ul>

<p>
We also have its dual:
</p>

<ul class="org-ul">
<li>max b<sup>T</sup> &pi;, s.t.</li>
<li>A<sup>T</sup> &pi; &le; c</li>
<li>&pi; &ge; 0</li>
</ul>

<p>
The differences between these are:
</p>
<ul class="org-ul">
<li>min/max</li>
<li>A becomes A<sup>T</sup></li>
<li>c and b are swapped</li>
<li>x becomes &pi;</li>
</ul>

<p>
We have weak duality, that b<sup>T</sup> &pi; &le; &pi;<sup>T</sup> A x &le; c<sup>T</sup> x.
</p>

<p>
<b>Strong Duality Theorem:</b> Suppose that the primal (or dual) has a finite,
optimal solution.  Then, so does the dual (primal), and they have the same
optimal objective value.
</p>

<p>
<b>Proof:</b> WLOG, assume that the primal has a finite, optimal solution x<sup>*</sup> (the
primal and dual can be swapped and the proof is the same).  Also assume WLOG
x<sup>*</sup> is a BFS.  First, we'll take the primal and put it into standard form:
</p>

<ul class="org-ul">
<li>min c<sup>T</sup> x, s.t.</li>
<li>Ax - Is = b</li>
<li>x, s &ge; 0</li>
</ul>

<p>
Let \(\tilde{x}=(x,s)\), \(\tilde{c}=(c,0)\), and \(\tilde{A}=(A,-I)\).  Then we can put this in
even nicer looking standard form:
</p>

<ul class="org-ul">
<li>\(\min \tilde{c}^T \tilde{x}\), s.t.</li>
<li>\(\tilde{A} \tilde{x} = b\)</li>
<li>\(\tilde{x} \geq 0\)</li>
</ul>

<p>
When we write this in "canonical form" (I seriously need to study this part
of the stuff):
</p>

<ul class="org-ul">
<li>\(\min (c_L^{\tilde{\pi}})^T \tilde{x}_L + \tilde{\pi}^T b\), s.t.</li>
<li>\(\tilde{x}_B + \tilde{A}\tilde{x}_L = \tilde{b}\)</li>
<li>\(\tilde{x}_B, \tilde{x}_L \geq 0\)</li>
</ul>

<p>
We'll call the objective function z, so we're minimizing \(z(\tilde{x})\).
\(z(x^*)=\tilde{\pi}^T b\).  The next thing is to look at the reduced costs.  First,
we know that \(c^{\tilde{\pi}} \geq 0\), because x<sup>*</sup> is optimal.  Next, we know
that \(c^{\tilde{\pi}} = \tilde{c} - \tilde{A}^T \tilde{\pi} =\):
</p>

\begin{align*}
c^{\tilde{\pi}} &= \tilde{c} - \tilde{A}^T \tilde{\pi} \\
   &= \begin{bmatrix} c \\ 0 \end{bmatrix} - \begin{bmatrix} A^T \\ -I \end{bmatrix} \\
   &= \begin{bmatrix} c - A^T \tilde{\pi} \\ \tilde{\pi} \end{bmatrix}
\end{align*}

<p>
This gives us that \(c \geq A^T \tilde{\pi}\), and \(\tilde{\pi} \geq 0\).  This
tells us that the \(\tilde{\pi}\) is feasible in the dual.  And then, we know that
the objective value of \(\tilde{\pi}\) in the dual is \(b^T \tilde{\pi}\), which is
z(x<sup>*</sup>).  We know by the weak duality theorem that no &pi; can have an objective
value greater than this, so it is an optimal solution for the dual!
</p>
</div>
</div>

<div id="outline-container-orgheadline94" class="outline-3">
<h3 id="orgheadline94"><span class="section-number-3">26.3</span> Complementary Slackness</h3>
<div class="outline-text-3" id="text-26-3">
<p>
Let x, &pi; be feasible solutions.  x, &pi; satisfy complementary slackness (p+q
equalities).
</p>

\begin{equation}
  \pi_i \left(\sum_{j=1}^q a_{ij} x_{j} - b_i \right) = 0
\end{equation}
<p>
for i=1, 2, &hellip;, p
</p>

\begin{equation}
  x_j \left(\sum_{i=1}^p a_{ij} \pi_i - c_j \right) = 0
\end{equation}
<p>
for j=1, 2, &hellip;, q
</p>

<p>
Essentially, each of these are the slack variables.  So, if you look at the
slackness in a constraint in one problem, and multiply it by the
corresponding variable in its dual, you'll find that quantity is zero.
</p>

<p>
If the slackness \(s_i > 0\), then \(\pi_i = 0\).  You can look at the \(\pi_i\) as a
"price" for how much you'd be willing to "get rid" of the constraint.  If
your constraint is not even constraining you, you wouldn't care to get rid of
it, and \(\pi_i\) is 0.  On the flip side, if your slackness is 0, the \(\pi_i\) will
tell you <i>kinda</i> how much you'd like to get rid of your constraint.
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline97" class="outline-2">
<h2 id="orgheadline97"><span class="section-number-2">27</span> 2015-09-11 Friday</h2>
<div class="outline-text-2" id="text-27">
<p>
LP <b>canonical</b> form.
</p>

\begin{align*}
  \min (c_L^{\pi})^T x_L + \pi^T b &\\
  \text{s.t. } x_B + \bar{A} x_L &= \bar{b} \\
  x_B, x_L &\geq 0 \\
  \text{where } \pi^T B &= c_B^T \\
  c^pi &= c - A^T \pi \\
\end{align*}

<p>
EG:
</p>

\begin{align*}
  \min x_3 + x_4 + 7 &\\
  \text{s.t. } x_1 + 2x_3 + 3x_4 &= 1 \\
  x_2 + x_3 + 7x_4 &= 2 \\
  x_1, x_2, x_3, x_4 &\geq 0 \\
\end{align*}

<ul class="org-ul">
<li>The basic variables are x<sub>1</sub> and x<sub>2</sub>.  You can come up with a BFS by setting x<sub>3</sub>
    and x<sub>4</sub> equal to 0, and reading off the values for the basic variables.</li>
</ul>

<p>
<b>Thm:</b> BFS \(\bar{x}\) is optimal iff c^&pi; &ge; 0.
</p>
<ul class="org-ul">
<li><b>Proof:</b> &larr; last time</li>
<li><p>
<b>Proof:</b> &rarr; (only the non-degenerate case)
</p>

<p>
&exist; s . c<sub>s</sub><sup>T</sup> &lt; 0 &rarr; \(\bar{x}\) is not optimal.
</p>

<p>
Look at the $s$th column of \(\bar{A}\), or \(\bar{A}_s\).
</p>

<p>
The ith constraint is \(x_1 + \bar{a}_{is} x_s + \mathcal{L} \text{ terms } =
    \bar{b}_i\).
</p>

<p>
If \(\bar{A}_{is} \leq 0 \: \forall i\) then x<sub>s</sub> can be increased arbitrarily to.
</p>

<p>
Assume &exist; i s.t. $\bar{a}<sub>is</sub> &gt; 0
</p>

\begin{equation}
  \theta = \min_{i: \bar{a}_{is} > 0} \frac{\bar{b}_i}{\bar{a}_{is}}
\end{equation}

<p>
Since \(\bar{b}_i \ge 0\) and \(\bar{a}_{is} > 0\), we can claim \(\theta \ge 0\).
However, we'll be doing the non-degenerate case, and assuming \(\theta > 0\).
</p>

<p>
More proof stuff that I really need to read about.
</p></li>
</ul>
</div>

<div id="outline-container-orgheadline96" class="outline-3">
<h3 id="orgheadline96"><span class="section-number-3">27.1</span> Duality</h3>
<div class="outline-text-3" id="text-27-1">
<ul class="org-ul">
<li>Primal: min C<sup>T</sup> x s.t. Ax &ge; b, x &ge; 0.</li>
<li>Dual: max b<sup>T</sup> &pi; s.t. A<sup>T</sup> &pi; &le; c, &pi; &ge; 0</li>
</ul>

<p>
Claim: dual of dual is primal.
</p>

<p>
Theorem (Weak Duality): &forall; feasible x, &pi;, b<sup>T</sup> &pi; &le; c<sup>t</sup> x.
</p>
</div>
</div>
</div>
<div id="outline-container-orgheadline99" class="outline-2">
<h2 id="orgheadline99"><span class="section-number-2">28</span> 2015-09-09 Wednesday</h2>
<div class="outline-text-2" id="text-28">
</div><div id="outline-container-orgheadline98" class="outline-3">
<h3 id="orgheadline98"><span class="section-number-3">28.1</span> Linear Programming</h3>
<div class="outline-text-3" id="text-28-1">
<p>
min c<sup>T</sup> x, s.t. Ax = b, x &ge; 0
</p>
<ul class="org-ul">
<li>A p &times; q matrix.</li>
<li>Rank(A) = p</li>
<li>A = (A<sub>1</sub>, A<sub>2</sub>, A<sub>3</sub>, &#x2026;, A<sub>q</sub>)</li>
</ul>

<p>
Let \(\bar{x}\) be a feasible solution.  Let \(A(\bar{x}) = \{A_i: \bar{x}_i >
   0\}\).  <b>Thm:</b> \(\bar{x}\) is an extreme point iff \(A(\bar{x})\) is a set of
linearly independent vectors.
</p>

<p>
<b>Def:</b> (B,L) is a basis structure iff:
</p>
<ul class="org-ul">
<li>(B,L) partition of {1, 2, &#x2026;, q}.</li>
<li>{A<sub>I</sub>: i &isin; B} is a basis for R<sup>p</sup></li>
</ul>

<p>
A = (B, L), x = (x<sub>B</sub>, x<sub>L</sub>), c = (c<sub>B</sub>, c<sub>L</sub>)
</p>

<p>
EG: min x<sub>1</sub>+x<sub>2</sub>+x<sub>3</sub>+x<sub>4</sub> s.
</p>
<ul class="org-ul">
<li>x<sub>1</sub> + 2x<sub>2</sub> + 3x<sub>4</sub> = 1</li>
<li>4x<sub>2</sub> + x<sub>3</sub> + 2x<sub>4</sub> = 2</li>
<li>All x &ge; 0</li>
</ul>

<p>
Rename variables x<sub>1</sub> to y<sub>1</sub>, x<sub>2</sub> to y<sub>3</sub>, x<sub>3</sub> to y<sub>2</sub>, x<sub>4</sub> to y<sub>4</sub>:
</p>

<p>
min y<sub>1</sub> + y<sub>2</sub> + y<sub>3</sub> + y<sub>4</sub>, s.t.
</p>
<ul class="org-ul">
<li>y<sub>1</sub> + 2y<sub>3</sub> + 3y<sub>4</sub> = 1</li>
<li>y<sub>2</sub> + 4y<sub>3</sub> + 2y<sub>4</sub> = 2</li>
<li>All y &ge; 0</li>
</ul>

\begin{equation}
A = \begin{bmatrix} 1 & 0 & 2 & 3 \\ 0 & 1 & 4 & 2 \end{bmatrix}
\end{equation}

<p>
The left half of A is B, and the right half is L.
</p>

\begin{equation}
y = \begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \end{bmatrix}
\end{equation}
\begin{equation}
c = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
\end{equation}

<p>
The top halves of these are \(y_B\) and \(c_B\) respectively.
</p>

\begin{align*}
  B x_B + L x_L &= b \\
  x_B + B^{-1} L x_L &= B^{-1} b \\
  x_B + \bar{A} x_L &= \bar{b}
\end{align*}

<p>
Here we're letting \(\bar{A} = B^{-1} L\) and \(\bar{b} = B^{-1} b\).
</p>

<p>
A basic solution is one where \(\bar{x_B} = \bar{b}\), or \(\bar{x_L} = 0\).  A
feasible basic solution is one where \(\bar{x_B} \geq 0\) as well.
</p>

<p>
<b>Def:</b> Simplex multipliers corresponding to \((B,L)\):
</p>

\begin{equation}
  \pi^T = c_B^T B^{-1}
\end{equation}

<p>
Let \(\bar{x} = \begin{bmatrix}\bar{x_B} \\ \bar{x_L} \end{bmatrix}\) be BFS
corresponding to (B, L).  The objective function at \(\bar{x}\) is:
</p>

\begin{align*}
  \begin{bmatrix} c_B^T & c_L^T \end{bmatrix}
  \begin{bmatrix} \bar{x_B} \\ \bar{x_L} \end{bmatrix}
  &= c_B^T \bar{x_B} + c_L^T x_L \\
  &= (\pi^T B) (B^{-1} b) + c_L^T (0) \\
  &= \pi^T b
\end{align*}

<p>
<b>Def:</b> Reduced costs corresponding to (B,L) = A
</p>

\begin{equation}
  c^{\pi}  = c - A^T \pi
\end{equation}

\begin{equation}
  c^{\pi} = \begin{bmatrix} c_B^{\pi} \\ c_L^{\pi} \end{bmatrix}
  = \begin{bmatrix} c_B \\ c_L \end{bmatrix} - \begin{bmatrix} B^T & L^T \end{bmatrix} \pi
  = \begin{bmatrix} c_B - B^T \pi \\ c_L - L^T \pi \end{bmatrix}
  = \begin{bmatrix} 0 \\ c_L - L^T \pi \end{bmatrix}
\end{equation}

<p>
I guess you can also rewrite it to \(c = c^{\pi} + A^T \pi\), but I'm not writing
out the vectors and matrices again.  Now he's doing more stuff with the
objective function.
</p>

\begin{equation}
  c^T x = (c^{\pi}_L)^T x_L + \pi^T b
\end{equation}

<p>
Once we find the basic feasible solution, the \(\pi^T b\) is pretty much fixed,
and so we just need to minimize (C<sub>L</sub><sup>&pi;</sup>)<sup>T</sup> x<sub>L</sub>.  Now, say we look at a non-basic
(i.e. in L, not B) variable x<sub>i</sub>, and look at its reduced costs.
</p>

<ul class="org-ul">
<li>If c<sub>i</sub><sup>&pi;</sup> &ge; 0, we would be happy to set x<sub>i</sub> = 0 (if it's feasible).</li>
<li>If c<sub>i</sub><sup>&pi;</sup> &lt; 0, we would be happy to set x<sub>i</sub> = +&infin; (if it's feasible).</li>
</ul>

<p>
We can see that if &forall; i c<sub>i</sub><sup>&pi;</sup> &ge; 0, then the BFS is optimal.  In fact, it's
also true the other way around.
</p>

<p>
<b>Thm:</b> BFS \(\bar{x}\) is optimal iff &forall; i c<sub>i</sub><sup>&pi;</sup> &ge; 0.
</p>
<ul class="org-ul">
<li>Proof &larr;: (basically what we've been talking about)</li>
<li>Proof &rarr;: is a really difficult, multi-lecture proof.  We'll sketch out the
non-degenerate case only, &theta; &gt; 0.  Next time.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgheadline108" class="outline-2">
<h2 id="orgheadline108"><span class="section-number-2">29</span> 2015-09-02 Wednesday</h2>
<div class="outline-text-2" id="text-29">
</div><div id="outline-container-orgheadline101" class="outline-3">
<h3 id="orgheadline101"><span class="section-number-3">29.1</span> Review of LP</h3>
<div class="outline-text-3" id="text-29-1">
<ul class="org-ul">
<li>min C<sup>T</sup> x, st.</li>
<li>Ax = b</li>
<li>x &ge; 0</li>
</ul>

<p>
Integer LP is same, except require that x is an integer.
</p>
</div>

<div id="outline-container-orgheadline100" class="outline-4">
<h4 id="orgheadline100"><span class="section-number-4">29.1.1</span> Example</h4>
<div class="outline-text-4" id="text-29-1-1">
<p>
Vertex Cover Problem - given a undirected graph G, find a vertex cover of
minimum size.  (vc = a set of vertices that cover every edge).
</p>

<p>
We are going to convert a VC problem into ILP.  The graph we have is (no
diagrams, sorry): V={1, 2, 3, 4}, E={(1,2), (1,3), (1,4), (2,3), (3,4)}
(undirected).
</p>

<p>
Decision variables are x<sub>i</sub>= 1, if i &isin; VC, 0 otherwise.  We minimize the
function x<sub>1</sub> + x<sub>2</sub> + x<sub>3</sub> + x<sub>4</sub>, s.t.:
</p>

<ul class="org-ul">
<li>x<sub>1</sub> + x<sub>2</sub> &ge; 1</li>
<li>x<sub>1</sub> + x<sub>3</sub> &ge; 1</li>
<li>x<sub>1</sub> + x<sub>4</sub> &ge; 1</li>
<li>x<sub>2</sub> + x<sub>3</sub> &ge; 1</li>
<li>x<sub>3</sub> + x<sub>4</sub> &ge; 1</li>
<li>x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, x<sub>4</sub> &isin; {0, 1}</li>
</ul>

<p>
In case you can't tell, there is a constraint for each edge, which basically
says that at least one of the vertices on the edge needs to be 1.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline102" class="outline-3">
<h3 id="orgheadline102"><span class="section-number-3">29.2</span> Reducing Vertex Cover to ILP</h3>
<div class="outline-text-3" id="text-29-2">
<p>
More generally, the vertex cover of G=(V,E) can be transformed to ILP like this:
</p>

<ul class="org-ul">
<li>Min &sum;<sub>i&isin;V</sub> x<sub>i</sub>, s.t.</li>
<li>x<sub>i</sub> + x<sub>j</sub> &ge; 1 &forall; (i,j) &isin; E</li>
<li>x<sub>i</sub> &isin; {0, 1} &forall; i &isin; V</li>
</ul>

<p>
When you remove the integrality constraint from an ILP, you get the <b>linear
relaxation</b> of the problem.  In the case of this problem, we get an
assignment of fractional weights to vertices such that each edge has sum &ge;
1, while minimizing the total vertex weights.  It's an entirely different
problem, and not really something we want.
</p>

<p>
According to the Liberator, the difference between a lot of the problems
dealt with in other fields and in computer science is the addition of these
"integrality constraints," which makes problems much more difficult than
their continuous relatives.
</p>
</div>
</div>

<div id="outline-container-orgheadline103" class="outline-3">
<h3 id="orgheadline103"><span class="section-number-3">29.3</span> "Slicing" Linear Programs</h3>
<div class="outline-text-3" id="text-29-3">
<p>
When you have the constraints Ax = b, you can think of it as a<sub>i</sub><sup>T</sup> x = b<sub>i</sub>,
where a<sub>i</sub><sup>T</sup> is a row vector of A.  This is totally linear algebra, and I'm sure
it'll come in useful later in the course.
</p>
</div>
</div>

<div id="outline-container-orgheadline104" class="outline-3">
<h3 id="orgheadline104"><span class="section-number-3">29.4</span> Semi Definite Programming</h3>
<div class="outline-text-3" id="text-29-4">
<ul class="org-ul">
<li><b>Def:</b> A real matrix A is positive (semi) definite iff &forall; x &ge; 0, x<sup>T</sup> A x &gt; 0
(x<sup>T</sup> A x &ge; 0).</li>

<li><b>Thm:</b> A is positive semidefinite iff all its eigenvalues are &ge; 0.</li>
</ul>

<p>
(note to self - go over linear algebra!)
</p>

<ul class="org-ul">
<li><b>Def:</b> A is symmetric, positive, semidefinite -&gt; A &sccue; 0.</li>

<li><b>Thm:</b> A &sccue; 0 iff &exist; B s.t. A = B<sup>T</sup> B.  Given A, B can be found in polynomial
time.  B is not necessarily square, but of course B<sup>T</sup> B will be.</li>

<li>Given two matrices C, X (n by m), C &sdot; X = &sum;<sub>i=1</sub><sup>n</sup> &sum;<sub>j=1</sub><sup>m</sup> c<sub>ij</sub> x<sub>ij</sub>.</li>
</ul>

<p>
The problem of Semi Definite Programming is:
</p>

<ul class="org-ul">
<li>minimize C &sdot; X, st:</li>
<li>A<sub>i</sub> &sdot; X = b<sub>i</sub></li>
<li>X &sccue; 0</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline105" class="outline-3">
<h3 id="orgheadline105"><span class="section-number-3">29.5</span> LP reduces to SDP</h3>
<div class="outline-text-3" id="text-29-5">
<ul class="org-ul">
<li><p>
<b>Claim:</b> Linear programming is a special case of (i.e. reduces to) Semi
Definite Programming.
</p>

\begin{equation}
X = \begin{bmatrix} x_1 & 0 & 0 \\ 0 & \ddots & 0 \\ 0 & 0 & x_q \end{bmatrix}
\end{equation}

\begin{equation}
C = \begin{bmatrix} c_1 & 0 & 0 \\ 0 & \ddots & 0 \\ 0 & 0 & c_q \end{bmatrix}
\end{equation}

\begin{equation}
A_i = \begin{bmatrix} a_{i1} & 0 & 0 \\ 0 & \ddots & 0 \\ 0 & 0 & a_{iq} \end{bmatrix}
\end{equation}</li>

<li>We wouldn't want to do this in practice, since we have more efficient
algorithms to LP.  But it exists.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline106" class="outline-3">
<h3 id="orgheadline106"><span class="section-number-3">29.6</span> Quadratically Constrained Quadratic Programming (QCQP)</h3>
<div class="outline-text-3" id="text-29-6">
<ul class="org-ul">
<li>min x<sup>T</sup> Q x + q<sup>T</sup> x</li>
<li>s.t. x<sup>T</sup> Q<sub>i</sub> x + q<sub>i</sub><sup>T</sup> x &le; b<sub>i</sub>, i=1,2,..,p</li>
</ul>

<p>
Both the objective function and the constraints may be quadratic.
</p>

<ul class="org-ul">
<li>It seems that you can reduce QCQP also to SDP.</li>
<li>I guess the way to think about it is that in SDP, X = B<sup>T</sup> B, so in the
decision variables you get quadratic terms.  Or something.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline107" class="outline-3">
<h3 id="orgheadline107"><span class="section-number-3">29.7</span> Back to Linear Programming</h3>
<div class="outline-text-3" id="text-29-7">
<p>
Like you could slice LP constraint matrices by rows, you can also do it by
columns.  Split A into columns A<sub>1</sub>, A<sub>2</sub>, &#x2026;, A<sub>q</sub>.  Then, you can break the
constraints into: A<sub>1</sub> x<sub>1</sub> + A<sub>2</sub> x<sub>2</sub> + &#x2026; + A<sub>q</sub> x<sub>q</sub> = b.
</p>

<p>
Back when we were looking at LP the first time, we saw the feasible region as
a polygon (or polyhedron), and the vertices were the extreme points, which
are the candidate solutions.  These extreme points cannot be expressed as
convex combination of other feasible solutions.  Even more exciting, <b>Thm:</b>
All feasible solutions are convex combinations of extreme points.
</p>

<p>
Each constraint point corresponds in some way to the column breakdown shown
above, which allows us to do LP is a Linear Algebra way.
</p>

<p>
<b>Thm:</b> A feasible solution is an extreme point iff:
</p>
<ul class="org-ul">
<li>A<sub>i</sub> corresponding to x<sub>i</sub> &gt; 0 are independent.  That is, given a point x, look
at its coordinates x<sub>i</sub>, find the ones greater than 0, and check if the A<sub>i</sub>
corresponding to them are independent.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgheadline113" class="outline-2">
<h2 id="orgheadline113"><span class="section-number-2">30</span> 2015-08-31 Monday</h2>
<div class="outline-text-2" id="text-30">
</div><div id="outline-container-orgheadline109" class="outline-3">
<h3 id="orgheadline109"><span class="section-number-3">30.1</span> Linear Programming (LP)</h3>
<div class="outline-text-3" id="text-30-1">
<p>
An instance of LP:
</p>

<ul class="org-ul">
<li>min &sum;<sub>j=1</sub><sup>q</sup> c<sub>j</sub> x<sub>j</sub>, subject to:</li>
<li>&sum;<sub>j=1</sub><sup>q</sup> a<sub>ij</sub> x<sub>j</sub> &le; b<sub>i</sub>, for i = 1, 2, &#x2026;, p, and j=1, 2, &#x2026;, q</li>
<li>x<sub>j</sub> &gt; 0</li>
</ul>

<p>
The constraints define X, the feasible region.  You can switch a minimization
problem to a maximization problem by negating the objective function.
Minimization is the "standard form".  You can also define the "slack
variables" in the constraints, which were covered a bit more in the EECS 440
lecture on LP.  EG, diet problem:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-right">i</td>
<td class="org-left">Food</td>
<td class="org-right">Energy</td>
<td class="org-right">Protein</td>
<td class="org-right">Calcium</td>
<td class="org-right">Price</td>
<td class="org-right">Max</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-left">Oatmeal</td>
<td class="org-right">110</td>
<td class="org-right">4</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-left">Chicken</td>
<td class="org-right">205</td>
<td class="org-right">32</td>
<td class="org-right">12</td>
<td class="org-right">24</td>
<td class="org-right">3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-left">Eggs</td>
<td class="org-right">160</td>
<td class="org-right">13</td>
<td class="org-right">54</td>
<td class="org-right">13</td>
<td class="org-right">2</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-left">Milk</td>
<td class="org-right">160</td>
<td class="org-right">8</td>
<td class="org-right">285</td>
<td class="org-right">9</td>
<td class="org-right">8</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-right">5</td>
<td class="org-left">Pie</td>
<td class="org-right">420</td>
<td class="org-right">4</td>
<td class="org-right">22</td>
<td class="org-right">20</td>
<td class="org-right">2</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-right">6</td>
<td class="org-left">Pork w/ beans</td>
<td class="org-right">260</td>
<td class="org-right">14</td>
<td class="org-right">80</td>
<td class="org-right">19</td>
<td class="org-right">2</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-right">&#xa0;</td>
<td class="org-left">GOALS</td>
<td class="org-right">2000</td>
<td class="org-right">55</td>
<td class="org-right">800</td>
<td class="org-right">min</td>
<td class="org-right">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
Decision variable is x<sub>1</sub>, so here is the problem:
</p>

<ul class="org-ul">
<li>minimize, 3x<sub>1</sub> + 24x<sub>2</sub> + 13x<sub>3</sub> + 9x<sub>4</sub> + 20x<sub>5</sub> + 19x<sub>6</sub>, subject to:</li>
<li>110x<sub>1</sub> + 205x<sub>2</sub> + 160x<sub>3</sub> + 160x<sub>4</sub> + 420x<sub>5</sub> + 260x<sub>6</sub> &ge; 2000</li>
<li>4x<sub>1</sub> + 32x<sub>2</sub> + 13x<sub>3</sub> + 8x<sub>4</sub> + 4x<sub>5</sub> + 14x<sub>6</sub> &ge; 55</li>
<li>2x<sub>1</sub> + 12x<sub>2</sub> + 54x<sub>3</sub> + 285x<sub>4</sub> + 22x<sub>5</sub> + 80x<sub>6</sub> &ge; 800</li>
<li>0 &le; x<sub>1</sub> &le; 4</li>
<li>0 &le; x<sub>2</sub> &le; 3</li>
<li>0 &le; x<sub>3</sub> &le; 2</li>
<li>0 &le; x<sub>4</sub> &le; 8</li>
<li>0 &le; x<sub>5</sub> &le; 2</li>
<li>0 &le; x<sub>6</sub> &le; 2</li>
</ul>

<p>
This isn't in standard form due to the greater than or equal to in the top 3
constraints, and the less than or equal to in the variable bounds.  I guess.
</p>

<p>
What to do to get decision variables unrestricted in sign (not in std form):
If you want x to be negative (or just allowed to be negative) replace it with
two variables (say, y and z).  Substitute x with y-z, and add the condition
that y,z &ge; 0.  This allows x (aka y-z) to be positive or negative, but you
could add more conditions on y-z to make it how you'd like.
</p>

<p>
The graphical representation of these problems is pretty simple (when you
have two variables).  The constraints create a nice shaded polygon that
represents your feasible region, and then you pick the vertex that maximizes
the objective function.
</p>

<p>
<b><b>Claim:</b></b> There is always an optimal solution in an extreme point.  That's
worded weird.  I prefer "an optimal solution is always an extreme point."
</p>

<p>
You can represent a LP instance in matrix form like this:
</p>
<ul class="org-ul">
<li>min C<sup>T</sup> x</li>
<li>s.t. Ax=b</li>
<li>x &ge; 0</li>
</ul>

<p>
Where, x = (x<sub>1</sub>, x<sub>2</sub>, &#x2026;, x<sub>q</sub>)<sup>T</sup>, c = (c<sub>1</sub>, c<sub>2</sub>, &#x2026;, c<sub>q</sub>)<sup>T</sup>, A=(a<sub>11</sub>, a<sub>12</sub>, &#x2026;, a<sub>1q</sub>;
&#x2026;; a<sub>p1</sub>, a<sub>p2</sub>, &#x2026;, a<sub>pq</sub>), b=(b<sub>1</sub>, b<sub>2</sub>, &#x2026;, b<sub>p</sub>)<sup>T</sup>.
</p>
</div>
</div>

<div id="outline-container-orgheadline110" class="outline-3">
<h3 id="orgheadline110"><span class="section-number-3">30.2</span> Integer Linear Programming</h3>
<div class="outline-text-3" id="text-30-2">
<p>
Same as ^, except that the x's must be integers.  Since this is a more
restricted problem, the solutions are no better than the LP solutions.
</p>
</div>
</div>

<div id="outline-container-orgheadline111" class="outline-3">
<h3 id="orgheadline111"><span class="section-number-3">30.3</span> Mixed Integer Linear Programming</h3>
<div class="outline-text-3" id="text-30-3">
<p>
MILP.  Really?
</p>

<p>
&gt; Matrix I'd Like to Program - Andrew Mason
</p>

<p>
Only some of the decision variables need to be integral, others can be
continuous.
</p>
</div>
</div>

<div id="outline-container-orgheadline112" class="outline-3">
<h3 id="orgheadline112"><span class="section-number-3">30.4</span> Next Time, on Advanced Algorithms:</h3>
<div class="outline-text-3" id="text-30-4">
<p>
Vertex cover, formulated as ILP.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline120" class="outline-2">
<h2 id="orgheadline120"><span class="section-number-2">31</span> 2015-08-28 Friday</h2>
<div class="outline-text-2" id="text-31">
</div><div id="outline-container-orgheadline114" class="outline-3">
<h3 id="orgheadline114"><span class="section-number-3">31.1</span> Last Time:</h3>
<div class="outline-text-3" id="text-31-1">
<p>
Approximation algorithms have approximation ratio:
</p>

<p>
apx ratio = \(max_{I\in{}\mathscr{I}} {\frac{c(I)}{c^*(I)}}\)
</p>

<p>
A c-approximation algorithm has cost &le; c &times; optimal cost on all instances I of
the problem \(\mathscr{I}\).  One example is the vertex cover problem.  We
covered a 2-approximation algorithm (called <code>VCapx</code>) that operates by
repeatedly choosing an edge, adding its endpoints to the VC, and removing all
incident edges from the graph.
</p>

<p>
We left off saying that today we would cover the proof that it is a 2-apx
algorithm.
</p>
</div>
</div>

<div id="outline-container-orgheadline115" class="outline-3">
<h3 id="orgheadline115"><span class="section-number-3">31.2</span> Proof</h3>
<div class="outline-text-3" id="text-31-2">
<p>
<b>Theorem</b> <code>VCapx</code> is a 2-approximation algorithm.
</p>

<p>
<b>Proof</b> Every edge is covered by <code>VCapx</code> at termination.  For every one of
these edges, the algorithm adds at most two vertices to \(V'\).  The optimal
solution contains at least one of these two.  <code>VCapx</code> never considers the
same vertex twice (since it deletes incident edges).  So, this is a 2
approximation algorithm.
</p>

<p>
Here's the actual text of his proof:
</p>

<ul class="org-ul">
<li>Every edge is covered by <code>VCapx</code> at terminates.</li>
<li>&forall; edge chosen by <code>VCapx</code>
<ul class="org-ul">
<li><code>VCapx</code> adds 2 vertices to \(V'\)</li>
<li>Opt contains at least one of the two vertices</li>
</ul></li>
<li><code>VCapx</code> never considers same vertex twice. (by deleting incident edges)
<ul class="org-ul">
<li>&rarr; edges are disjoint, &rarr; \(V'\) can be partitioned by edges added by <code>VCapx</code></li>
</ul></li>
<li>&rarr; 2-apx algorithm</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline118" class="outline-3">
<h3 id="orgheadline118"><span class="section-number-3">31.3</span> Reduction</h3>
<div class="outline-text-3" id="text-31-3">
<p>
The pipeline of reduction:
</p>

<p>
(X, c) &isin; <b>I</b> &rarr; (X', c') &isin; <b>I' &rarr;</b> x'<sup>*</sup> &rarr; x<sup>*</sup>
</p>

<p>
If the time to translate (X,c) to (X', c') is T<sub>1</sub>, and the time to translate
x'<sup>*</sup> to x<sup>*</sup> is T<sub>2</sub>, then problem <b>I</b> reduces to *I^'* in time T<sub>1</sub> + T<sub>2</sub>.
</p>

<p>
EG: Any maximization problem reduces to a minimization problem in O(1) time.
</p>
</div>

<div id="outline-container-orgheadline116" class="outline-4">
<h4 id="orgheadline116"><span class="section-number-4">31.3.1</span> Optimal Message Passing</h4>
<div class="outline-text-4" id="text-31-3-1">
<p>
Given a graph G=(V,E) with probability p<sub>e</sub> (0 &lt; p<sub>e</sub> &lt; 1) associated to each e
&isin; E.  Find a spanning tree of G that minimizes the probability of failure.
(The probabilities are of failure, and independent).
</p>

<p>
So, the probability of survival for the whole tree is &Pi;<sub>e&isin; T</sub> (1-p<sub>e</sub>).
</p>

<p>
We can reduce the OMP to Minimum Spanning Tree problem in linear time.  We
define the weight of an edge to be w<sub>e</sub> = -log (1-p<sub>e</sub>).  The cost of an MST
is c(T) = &sum;<sub>e&isin; T</sub> w<sub>e</sub> = &sum;<sub>e&isin; T</sub> log 1/(1-p<sub>e</sub>) = log &Pi;<sub>e&isin; T</sub> 1/(1-p<sub>e</sub>) =
log 1/(&Pi;<sub>e&isin; T</sub>(1-p<sub>e</sub>)).  Since we're trying to minimize that logarithm, and
logarithms are strictly increasing functions, we also are minimizing the
inside of the logarithm.  This is the same as maximizing the denominator,
which happens to be the probability of survival of the tree.
</p>
</div>
</div>

<div id="outline-container-orgheadline117" class="outline-4">
<h4 id="orgheadline117"><span class="section-number-4">31.3.2</span> Choosing your reduction</h4>
<div class="outline-text-4" id="text-31-3-2">
<p>
This isn't necessarily like EECS 343 reductions, where you find the easiest
reduction to do.  There are entire families of problems that are special
cases of each other.  A problem might be able to be reduced to the simplest
of these, or the most general of these.  The reduction to the most general
problem is usually easiest, and the reduction to the simpler problem is more
difficult.  The advantage of doing the harder reduction is generally a
faster algorithm to solve the simpler problem.  It's just a wonderful world
of tradeoffs here in computer science land.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline119" class="outline-3">
<h3 id="orgheadline119"><span class="section-number-3">31.4</span> GNU Octave</h3>
<div class="outline-text-3" id="text-31-4">
<ul class="org-ul">
<li>Download it via your package manager, or from the GNU website if you're a
Win/Mac user.</li>
<li>There is a good deal of documentation on the GNU site about how to use
Octave.  It looks like a less powerful Python+NumPy+Matplotlib, or maybe a
less powerful (open source) Mathematica.</li>
<li><code>glpk</code> function for linear programming.</li>
<li>First homework this afternoon, due in two weeks!</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgheadline125" class="outline-2">
<h2 id="orgheadline125"><span class="section-number-2">32</span> 2015-08-26 Wednesday</h2>
<div class="outline-text-2" id="text-32">
</div><div id="outline-container-orgheadline121" class="outline-3">
<h3 id="orgheadline121"><span class="section-number-3">32.1</span> Optimization Problems (cont.)</h3>
<div class="outline-text-3" id="text-32-1">
<p>
<b><b>Definition:</b></b> an <b>Optimization problem</b> is a set of instances of
optimization problems (iops).
</p>

<p>
Ex. Minimum Spanning Tree (MST) - give spanning tree \(T'\) of graph \(G\).
    Minimize \(\sum_{e \in T'}c(e)\), where \(c(e)\) is the cost of an edge.
</p>

<p>
Ex. Vertex Cover - (Find \(V' \subseteq V\) s.t. \(\forall (e \in E)\:
  e \bigcap V \neq \emptyset \). V' is the vertex cover. Minimize \( \vert V' \vert\)).
</p>
</div>
</div>

<div id="outline-container-orgheadline122" class="outline-3">
<h3 id="orgheadline122"><span class="section-number-3">32.2</span> Maximization problems</h3>
<div class="outline-text-3" id="text-32-2">
<p>
What about maximization problems? Well, they can reduce to minimization
problems:
\[ I_{max} = (X, f) \Rightarrow \tilde{I} = (X, -f)
    \:( \text{or sometimes }\tilde{I} = \left(X, \frac{1}{f}\right) )
   \]
</p>

<p>
\(\tilde{I}\) is now a minimization problem.
</p>
</div>
</div>

<div id="outline-container-orgheadline123" class="outline-3">
<h3 id="orgheadline123"><span class="section-number-3">32.3</span> Approximation Algorithms</h3>
<div class="outline-text-3" id="text-32-3">
<p>
Let I be an iop with an optimal solution \(x^*\) with an optimal value
\(c(x^*) = c^*(I)\). Let \(c(I)\) be the cost accrued by running algorithm
A on I. Then, the approximation ratio is:
\[ R = max_I \left( \frac{c(I)}{c^*(I)}, \frac{c^*(I)}{c(I)} \right) \]
</p>

<p>
(This formulation allows us to use the same ratio function for max and min
problems.)
</p>

<p>
If for any \(\epsilon > 0 \exists\:A\text{ s.t. }R = 1 + \epsilon \wedge O(n^k)\text{ w.r.t. input size}
  \) then \(A\) is a <b><b>polynomial time approximation scheme</b></b> (PTAS).
</p>

<p>
If A is polynomial w.r.t. input size and \(\epsilon\), then it is called a fully
polynomial time approximation scheme.
</p>

<p>
<b><b>def</b></b>: a k-approximation algorithm is if its result is within k-times the
minimum result. e.g. \(\forall G\:|VC| \le 2|VC^*|\) for approx vertex cover.
</p>
</div>
</div>

<div id="outline-container-orgheadline124" class="outline-3">
<h3 id="orgheadline124"><span class="section-number-3">32.4</span> Approx Vertex Cover</h3>
<div class="outline-text-3" id="text-32-4">
<p>
Algorithm: Given \(G = (V,E)\)
</p>
<ul class="org-ul">
<li>repeatedly
<ul class="org-ul">
<li>choose \( e = (u,v) \in E \)</li>
<li>\(V' = V' \cup e\)</li>
<li>delete all \(d \in E\) incident to \(u,v\)</li>
</ul></li>
<li>until \( E = \emptyset \)</li>
<li>return \(V'\)</li>
</ul>

<p>
Proof of correctness / 2 approx algorithm:
</p>
<ul class="org-ul">
<li>Every edge covered by VC aprx at termination</li>
<li>\(\forall e \in VC_{approx}\)
<ul class="org-ul">
<li>VC aprx adds at most 2 vertexes to \(V'\)</li>
<li>VC opt contains at least 1 of two vertexes to remain feasible</li>
</ul></li>
<li>VC aprx never considers same vertex twice because edges are removed</li>
<li>edges are disjoint \(\Rightarrow\:V'\) can be partitioned by edges added by VC
aprc</li>
<li>therefore, 2 aprx algorithm</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline128" class="outline-2">
<h2 id="orgheadline128"><span class="section-number-2">33</span> 2015-08-24 Monday</h2>
<div class="outline-text-2" id="text-33">
<ul class="org-ul">
<li>5 books, get sections from library</li>
<li>2 tests:
<ul class="org-ul">
<li>Final exam, possibly oral.</li>
<li>Midterm</li>
</ul></li>
<li>6 homeworks:
<ul class="org-ul">
<li>Need to know octave</li>
</ul></li>
</ul>
</div>

<div id="outline-container-orgheadline126" class="outline-3">
<h3 id="orgheadline126"><span class="section-number-3">33.1</span> Asymptotics</h3>
<div class="outline-text-3" id="text-33-1">
<p>
Measure time complexity.  Focus is on large inputs.
</p>

<ul class="org-ul">
<li><p>
f(n) &isin; O(g(n)) means "f(n) &le; g(n)"
</p>

<p>
&exist; c &gt; g, n<sub>0</sub> &gt; 0 s.t. &forall; n &ge; n<sub>0</sub> : f(n) &le; c g(n)
</p></li>

<li>f(n) = &Omega;(g(n)) defined: g(n) &isin; O(f(n))</li>

<li>f(n) &isin; &Theta;(g(n)) defined: f(n) &isin; O(g(n)) and f(n) &isin; &Omega;(g(n))</li>

<li>f(n) &isin; o(g(n)) defined: \(\lim_{n\to\infty} \frac{f(n)}{g(n)} = 0\)</li>

<li>f(n) &isin; &omega;(g(n)) defined: \(\lim{n\to\infty} \frac{f(n)}{g(n)}
     =\infty\)</li>
</ul>

<p>
What is \(n\)?  Input size.  Sometimes it's a number of elements, or it could
be multiple parameters (number of nodes, number of edges).
</p>

<p>
Sometimes we use the number of bits of the input.  For example, an algorithm
with input integer \(k\).  The number of bits is \(n=\Theta(\log k)\).  If the
runtime is \(O(k)\), it looks like it's linear time.  But in the number of
bits, it's exponential (\(O(2^n)\)).  It looks polynomial, but it's
exponential.  It's called pseudo-polynomial.
</p>

<p>
Formula:
</p>

<p>
\((1-\frac{x}{k})^k\), where \(x \in R\), \(k \in N^+\).  We have that quantity
\(< e^{-k}\), and \(\geq (1-x)\).  This will be used a lot apparently.
</p>
</div>
</div>

<div id="outline-container-orgheadline127" class="outline-3">
<h3 id="orgheadline127"><span class="section-number-3">33.2</span> Optimization Problems</h3>
<div class="outline-text-3" id="text-33-2">
<p>
<b><b>Definition:</b></b> An instance of an optimization problem is a pair \((X,f)\),
  where \(X\) is a set of feasible solutions, and \(f\) is an objective function.
  \(f\) maps from \(X\) to the real numbers.  An <i>optimal solution</i> \(x^*\) is an
  element of \(X\) with the property that \(f(x^*) \leq f(x) \: \forall x \in
     X\).
</p>

<p>
For instance, if you have a graph and you're talking about the minimum
spanning tree problem, \(X\) is the set of all MSTs, and \(f\) maps each to the
sum of the edge weights in the tree.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Stephen Brennan</p>
<p class="date">Created: 2015-11-18 Wed 16:24</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
