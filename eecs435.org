# -*- mode: Org; eval: (auto-fill-mode); eval: (flyspell-mode) -*-
#+TITLE: EECS 435 Notes
#+AUTHOR: Stephen Brennan
#+SETUPFILE: config.setup

* 2016-01-19 Tuesday

  *Constraints + Apriori Algorithm*

  - For anti-monotone constraints, you can prevent candidates from being
    generated when you know they and their successors will not satisfy the
    constraint.

** Preprocessing

   - Data mining is based on existing databases
   - In the real world, data is dirty:
     - Incomplete (lacking attribute values)
     - Noisy (containing errors or outliers)
     - Inconsistent (may have contradictions or discrepancies)
   - Quality of data mining relies on quality of data.
     - Garbage in, garbage out!

   *Types of Preprocessing*
   - Cleaning:
     - Fill in missing values, smooth noisy data, identify or remove outliers,
       resolve inconsistencies.
   - Integration
     - Integrating multiple databases, data cubes, or files
   - Transformation
     - Normalization, aggregation
   - Reduction
     - Reduce number of records, attributes, attribute values
     - (maybe use bagging/boosting, feature selection, dimensionality reduction?
       -- Dr. Soumya Ray, EECS 440, last semester!)

*** Missing Data
   - Missing data may be due to (not sure why this matters for us):
     - Equipment malfunction
     - Inconsistent, and therefore deleted
     - Not entered.
     - Data taken at different times.
     - Etc.
   - Strategies:
     - Delete entries with missing data
     - Fill in missing data.  How?
       - Manually?  Could be tedious.
       - Default value (e.g. unknown)
       - Average data (mean imputation, that is)
       - Most probable value (i.e. model-based imputation, perhaps using a
         Bayesian/other probabilistic model)

   *Noisy Data*
   - Binning:
     - partition continuous data into discrete "bins"
     - more details in next section
   - Regression:
     - Smooth by fitting a regression function, then use that.
   - Clustering
     - Could be used to bin things.
     - Could also be used to detect outliers.
   - Human + Computer
     - Examine records manually?

   *Binning*
   - Equal width:
     - divide range into N intervals of equal size.
     - simple, but outliers may dominate your result.
   - Equal depth:
     - sort data, divide into N groups with approximately the same number of
       records
     - handles skewed data better
   - After partitioning, you can smooth the bins by giving each member the mean
     value of the bin, or you can use the boundaries of the bin (whichever the
     record is closest to)

   *Regression*
   - Replace noisy or missing values by predicted values.
   - Of course, this requires some model of the relationship between
     attributes.  And you might be wrong.
     - Especially since if you have a solid model of the relationships between
       these attributes, you may be less likely to do data mining!
   - Useful both for smoothing and for handling missing data.

   Looks like we're skipping over clustering et al. and moving on to the next
   form of preprocessing:

*** Data Integration

    Want to identify duplicates, can use automated methods for comparing text,
    numbers, etc, for approximate equality.

    Could use information theoretic approaches to determine how much information
    each record adds to the database.

*** Data Transformation

    - Normalization: makes different records comparable.
    - Discretization: make continuous attributes discrete so we can apply
      methods that require discrete input
    - Attribute/feature construction: maybe new attributes created from the
      given ones are more useful for our purposes than ones given.
    - Map into vector space (awwww yeah)

    *Normalization*
    - min-max normalization
      \begin{equation}
        v' = \frac{v - min}{max - min}
      \end{equation}
    - z-score normalization
      \begin{equation}
        v' = \frac{v - \mu}{\sigma}
      \end{equation}
    - another one I missed

    *Discretization*
    - Three types of attributes:
      - Nominal (categorical) - values from an unordered set
      - Ordinal - values from an ordered set (presumably either finite or
        countably infinite, but not certain)
      - Continuous - values chosen from a continuous range (presumably
        uncountably infinite set).
    - Methods:
      - Binning
      - Clustering
      - Entropy-based approaches

    *Entropy-Based Discretization*
    - Binary discretization:
      - determine a boundary to partition the data into two intervals
      - pick the boundary that minimizes the entropy
      - this doesn't make sense to me, wouldn't this just partition into a set
        containing one, and a set containing the rest (this minimizes entropy)
        - google this later

    *Mapping Into Vector Space*
    - take "objects" or whatever our data is
    - determine attributes (e.g. counts of particular words, etc)
    - put these attributes as a vector

    *Data Reduction: Feature Selection*
    - sometimes additional information is hurting your performance
    - maybe cleaner data would help!
    - you want to select the attributes that give you the important information,
      get rid of the ones that are duplicate
    - could do greedy top-down or bottom up approaches that add/remove the next
      "best" feature
      - requires some measure of feature quality, and typically a cutoff
      - measure is typically information theory based: conditional entropy
      - heuristic: may not return the optimal feature set according to your
        criteria.
    - "branch and bound"
      - optimal, but takes longer
      - requires "monotone" shape of feature space (??)

    *Principle Component Analysis*
    - Projects vectors from d-dimensional space into a smaller (c \lt d) space.
    - Chooses first the dimension that minimizes "reproduction error".
    - Then the dimension that minimizes the error from the residuals, etc.
    - drawbacks:
      - difficult to interpret
      - requires numeric data

    *Sampling*
    - Random sampling can be used to reduce data (I guess).
    - You can choose with or without replacement

* 2016-01-14 Thursday

  *Illustration of Closed Patterns, Max-Patterns*

  - If your DB is {<a_1, ..., a_100>, <a_1, ..., a_50>}, and your minimum (absolute)
    support is 1:
  - Closed itemsets:
    - <a_1, ..., a_100>
    - <a_1, ..., a_50>
  - Max itemsets:
    - <a_1, ..., a_100>

  *Algorithm*
  - Subsets of frequent itemsets are frequent as well.
  - So itemsets which are infrequent should not have their supersets generated
    or tested.
  - Method (Apriori Algorithm):
    - Initially, scan DB once to get frequent 1-itemsets
    - Generate length (k+1) candidate itemsets from length k frequent itemsets
    - Test the candidates against DB
    - Terminate when no frequent or candidate set can be generated
  - Seems simple, but there are lost of detail on the algorithm.
  - How to generate candidates?  Two steps:
    - Step 1: self-joining
    - Step 2: pruning
  - Example of candidate generation:
    - l_3 = {abc, abd, acd, ace, bcd}
    - First, compute L_3 * L_3 (cartesian product, sort of)
    - abcd from abc and abd
    - acde from acd and ace
    - However, groups like acde are removed because their superset (ade) are not
      in the previous frequent itemset.
  - There was some Candidate Generation pseudo-code written in a SQL-like syntax
    that was rather confusing.
  - The pruning is pretty simple because you just need to check the k subsets of
    size k-1 to make sure they are all frequent.
  - Counting support of candidates:
    - Candidates are stored in a hash-tree
    - A leaf node contains a list of itemsets and counts.
    - Interior node contains a hash table of something (I hope the slides go up
      soon)
  - The issues for frequent pattern mining are:
    - Multiple scans of transaction database
    - Huge number of candidates
    - Tedious workload of support counting for candidates.
  - The improvements to the algorithm aim to address these concerns.
  - Partitioning:
    - Itemsets that are potentially frequent must be frequent in at least one of
      the partitions.
  - DHP: Reduce the number of candidates
    - A k-itemset whose corresponding hashing bucket count is below the
      threshold cannot be frequent.

  Multiple-Level Association Rules
  - Items frequently form hierarchies, eg milk can be whole, skim, 2%, etc.
  - May want to have different support thresholds for lower levels of the
    hierarchy.
  - Need to filter "redundant" association rules where child items are involved
    but they add no more information to the parent rule.

  Constraint based mining
  - Anti-monotone constraint: when an itemset S violates a constraint, so do all
    supersets of S.
  - Monotone constraint: when an itemset S satisfies a constraint, so do all
    supersets of S.

* 2016-01-12 Tuesday

** Introductory Material

  I was late (and probably will be late frequently due to another class).
  Hopefully I didn't miss too much.

  Grading:
  - 30% paper presentation
  - 10% attendance, participation
  - 60% project

  Paper presentation:
  - Group of two
  - Present one or two papers.
  - Lead discussion.
  - May be helpful to read a few references.

  Project:
  - Best to come up with your own ideas, but he does have some ideas on his
    slides.
  - Project proposal (3-5 pages, due Feb 16) should include:
    - Title, idea, survey of related work, data source, key
      algorithms/technology, and what you expect to submit at the end of the
      semester.
  - Final report (1-20 pages, due April 21) must include:
    - Comprehensive description of your project
    - Project idea, extended survey of related work, result, etc.
    - What worked, what didn't work, what surprised you, etc.

  Data mining conferences:
  - ACM-SIGKDD
  - IEEE-ICDM
  - SIAM-DM
  - PKDD
  - PAKDD

  Journal:
  - Data Mining and Knowledge Discovery
  - KDD Explorations

  Other references can be found in the conferences and journals for the fields:
  - Database systems
  - Artificial Intelligence
  - Statistics
  - Bioinformatics

** Chapter 5
   *Mining Frequent Patterns, Association, and Correlations*

   - Original slides at: http://www.cs.uiuc.edu/homes/hanj/bk2/slidesindex.htm
     (broken link).
   - Frequent pattern analysis - finding items, subsequences, substructures,
     etc, that occur frequently in a data set.
     - Related - frequent itemsets, association rule mining
     - Agrawal, Imielinski, and Swami '93 introduced these ideas
   - Motivation is to find regularities, rules, etc that give you insight into
     data.  For instance:
     - Beer and diapers are frequently purchased together.
     - What are subsequent purchases after a PC?
   - Itemset: a set of items
   - k-itemset: an itemset containing k items
   - Call the set of all items \(I = \{I_1, \dots, I_m\}\)
   - \(T \subseteq I\) is a transaction, and has a transaction id.
   - An association rule is of the form \(A \to B\), with minimum support and
     confidence, where \(A \subset I\), \(B \subset I\), \(A \cap B =
     \emptyset\).
   - Support (for \(A \to B\)) is \(Pr[A \cup B \subseteq T]\).
     - That is, the probability that a transaction contains all of A and B.
     - As a percentage, also referred to as relative support.
     - As a number, referred to as the absolute support
   - Confidence (for \(A \to B\)) is \(Pr[B \subset T | A \subset T]\)
     - That is, the conditional probability that a transaction contains B, given
       it contains A.
     - \(Pr[B \subset T | A \subset T] = \frac{Pr[B \cup A \subseteq T]}{Pr[A
       \subset T]} = \frac{support(A \cup B)}{support(A)}\)
   - Confidence can be easily derived from the support of \(A\) and \(A \cup
     B\).  Which means you can "reduce" this problem to finding itemsets with
     high confidence, and then looking for disjoint pairs with high confidence.
   - Big patterns have lots of subsets, and therefore lots of other frequent
     patterns.  So, you want to find closed patterns and max-patterns.
   - Closed pattern: X is closed if X is frequent and there is no super-pattern
     \(Y \supset X\) with the same support as X.
   - An itemset X is a max-pattern if X is frequent and there is no frequent
     super-pattern \(Y \supset X\).
     - A "maximal" frequent super-pattern.
     - All max-patterns are closed.
